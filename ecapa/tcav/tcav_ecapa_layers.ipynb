{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aaa618",
   "metadata": {},
   "source": [
    "# ECAPA-TDNN + TCAV on Mel Concepts\n",
    "\n",
    "This notebook:\n",
    "- Loads a pretrained SpeechBrain ECAPA-TDNN model\n",
    "- Uses synthetic mel-based concepts (rising / dropping / constant lines, etc.)\n",
    "- Computes CAVs on 3 internal SE-Res2 blocks\n",
    "- Computes TCAV scores:\n",
    "  - Globally (all 90 recordings)\n",
    "  - Per speaker (eden / idan / yoav)\n",
    "- Saves results to `tcav_results.csv`\n",
    "- Draws heatmaps for:\n",
    "  - Concept × Layer (global)\n",
    "  - Concept × Layer per speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaccfb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root = parent of this notebook folder\n",
    "ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Add concept folder to Python path\n",
    "sys.path.append(str(ROOT / \"concept\"))\n",
    "\n",
    "CONCEPT_ROOT = ROOT / \"concept\" / \"positive concepts dataset\"\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "from Preprocess import audio_to_mel_spectrogram\n",
    "from PreprocessParams import FREQUENCY_BIN_COUNT\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44f99a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchaudio' has no attribute 'list_audio_backends'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpretrained\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderClassifier\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlobes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mECAPA_TDNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ECAPA_TDNN\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading SpeechBrain ECAPA-TDNN...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" Comprehensive speech processing toolkit\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Brain, Stage, create_experiment_directory, parse_arguments\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimportutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated_redirect, lazy_export_all\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m     10\u001b[39m     os.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mversion.txt\u001b[39m\u001b[33m\"\u001b[39m), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\core.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoopedLoader, SaveableDataLoader\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     41\u001b[39m     DistributedSamplerWrapper,\n\u001b[32m     42\u001b[39m     ReproducibleRandomSampler,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautocast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AMPConfig, TorchAutocast\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\dataio\\dataloader.py:46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _BaseDataLoaderIter\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchsizeGuesser, PaddedBatch\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DynamicItemDataset\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     DistributedSamplerWrapper,\n\u001b[32m     49\u001b[39m     ReproducibleRandomSampler,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     mark_as_loader,\n\u001b[32m     53\u001b[39m     mark_as_saver,\n\u001b[32m     54\u001b[39m     register_checkpoint_hooks,\n\u001b[32m     55\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\dataio\\dataset.py:15\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MethodType\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data_csv, load_data_json\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataPipeline\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m batch_shuffle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\dataio\\dataio.py:36\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch_audio_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     check_torchaudio_backend,\n\u001b[32m     33\u001b[39m     validate_backend,\n\u001b[32m     34\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mcheck_torchaudio_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m logger = get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data_json\u001b[39m(json_path, replacements={}):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\ecapa_tcav\\.venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57\u001b[39m, in \u001b[36mcheck_torchaudio_backend\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m     logger.warning(\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to detect torchaudio major version; unsure how to check your setup. We recommend that you keep torchaudio up-to-date.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m torchaudio_major >= \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m torchaudio_minor >= \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     available_backends = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_audio_backends\u001b[49m()\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_backends) == \u001b[32m0\u001b[39m:\n\u001b[32m     60\u001b[39m         logger.warning(\n\u001b[32m     61\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m         )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchaudio' has no attribute 'list_audio_backends'"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Ensure torchaudio backend works\n",
    "try:\n",
    "    print(\"Torchaudio backends:\", torchaudio.list_audio_backends())\n",
    "except Exception as e:\n",
    "    print(\"Warning: torchaudio backend issue:\", e)\n",
    "    print(\"Continuing with soundfile backend only...\")\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN\n",
    "\n",
    "print(\"Loading SpeechBrain ECAPA-TDNN...\")\n",
    "\n",
    "try:\n",
    "    # The SAFE way to load SpeechBrain models\n",
    "    model = EncoderClassifier.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "        savedir=str(ROOT / \"ecapa_pretrained\"),\n",
    "        run_opts={\"device\": str(device)},  # <-- CRITICAL\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"ERROR loading ECAPA:\", e)\n",
    "    raise\n",
    "\n",
    "# Extract ECAPA backbone\n",
    "backbone: ECAPA_TDNN = model.mods[\"embedding_model\"]\n",
    "backbone.to(device)\n",
    "backbone.eval()\n",
    "\n",
    "print(\"ECAPA loaded successfully.\")\n",
    "print(\"Running on device:\", device)\n",
    "print()\n",
    "print(\"Printing backbone architecture:\")\n",
    "print(backbone)\n",
    "print()\n",
    "print(\"Backbone blocks:\")\n",
    "print(backbone.blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_to_input(mel: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel: (F, T) = (64, T) → Tensor (1, T, 64) on device\n",
    "    \"\"\"\n",
    "    mel = torch.tensor(mel, dtype=torch.float32)  # (F, T)\n",
    "    return mel.T.unsqueeze(0).to(device)  # (1, T, F)\n",
    "\n",
    "\n",
    "def get_activation_from_layer(mel: np.ndarray, layer: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Forward a single mel through ECAPA backbone and capture\n",
    "    the output activation of the given layer (no gradient).\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "\n",
    "    def hook_fn(_, __, out):\n",
    "        activations[\"A\"] = out.detach().cpu().numpy()\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "    x = mel_to_input(mel)\n",
    "    with torch.no_grad():\n",
    "        _ = backbone(x)\n",
    "    handle.remove()\n",
    "\n",
    "    return activations[\"A\"]  # shape (1, C, T') or (1, T', C)\n",
    "\n",
    "\n",
    "def activation_to_vec(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a layer activation to a 1D vector by averaging over time.\n",
    "    \"\"\"\n",
    "    A = A.squeeze(0)  # remove batch → (C, T') or (T', C)\n",
    "    if A.ndim != 2:\n",
    "        raise RuntimeError(f\"Expected 2D activation, got shape {A.shape}\")\n",
    "\n",
    "    # If first dim is channels\n",
    "    if A.shape[0] < A.shape[1]:\n",
    "        return A.mean(axis=1)  # (C,)\n",
    "    else:\n",
    "        return A.mean(axis=0)  # (C,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All concept subfolders in positive concepts dataset\n",
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "\n",
    "print(\"Found concept dirs:\")\n",
    "for d in concept_dirs:\n",
    "    print(\" -\", d.name)\n",
    "\n",
    "if len(concept_dirs) < 2:\n",
    "    raise RuntimeError(\"Need at least 2 concept folders for TCAV (pos vs neg).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc58e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_eval_mels_and_group():\n",
    "    wavs = sorted(DATA_DIR.glob(\"*.wav\"))\n",
    "    if len(wavs) == 0:\n",
    "        raise RuntimeError(f\"No wav files found in {DATA_DIR}\")\n",
    "\n",
    "    wav_files = wavs\n",
    "    eval_mels = []\n",
    "\n",
    "    for w in wav_files:\n",
    "        mel = audio_to_mel_spectrogram(w)\n",
    "        if mel.shape[0] != FREQUENCY_BIN_COUNT:\n",
    "            raise RuntimeError(f\"Mel dimension mismatch in {w}, got {mel.shape}\")\n",
    "        eval_mels.append(mel)\n",
    "\n",
    "    # Group by speaker prefix: eden / idan / yoav\n",
    "    speaker_mels = defaultdict(list)\n",
    "    speaker_wavs = defaultdict(list)\n",
    "\n",
    "    for w, mel in zip(wav_files, eval_mels):\n",
    "        name = w.name.lower()\n",
    "        if name.startswith(\"eden\"):\n",
    "            speaker = \"eden\"\n",
    "        elif name.startswith(\"idan\"):\n",
    "            speaker = \"idan\"\n",
    "        elif name.startswith(\"yoav\"):\n",
    "            speaker = \"yoav\"\n",
    "        else:\n",
    "            speaker = \"other\"\n",
    "        speaker_mels[speaker].append(mel)\n",
    "        speaker_wavs[speaker].append(w.name)\n",
    "\n",
    "    return wav_files, eval_mels, speaker_mels, speaker_wavs\n",
    "\n",
    "wav_files, eval_mels, speaker_mels, speaker_wavs = load_eval_mels_and_group()\n",
    "\n",
    "print(\"Total eval mels:\", len(eval_mels))\n",
    "for spk, files in speaker_wavs.items():\n",
    "    print(f\"Speaker {spk}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cav_for_concept_and_layer(layer_name: str,\n",
    "                                    concept_dir: Path,\n",
    "                                    all_concept_dirs: list[Path]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a CAV vector for:\n",
    "      - positive examples from concept_dir\n",
    "      - negative examples from all other concept dirs (sampled)\n",
    "    \"\"\"\n",
    "    layer = TARGET_LAYERS[layer_name]\n",
    "\n",
    "    pos_paths = sorted(concept_dir.glob(\"*.npy\"))\n",
    "    if len(pos_paths) == 0:\n",
    "        raise RuntimeError(f\"No .npy files in {concept_dir}\")\n",
    "\n",
    "    # Collect all negative npy files from other concept dirs\n",
    "    neg_paths_all = []\n",
    "    for d in all_concept_dirs:\n",
    "        if d == concept_dir:\n",
    "            continue\n",
    "        neg_paths_all.extend(sorted(d.glob(\"*.npy\")))\n",
    "\n",
    "    if len(neg_paths_all) == 0:\n",
    "        raise RuntimeError(\"No negative concept samples found.\")\n",
    "\n",
    "    # Sample negatives to have roughly balanced classes\n",
    "    n_pos = len(pos_paths)\n",
    "    n_neg = min(len(neg_paths_all), n_pos * 2)  # up to 2x negatives\n",
    "    neg_indices = np.random.choice(len(neg_paths_all), size=n_neg, replace=False)\n",
    "    neg_paths = [neg_paths_all[i] for i in neg_indices]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Positive = concept\n",
    "    for p in pos_paths:\n",
    "        mel = np.load(p)\n",
    "        A = get_activation_from_layer(mel, layer)\n",
    "        X.append(activation_to_vec(A))\n",
    "        Y.append(1)\n",
    "\n",
    "    # Negative = all other concepts\n",
    "    for p in neg_paths:\n",
    "        mel = np.load(p)\n",
    "        A = get_activation_from_layer(mel, layer)\n",
    "        X.append(activation_to_vec(A))\n",
    "        Y.append(0)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=1e-4, max_iter=2000)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    cav = clf.coef_.reshape(-1)\n",
    "    cav /= (np.linalg.norm(cav) + 1e-8)\n",
    "    return cav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c640e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcav_score_all(layer_name: str, cav: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute TCAV score on all eval_mels:\n",
    "    fraction of examples where directional derivative > 0.\n",
    "    \"\"\"\n",
    "    layer = TARGET_LAYERS[layer_name]\n",
    "    positives = 0\n",
    "    total = 0\n",
    "\n",
    "    for mel in eval_mels:\n",
    "        x = mel_to_input(mel)\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        activations = {}\n",
    "\n",
    "        def hook_fn(_, __, out):\n",
    "            out.retain_grad()\n",
    "            activations[\"A\"] = out\n",
    "\n",
    "        handle = layer.register_forward_hook(hook_fn)\n",
    "        out = backbone(x)\n",
    "        handle.remove()\n",
    "\n",
    "        # simple scalar \"task\" = norm of embedding\n",
    "        loss = out.norm()\n",
    "        loss.backward()\n",
    "\n",
    "        grad = activations[\"A\"].grad.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "        # time-mean\n",
    "        if grad.shape[0] < grad.shape[1]:\n",
    "            g_vec = grad.mean(axis=1)\n",
    "        else:\n",
    "            g_vec = grad.mean(axis=0)\n",
    "\n",
    "        directional_derivative = np.dot(g_vec, cav)\n",
    "        positives += (directional_derivative > 0)\n",
    "        total += 1\n",
    "\n",
    "        # clear gradients for safety\n",
    "        backbone.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "    return positives / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcav_score_for_speaker(layer_name: str,\n",
    "                           cav: np.ndarray,\n",
    "                           speaker: str) -> float:\n",
    "    \"\"\"\n",
    "    TCAV score restricted to a given speaker's mel list.\n",
    "    \"\"\"\n",
    "    if speaker not in speaker_mels:\n",
    "        raise RuntimeError(f\"No mels for speaker '{speaker}'\")\n",
    "\n",
    "    layer = TARGET_LAYERS[layer_name]\n",
    "    mels = speaker_mels[speaker]\n",
    "\n",
    "    positives = 0\n",
    "    total = 0\n",
    "\n",
    "    for mel in mels:\n",
    "        x = mel_to_input(mel)\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        activations = {}\n",
    "\n",
    "        def hook_fn(_, __, out):\n",
    "            out.retain_grad()\n",
    "            activations[\"A\"] = out\n",
    "\n",
    "        handle = layer.register_forward_hook(hook_fn)\n",
    "        out = backbone(x)\n",
    "        handle.remove()\n",
    "\n",
    "        loss = out.norm()\n",
    "        loss.backward()\n",
    "\n",
    "        grad = activations[\"A\"].grad.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "        if grad.shape[0] < grad.shape[1]:\n",
    "            g_vec = grad.mean(axis=1)\n",
    "        else:\n",
    "            g_vec = grad.mean(axis=0)\n",
    "\n",
    "        directional_derivative = np.dot(g_vec, cav)\n",
    "        positives += (directional_derivative > 0)\n",
    "        total += 1\n",
    "\n",
    "        backbone.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "    return positives / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_global = []   # list of dicts: {concept, layer, tcav}\n",
    "cavs = {}            # (concept_name, layer_name) -> cav vector\n",
    "\n",
    "for cdir in tqdm(concept_dirs, desc=\"Concepts\"):\n",
    "    cname = cdir.name\n",
    "    print(f\"\\n=== Concept: {cname} ===\")\n",
    "\n",
    "    for layer_name in TARGET_LAYERS.keys():\n",
    "        print(f\"  Building CAV for layer {layer_name} ...\")\n",
    "        cav = build_cav_for_concept_and_layer(layer_name, cdir, concept_dirs)\n",
    "        cavs[(cname, layer_name)] = cav\n",
    "\n",
    "        print(\"  Computing global TCAV...\")\n",
    "        score = tcav_score_all(layer_name, cav)\n",
    "        results_global.append({\n",
    "            \"Concept\": cname,\n",
    "            \"Layer\": layer_name,\n",
    "            \"Speaker\": \"all\",\n",
    "            \"TCAV\": score,\n",
    "        })\n",
    "        print(f\"    TCAV(all) = {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_speaker = []\n",
    "\n",
    "for (cname, layer_name), cav in cavs.items():\n",
    "    for speaker in [\"eden\", \"idan\", \"yoav\"]:\n",
    "        if speaker not in speaker_mels or len(speaker_mels[speaker]) == 0:\n",
    "            continue\n",
    "\n",
    "        s_score = tcav_score_for_speaker(layer_name, cav, speaker)\n",
    "        results_speaker.append({\n",
    "            \"Concept\": cname,\n",
    "            \"Layer\": layer_name,\n",
    "            \"Speaker\": speaker,\n",
    "            \"TCAV\": s_score,\n",
    "        })\n",
    "        print(f\"{cname} | {layer_name} | {speaker} → {s_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global = pd.DataFrame(results_global)\n",
    "df_speaker = pd.DataFrame(results_speaker)\n",
    "\n",
    "df_all = pd.concat([df_global, df_speaker], ignore_index=True)\n",
    "\n",
    "csv_path = ROOT / \"tcav_results.csv\"\n",
    "df_all.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Saved all TCAV results to:\", csv_path)\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only global rows (Speaker == \"all\")\n",
    "df_global_only = df_all[df_all[\"Speaker\"] == \"all\"]\n",
    "\n",
    "heatmap_df = df_global_only.pivot(\n",
    "    index=\"Concept\",\n",
    "    columns=\"Layer\",\n",
    "    values=\"TCAV\"\n",
    ").sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, max(4, 0.4 * len(heatmap_df))))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    annot=True,\n",
    "    cmap=\"viridis\",\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Global TCAV Heatmap (all speakers)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in [\"eden\", \"idan\", \"yoav\"]:\n",
    "    df_spk = df_all[df_all[\"Speaker\"] == speaker]\n",
    "    if df_spk.empty:\n",
    "        continue\n",
    "\n",
    "    pivot_df = df_spk.pivot(\n",
    "        index=\"Concept\",\n",
    "        columns=\"Layer\",\n",
    "        values=\"TCAV\"\n",
    "    ).sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, max(4, 0.4 * len(pivot_df))))\n",
    "    sns.heatmap(\n",
    "        pivot_df,\n",
    "        annot=True,\n",
    "        cmap=\"mako\",\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f\"TCAV Heatmap – Speaker: {speaker}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
