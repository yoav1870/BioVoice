{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "84a5af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import itertools\n",
    "import ast\n",
    "import json\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "182a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Grad-CAM hook class ----------\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM (Gradient-weighted Class Activation Mapping) for audio.\n",
    "    \n",
    "    Identifies which time frames in audio are most important for the model's \n",
    "    classification decision by:\n",
    "    1. Recording layer activations during forward pass\n",
    "    2. Recording gradients during backward pass\n",
    "    3. Computing importance weights (gradients averaged over time)\n",
    "    4. Creating a heatmap showing per-frame importance\n",
    "    \n",
    "    This helps visualize what the neural network \"pays attention to\" when \n",
    "    classifying a speaker.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_layer):\n",
    "        \"\"\"\n",
    "        Initialize Grad-CAM for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            target_layer: The neural network layer to analyze (e.g., last embedding layer)\n",
    "        \"\"\"\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None  # Will store layer output: [batch, channels, time_frames]\n",
    "        self.gradients = None    # Will store gradients: [batch, channels, time_frames]\n",
    "\n",
    "        # Install hooks that automatically capture data during forward/backward passes\n",
    "        self.target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.target_layer.register_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, inp, out):\n",
    "        \"\"\"\n",
    "        Called automatically during forward pass.\n",
    "        Saves the layer's output (activations) for later analysis.\n",
    "        \n",
    "        Shape: [batch=1, channels=128, time_frames=1000] for audio\n",
    "        \"\"\"\n",
    "        self.activations = out.detach()  \n",
    "\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"\n",
    "        Called automatically during backward pass.\n",
    "        Saves gradients flowing back through this layer.\n",
    "        \n",
    "        Gradients show how much each activation contributed to the loss.\n",
    "        Shape: [batch=1, channels=128, time_frames=1000]\n",
    "        \"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate the Grad-CAM heatmap from captured activations and gradients.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Compute importance weight for each channel (average gradient over time)\n",
    "        2. Weight each channel's activation by its importance\n",
    "        3. Sum weighted activations across channels → per-frame importance score\n",
    "        4. Normalize to [0, 1] range for visualization\n",
    "        \n",
    "        Returns:\n",
    "            np.array: 1D array of shape [time_frames] with values in [0, 1]\n",
    "                     0 = not important, 1 = very important for classification\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        grads = self.gradients        # [batch=1, channels=128, time=1000]\n",
    "        acts = self.activations       # [batch=1, channels=128, time=1000]\n",
    "\n",
    "        # STEP 1: Compute importance weight per channel\n",
    "        # Average gradients over time dimension to get one weight per channel\n",
    "        # This tells us: \"how important is channel X overall?\"\n",
    "        weights = grads.mean(dim=2, keepdim=True)  # [batch=1, channels=128, 1]\n",
    "\n",
    "        # STEP 2: Weight each activation by its channel's importance\n",
    "        # Then sum across channels → importance score per time frame\n",
    "        # This tells us: \"for each time frame, how important is it?\"\n",
    "        cam = (weights * acts).sum(dim=1)  # [batch=1, time=1000]\n",
    "\n",
    "        # STEP 3: Clean up the heatmap\n",
    "        cam = F.relu(cam)  # Keep only positive contributions\n",
    "        cam = cam.squeeze(0).detach().cpu().numpy()  # Convert [1, 1000] → [1000] numpy array\n",
    "\n",
    "        # STEP 4: Normalize to [0, 1] range for visualization\n",
    "        cam -= cam.min()  # Shift minimum to 0\n",
    "        cam /= (cam.max() + 1e-8)  # Scale maximum to 1 (add 1e-8 to avoid division by zero)\n",
    "        \n",
    "        return cam  # [time_frames] with values in [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d7c8be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New classifier: Linear(in_features=192, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"gradcam_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "data_dir = \"data\"\n",
    "# Reload the ecapa model fresh\n",
    "model_path = \"ecapa_pretrained\"  \n",
    "ecapa = EncoderClassifier.from_hparams(\n",
    "    source=model_path,\n",
    "    savedir=model_path\n",
    ")\n",
    "\n",
    "# # # Save model layers to a text file\n",
    "# model_str = str(ecapa.mods)\n",
    "# with open(\"ecapa_layers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(model_str)\n",
    "\n",
    "# Use the last SERes2Net block \n",
    "target_layer = ecapa.mods.embedding_model.blocks[-1]\n",
    "cam_extractor = GradCAM(target_layer)\n",
    "\n",
    "num_speakers = 3  # yoav, idan, eden #NOTE: ecapa pretrained has 1 speaker in classifier head so we need to change it to 3\n",
    "\n",
    "# New classifier head on top of ECAPA embedding\n",
    "new_classifier = nn.Linear(192, num_speakers)\n",
    "new_classifier = new_classifier.to(next(ecapa.parameters()).device)\n",
    "\n",
    "print(\"New classifier:\", new_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "82aded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "\n",
    "speakers = {\"yoav\": [], \"idan\": [], \"eden\": []}\n",
    "\n",
    "for f in files:\n",
    "    prefix = f.split(\"_\")[0]  \n",
    "    if prefix in speakers:\n",
    "        speakers[prefix].append(os.path.join(data_dir, f))\n",
    "\n",
    "\n",
    "selected = {}\n",
    "\n",
    "for speaker, file_list in speakers.items():\n",
    "    selected[speaker] = file_list[:5]  \n",
    "\n",
    "# with open(\"selected_files.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(selected, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam_on_wav(wav_path, speaker):\n",
    "    # Load audio at 16kHz and convert to tensor\n",
    "    wav, sr = librosa.load(wav_path, sr=16000)\n",
    "    wav_tensor = torch.tensor([wav]).float()\n",
    "\n",
    "    # Switch to eval mode for inference\n",
    "    ecapa.eval()\n",
    "    new_classifier.eval()\n",
    "\n",
    "    # Enable gradients even in eval mode (needed for Grad-CAM)\n",
    "    with torch.enable_grad():\n",
    "        # Step 1: Convert raw audio to mel-spectrogram features\n",
    "        features = ecapa.mods.compute_features(wav_tensor)\n",
    "        lengths = torch.tensor([features.shape[-1]])\n",
    "        # Normalize features (zero mean, unit variance)\n",
    "        features = ecapa.mods.mean_var_norm(features, lengths)\n",
    "\n",
    "        # Enable gradient tracking for feature analysis\n",
    "        features.requires_grad_(True)\n",
    "\n",
    "        # Step 2: Extract speaker embedding (192-dim vector)\n",
    "        emb = ecapa.mods.embedding_model(features)\n",
    "        emb = emb.squeeze()  # Remove any size-1 dimensions\n",
    "\n",
    "        # Ensure embedding is 2D for classifier: [1, 192]\n",
    "        if emb.dim() == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "\n",
    "        # Step 3: Classify speaker (get logits for 3 speakers)\n",
    "        logits = new_classifier(emb)  # [1, 3] → scores for each speaker\n",
    "        logits = logits.squeeze(0)    # Remove batch dim → [3]\n",
    "\n",
    "        # Get predicted class (0=yoav, 1=idan, 2=eden)\n",
    "        pred_class = logits.argmax().item()\n",
    "        print(f\"Predicted class: {pred_class}, logits: {logits}\")\n",
    "\n",
    "        # Step 4: Compute gradients for Grad-CAM\n",
    "        # Zero out previous gradients\n",
    "        ecapa.zero_grad()\n",
    "        new_classifier.zero_grad()\n",
    "\n",
    "        # Create one-hot vector for predicted class\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot[pred_class] = 1\n",
    "\n",
    "        # Backward pass: compute gradients w.r.t. predicted class\n",
    "        logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        # Step 5: Generate Grad-CAM heatmap (not displayed, just for reference)\n",
    "        cam = cam_extractor.generate()  # Returns normalized heatmap [T]\n",
    "\n",
    "        # --- Prepare spectrogram for visualization ---\n",
    "        # Compute mel-spectrogram to match ECAPA preprocessing for display\n",
    "        n_fft = 512\n",
    "        hop_length = 80\n",
    "        n_mels = 80\n",
    "        mel_spec = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft,\n",
    "                                                  hop_length=hop_length, n_mels=n_mels)\n",
    "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Step 6: Plot mel-spectrogram only\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_mel, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', ax=ax)\n",
    "        ax.set_title(f\"Mel-spectrogram: {os.path.basename(wav_path)} (Predicted: {pred_class})\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(output_dir, f\"gradcam_{os.path.basename(wav_path)}.png\")\n",
    "        fig.savefig(out_path)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved Mel-spectrogram to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1556be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Speaker: yoav =====\n",
      "Predicted class: 1, logits: tensor([-15.6716,   6.0583, -12.9387], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([-15.6716,   6.0583, -12.9387], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_001.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_001.wav.png\n",
      "Predicted class: 0, logits: tensor([ -2.1351,  -5.8661, -13.7958], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 0, logits: tensor([ -2.1351,  -5.8661, -13.7958], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_002.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_002.wav.png\n",
      "Predicted class: 0, logits: tensor([16.5364,  0.0655,  0.9121], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 0, logits: tensor([16.5364,  0.0655,  0.9121], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_003.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_003.wav.png\n",
      "Predicted class: 1, logits: tensor([  2.9928,   6.7017, -14.2316], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([  2.9928,   6.7017, -14.2316], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_004.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_004.wav.png\n",
      "Predicted class: 1, logits: tensor([ -2.0385,   1.4338, -25.1462], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([ -2.0385,   1.4338, -25.1462], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_005.wav.png\n",
      "\n",
      "===== Speaker: idan =====\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_yoav_005.wav.png\n",
      "\n",
      "===== Speaker: idan =====\n",
      "Predicted class: 1, logits: tensor([5.6853, 9.3614, 8.9513], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([5.6853, 9.3614, 8.9513], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_001.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_001.wav.png\n",
      "Predicted class: 1, logits: tensor([-4.7497, 15.1137,  5.0679], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([-4.7497, 15.1137,  5.0679], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_002.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_002.wav.png\n",
      "Predicted class: 2, logits: tensor([ 5.3067, -4.1149,  5.8800], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 2, logits: tensor([ 5.3067, -4.1149,  5.8800], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_003.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_003.wav.png\n",
      "Predicted class: 1, logits: tensor([-3.1994, 13.8111,  2.9084], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([-3.1994, 13.8111,  2.9084], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_004.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_004.wav.png\n",
      "Predicted class: 1, logits: tensor([ 2.0378, 15.3196, 10.3454], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 1, logits: tensor([ 2.0378, 15.3196, 10.3454], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_005.wav.png\n",
      "\n",
      "===== Speaker: eden =====\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_idan_005.wav.png\n",
      "\n",
      "===== Speaker: eden =====\n",
      "Predicted class: 2, logits: tensor([-18.3422,  -2.3887,  11.4457], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 2, logits: tensor([-18.3422,  -2.3887,  11.4457], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_001.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_001.wav.png\n",
      "Predicted class: 2, logits: tensor([ 0.3395,  5.7570, 23.6291], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 2, logits: tensor([ 0.3395,  5.7570, 23.6291], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_002.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_002.wav.png\n",
      "Predicted class: 0, logits: tensor([16.5224,  1.8301, 11.9545], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 0, logits: tensor([16.5224,  1.8301, 11.9545], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_003.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_003.wav.png\n",
      "Predicted class: 2, logits: tensor([-2.7896, -2.4024, 14.4296], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 2, logits: tensor([-2.7896, -2.4024, 14.4296], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_004.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_004.wav.png\n",
      "Predicted class: 2, logits: tensor([-1.4720,  0.0282, 15.2562], grad_fn=<SqueezeBackward1>)\n",
      "Predicted class: 2, logits: tensor([-1.4720,  0.0282, 15.2562], grad_fn=<SqueezeBackward1>)\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_005.wav.png\n",
      "Saved Mel-spectrogram to gradcam_results\\gradcam_eden_005.wav.png\n"
     ]
    }
   ],
   "source": [
    "# run_gradcam_on_wav(\"data/yoav_001.wav\", \"yoav\")\n",
    "for speaker in selected:\n",
    "    print(f\"\\n===== Speaker: {speaker} =====\")\n",
    "    for wav_path in selected[speaker]:\n",
    "        run_gradcam_on_wav(wav_path, speaker)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
