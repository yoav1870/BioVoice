{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:22: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:22: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import itertools\n",
    "import ast\n",
    "import json\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Grad-CAM hook class ----------\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM (Gradient-weighted Class Activation Mapping) for audio.\n",
    "    \n",
    "    Identifies which time frames in audio are most important for the model's \n",
    "    classification decision by:\n",
    "    1. Recording layer activations during forward pass\n",
    "    2. Recording gradients during backward pass\n",
    "    3. Computing importance weights (gradients averaged over time)\n",
    "    4. Creating a heatmap showing per-frame importance\n",
    "    \n",
    "    This helps visualize what the neural network \"pays attention to\" when \n",
    "    classifying a speaker.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_layer):\n",
    "        \"\"\"\n",
    "        Initialize Grad-CAM for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            target_layer: The neural network layer to analyze (e.g., last embedding layer)\n",
    "        \"\"\"\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None  # Will store layer output: [batch, channels, time_frames]\n",
    "        self.gradients = None    # Will store gradients: [batch, channels, time_frames]\n",
    "\n",
    "        # Install hooks that automatically capture data during forward/backward passes\n",
    "        self.target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.target_layer.register_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, inp, out):\n",
    "        \"\"\"\n",
    "        Called automatically during forward pass.\n",
    "        Saves the layer's output (activations) for later analysis.\n",
    "        \n",
    "        Shape: [batch=1, channels=128, time_frames=1000] for audio\n",
    "        \"\"\"\n",
    "        self.activations = out.detach()  \n",
    "\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"\n",
    "        Called automatically during backward pass.\n",
    "        Saves gradients flowing back through this layer.\n",
    "        \n",
    "        Gradients show how much each activation contributed to the loss.\n",
    "        Shape: [batch=1, channels=128, time_frames=1000]\n",
    "        \"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate the Grad-CAM heatmap from captured activations and gradients.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Compute importance weight for each channel (average gradient over time)\n",
    "        2. Weight each channel's activation by its importance\n",
    "        3. Sum weighted activations across channels → per-frame importance score\n",
    "        4. Normalize to [0, 1] range for visualization\n",
    "        \n",
    "        Returns:\n",
    "            np.array: 1D array of shape [time_frames] with values in [0, 1]\n",
    "                     0 = not important, 1 = very important for classification\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        grads = self.gradients        # [batch=1, channels=128, time=1000]\n",
    "        acts = self.activations       # [batch=1, channels=128, time=1000]\n",
    "\n",
    "        # STEP 1: Compute importance weight per channel\n",
    "        # Average gradients over time dimension to get one weight per channel\n",
    "        # This tells us: \"how important is channel X overall?\"\n",
    "        weights = grads.mean(dim=2, keepdim=True)  # [batch=1, channels=128, 1]\n",
    "\n",
    "        # STEP 2: Weight each activation by its channel's importance\n",
    "        # Then sum across channels → importance score per time frame\n",
    "        # This tells us: \"for each time frame, how important is it?\"\n",
    "        cam = (weights * acts).sum(dim=1)  # [batch=1, time=1000]\n",
    "\n",
    "        # STEP 3: Clean up the heatmap\n",
    "        cam = F.relu(cam)  # Keep only positive contributions\n",
    "        cam = cam.squeeze(0).detach().cpu().numpy()  # Convert [1, 1000] → [1000] numpy array\n",
    "\n",
    "        # STEP 4: Normalize to [0, 1] range for visualization\n",
    "        cam -= cam.min()  # Shift minimum to 0\n",
    "        cam /= (cam.max() + 1e-8)  # Scale maximum to 1 (add 1e-8 to avoid division by zero)\n",
    "        \n",
    "        return cam  # [time_frames] with values in [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c8be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\utils\\checkpoints.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(path, map_location=device), strict=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New classifier: Linear(in_features=192, out_features=3, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\processing\\features.py:1218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"gradcam_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "data_dir = \"data\"\n",
    "# Reload the ecapa model fresh\n",
    "model_path = \"ecapa_pretrained\"  \n",
    "ecapa = EncoderClassifier.from_hparams(\n",
    "    source=model_path,\n",
    "    savedir=model_path\n",
    ")\n",
    "\n",
    "# # # Save model layers to a text file\n",
    "# model_str = str(ecapa.mods)\n",
    "# with open(\"ecapa_layers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(model_str)\n",
    "\n",
    "# Use the last SERes2Net block \n",
    "target_layer = ecapa.mods.embedding_model.blocks[-1]\n",
    "cam_extractor = GradCAM(target_layer)\n",
    "\n",
    "num_speakers = 3  # yoav, idan, eden #NOTE: ecapa pretrained has 1 speaker in classifier head so we need to change it to 3\n",
    "\n",
    "# New classifier head on top of ECAPA embedding\n",
    "new_classifier = nn.Linear(192, num_speakers)\n",
    "new_classifier = new_classifier.to(next(ecapa.parameters()).device)\n",
    "\n",
    "print(\"New classifier:\", new_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82aded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "\n",
    "speakers = {\"yoav\": [], \"idan\": [], \"eden\": []}\n",
    "\n",
    "for f in files:\n",
    "    prefix = f.split(\"_\")[0]  \n",
    "    if prefix in speakers:\n",
    "        speakers[prefix].append(os.path.join(data_dir, f))\n",
    "\n",
    "\n",
    "selected = {}\n",
    "\n",
    "for speaker, file_list in speakers.items():\n",
    "    selected[speaker] = file_list[:5]  \n",
    "\n",
    "# with open(\"selected_files.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(selected, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f61701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_gradcam_on_wav(wav_path, speaker):\n",
    "#     # Load audio at 16kHz and convert to tensor\n",
    "#     wav, sr = librosa.load(wav_path, sr=16000)\n",
    "#     wav_tensor = torch.tensor([wav]).float()\n",
    "\n",
    "#     # Switch to eval mode for inference\n",
    "#     ecapa.eval()\n",
    "#     new_classifier.eval()\n",
    "\n",
    "#     # Enable gradients even in eval mode (needed for Grad-CAM)\n",
    "#     with torch.enable_grad():\n",
    "#         # Step 1: Convert raw audio to mel-spectrogram features\n",
    "#         features = ecapa.mods.compute_features(wav_tensor)\n",
    "#         lengths = torch.tensor([features.shape[-1]])\n",
    "#         # Normalize features (zero mean, unit variance)\n",
    "#         features = ecapa.mods.mean_var_norm(features, lengths)\n",
    "\n",
    "#         # Enable gradient tracking for feature analysis\n",
    "#         features.requires_grad_(True)\n",
    "\n",
    "#         # Step 2: Extract speaker embedding (192-dim vector)\n",
    "#         emb = ecapa.mods.embedding_model(features)\n",
    "#         emb = emb.squeeze()  # Remove any size-1 dimensions\n",
    "\n",
    "#         # Ensure embedding is 2D for classifier: [1, 192]\n",
    "#         if emb.dim() == 1:\n",
    "#             emb = emb.unsqueeze(0)\n",
    "\n",
    "#         # Step 3: Classify speaker (get logits for 3 speakers)\n",
    "#         logits = new_classifier(emb)  # [1, 3] → scores for each speaker\n",
    "#         logits = logits.squeeze(0)    # Remove batch dim → [3]\n",
    "\n",
    "#         # Get predicted class (0=yoav, 1=idan, 2=eden)\n",
    "#         pred_class = logits.argmax().item()\n",
    "#         print(f\"Predicted class: {pred_class}, logits: {logits}\")\n",
    "\n",
    "#         # Step 4: Compute gradients for Grad-CAM\n",
    "#         # Zero out previous gradients\n",
    "#         ecapa.zero_grad()\n",
    "#         new_classifier.zero_grad()\n",
    "\n",
    "#         # Create one-hot vector for predicted class\n",
    "#         one_hot = torch.zeros_like(logits)\n",
    "#         one_hot[pred_class] = 1\n",
    "\n",
    "#         # Backward pass: compute gradients w.r.t. predicted class\n",
    "#         logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "#         # Step 5: Generate Grad-CAM heatmap (not displayed, just for reference)\n",
    "#         cam = cam_extractor.generate()  # Returns normalized heatmap [T]\n",
    "\n",
    "#         # --- Prepare spectrogram for visualization ---\n",
    "#         # Compute mel-spectrogram to match ECAPA preprocessing for display\n",
    "#         n_fft = 512\n",
    "#         hop_length = 80\n",
    "#         n_mels = 80\n",
    "#         mel_spec = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft,\n",
    "#                                                   hop_length=hop_length, n_mels=n_mels)\n",
    "#         log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "#         # Step 6: Plot mel-spectrogram only\n",
    "#         fig, ax = plt.subplots(figsize=(10, 4))\n",
    "#         librosa.display.specshow(log_mel, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', ax=ax)\n",
    "#         ax.set_title(f\"Mel-spectrogram: {os.path.basename(wav_path)} (Predicted: {pred_class})\")\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         out_path = os.path.join(output_dir, f\"gradcam_{os.path.basename(wav_path)}.png\")\n",
    "#         fig.savefig(out_path)\n",
    "#         plt.close(fig)\n",
    "#         print(f\"Saved Mel-spectrogram to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4511eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam_on_wav(wav_path, speaker):\n",
    "    # Load audio at 16kHz and convert to tensor\n",
    "    wav, sr = librosa.load(wav_path, sr=16000)\n",
    "    wav_tensor = torch.tensor([wav]).float()\n",
    "\n",
    "    # Switch to eval mode for inference\n",
    "    ecapa.eval()\n",
    "    new_classifier.eval()\n",
    "\n",
    "    # Enable gradients even in eval mode (needed for Grad-CAM)\n",
    "    with torch.enable_grad():\n",
    "        # Step 1: Convert raw audio to mel-spectrogram features\n",
    "        features = ecapa.mods.compute_features(wav_tensor)\n",
    "        lengths = torch.tensor([features.shape[-1]])\n",
    "        features = ecapa.mods.mean_var_norm(features, lengths)\n",
    "\n",
    "        # Enable gradient tracking for feature analysis\n",
    "        features.requires_grad_(True)\n",
    "\n",
    "        # Step 2: Extract speaker embedding\n",
    "        emb = ecapa.mods.embedding_model(features)\n",
    "        emb = emb.squeeze()\n",
    "\n",
    "        if emb.dim() == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "\n",
    "        # Step 3: Classify speaker\n",
    "        logits = new_classifier(emb)\n",
    "        logits = logits.squeeze(0)\n",
    "        pred_class = logits.argmax().item()\n",
    "        print(f\"Predicted class: {pred_class}, logits: {logits}\")\n",
    "\n",
    "        # Step 4: Compute gradients for Grad-CAM\n",
    "        ecapa.zero_grad()\n",
    "        new_classifier.zero_grad()\n",
    "\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot[pred_class] = 1\n",
    "        logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        # Step 5: Generate Grad-CAM heatmap\n",
    "        cam = cam_extractor.generate()  # [T]\n",
    "\n",
    "        # Prepare mel spectrogram\n",
    "        n_fft = 512\n",
    "        hop_length = 80\n",
    "        n_mels = 80\n",
    "        mel_spec = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft,\n",
    "                                                  hop_length=hop_length, n_mels=n_mels)\n",
    "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "        # 1. plot mel spectrogram\n",
    "        librosa.display.specshow(log_mel, sr=sr, hop_length=hop_length,\n",
    "                                 x_axis='time', y_axis='mel', cmap='magma', ax=ax)\n",
    "\n",
    "        # 2. resize CAM to spectrogram time axis\n",
    "        cam_resized = np.interp(\n",
    "            np.linspace(0, len(cam), log_mel.shape[1]),\n",
    "            np.arange(len(cam)),\n",
    "            cam\n",
    "        )\n",
    "\n",
    "        # 3. overlay CAM (heatmap)\n",
    "        ax.imshow(cam_resized[np.newaxis, :],\n",
    "                  cmap='jet',\n",
    "                  aspect='auto',\n",
    "                  alpha=0.4,\n",
    "                  extent=[0, log_mel.shape[1], 0, log_mel.shape[0]])\n",
    "\n",
    "        ax.set_title(f\"Grad-CAM: {os.path.basename(wav_path)} (Predicted: {pred_class})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(output_dir, f\"gradcam_{os.path.basename(wav_path)}.png\")\n",
    "        fig.savefig(out_path)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved Grad-CAM overlay to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1556be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Speaker: yoav =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav1\\AppData\\Local\\Temp\\ipykernel_9148\\394123404.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  wav_tensor = torch.tensor([wav]).float()\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\functional.py:666: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:878.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1, logits: tensor([-15.2227,  -1.5052, -10.4673], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav1\\AppData\\Local\\Temp\\ipykernel_9148\\394123404.py:76: UserWarning: All values for SymLogScale are below linthresh, making it effectively linear. You likely should lower the value of linthresh. \n",
      "  fig.savefig(out_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_yoav_001.wav.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1, logits: tensor([-13.1848,  -8.0947, -13.4141], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_yoav_002.wav.png\n",
      "Predicted class: 1, logits: tensor([-9.2355,  4.7210,  0.4761], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_yoav_003.wav.png\n",
      "Predicted class: 1, logits: tensor([-16.0184,  -7.0774,  -9.7240], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_yoav_004.wav.png\n",
      "Predicted class: 1, logits: tensor([-3.9303,  5.8597, -6.4010], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_yoav_005.wav.png\n",
      "\n",
      "===== Speaker: idan =====\n",
      "Predicted class: 1, logits: tensor([-13.3602,   8.0547,   1.7203], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_idan_001.wav.png\n",
      "Predicted class: 2, logits: tensor([-7.7366,  1.1696, 10.7644], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_idan_002.wav.png\n",
      "Predicted class: 1, logits: tensor([-11.0591,   4.1225,  -0.2376], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_idan_003.wav.png\n",
      "Predicted class: 2, logits: tensor([-0.7887,  4.0682,  6.6174], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_idan_004.wav.png\n",
      "Predicted class: 2, logits: tensor([-13.2126,  -3.6035,  -3.3030], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_idan_005.wav.png\n",
      "\n",
      "===== Speaker: eden =====\n",
      "Predicted class: 0, logits: tensor([  3.6347, -15.6877, -11.2965], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_eden_001.wav.png\n",
      "Predicted class: 0, logits: tensor([ -7.3443,  -9.0539, -10.1299], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_eden_002.wav.png\n",
      "Predicted class: 0, logits: tensor([  7.4039, -13.2898, -11.0849], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_eden_003.wav.png\n",
      "Predicted class: 0, logits: tensor([ 6.1139, -3.6556, -9.6139], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_eden_004.wav.png\n",
      "Predicted class: 0, logits: tensor([10.2431, -7.9975, -7.7393], grad_fn=<SqueezeBackward1>)\n",
      "Saved Grad-CAM overlay to gradcam_results\\gradcam_eden_005.wav.png\n"
     ]
    }
   ],
   "source": [
    "# run_gradcam_on_wav(\"data/yoav_001.wav\", \"yoav\")\n",
    "for speaker in selected:\n",
    "    print(f\"\\n===== Speaker: {speaker} =====\")\n",
    "    for wav_path in selected[speaker]:\n",
    "        run_gradcam_on_wav(wav_path, speaker)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
