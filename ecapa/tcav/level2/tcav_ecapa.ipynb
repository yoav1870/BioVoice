{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1ba65e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === 2. Import modules inside concept/ ===\n",
    "# concept/ MUST contain __init__.py\n",
    "from Preprocess import audio_to_mel_spectrogram\n",
    "from PreprocessParams import TARGET_FRAMES, FREQUENCY_BIN_COUNT\n",
    "from concepts_creation import generate_random_pattern_spectrogram\n",
    "\n",
    "# === 3. Other required imports ===\n",
    "from typing import List, Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from captum.concept import TCAV, Concept\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "\n",
    "# For ECAPA model\n",
    "from speechbrain.pretrained import EncoderClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f72a7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCEPT_UNIQUE_NAMES = [\n",
    "    \"long-constant-thick\",\n",
    "    \"long-dropping-flat-thick\",\n",
    "    \"long-dropping-steep-thick\",\n",
    "    \"long-dropping-steep-thin\",\n",
    "    \"long-rising-flat-thick\",\n",
    "    \"long-rising-steep-thick\",\n",
    "    \"long-rising-steep-thin\",\n",
    "    \"short-constant-thick\",\n",
    "    \"short-dropping-steep-thick\",\n",
    "    \"short-dropping-steep-thin\",\n",
    "    \"short-rising-steep-thick\",\n",
    "    \"short-rising-steep-thin\"\n",
    "]\n",
    "\n",
    "label_2_index = {\n",
    "    \"eden\": 0,\n",
    "    \"idan\": 1,\n",
    "    \"yoav\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c7bfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreGeneratedRandomSpectrogramDataset(Dataset):\n",
    "    def __init__(self, n_samples: int, freq_count=FREQUENCY_BIN_COUNT, frames=TARGET_FRAMES, rng_seed=None):\n",
    "        self.n_samples = n_samples\n",
    "        self.freq_count = freq_count\n",
    "        self.frames = frames\n",
    "        self.rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "        self.data = np.array([\n",
    "            generate_random_pattern_spectrogram(freq_count, frames, rng=self.rng)\n",
    "            for _ in range(n_samples)\n",
    "        ])\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].unsqueeze(0)  # [1, H, W]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "07635267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreGeneratedConceptDataset(Dataset):\n",
    "    def __init__(self, n_samples: int, concept_name: str, root_concept_dir: Path,\n",
    "                 freq_count=FREQUENCY_BIN_COUNT, frames_count=TARGET_FRAMES, rng_seed=None):\n",
    "        \n",
    "        self.concept_name = concept_name\n",
    "        self.root_concept_dir = Path(root_concept_dir)\n",
    "        concept_dir = self.root_concept_dir / concept_name\n",
    "        concept_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        self.data = [\n",
    "            np.load(f) for f in concept_dir.glob(\"*.npy\")\n",
    "        ]\n",
    "        self.data = torch.tensor(np.array(self.data), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "03b1b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tcav_with_pamalia_dict(model_path: Path, concept_samples_count=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ecapa = EncoderClassifier.from_hparams(\n",
    "        source=str(model_path),\n",
    "        savedir=str(model_path),\n",
    "        run_opts={\"device\": str(device)}\n",
    "    )\n",
    "    ecapa.eval()\n",
    "\n",
    "    # Make embedding_model accessible at top level\n",
    "    ecapa.embedding_model = ecapa.mods.embedding_model\n",
    "    encoder = ecapa.embedding_model\n",
    "\n",
    "    # ---------- DYNAMIC LAYER DETECTION ----------\n",
    "    # Find last block index\n",
    "    last_block_index = len(encoder.blocks) - 1\n",
    "    target_layer_obj = encoder.blocks[last_block_index]\n",
    "\n",
    "    # Expose block under valid attribute name\n",
    "    block_attr_name = f\"block{last_block_index}\"\n",
    "    setattr(encoder, block_attr_name, target_layer_obj)\n",
    "\n",
    "    # TCAV layer name (string path)\n",
    "    layer_name = f\"embedding_model.{block_attr_name}\"\n",
    "    # --------------------------------------------\n",
    "\n",
    "    tcav = TCAV(\n",
    "        model=ecapa,\n",
    "        layers=[layer_name],\n",
    "        layer_dict={layer_name: target_layer_obj},\n",
    "        test_split_ratio=0.33\n",
    "    )\n",
    "\n",
    "    # Load concepts\n",
    "    concepts_root = Path.cwd() / \"positive concepts dataset\"\n",
    "    if not concepts_root.exists():\n",
    "        raise FileNotFoundError(f\"Concepts directory not found: {concepts_root}\")\n",
    "\n",
    "    positive_concepts = [\n",
    "        Concept(\n",
    "            id=i,\n",
    "            name=name,\n",
    "            data_iter=DataLoader(\n",
    "                PreGeneratedConceptDataset(\n",
    "                    n_samples=concept_samples_count,\n",
    "                    concept_name=name,\n",
    "                    root_concept_dir=concepts_root\n",
    "                ),\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        for i, name in enumerate(CONCEPT_UNIQUE_NAMES)\n",
    "    ]\n",
    "\n",
    "    random_dataset = PreGeneratedRandomSpectrogramDataset(n_samples=concept_samples_count)\n",
    "    random_concept = Concept(\n",
    "        id=len(positive_concepts),\n",
    "        name=\"random\",\n",
    "        data_iter=DataLoader(random_dataset, shuffle=False)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"tcav\": tcav,\n",
    "        \"positive-concepts\": positive_concepts,\n",
    "        \"random-concept\": random_concept,\n",
    "        \"layer\": target_layer_obj,\n",
    "        \"layer_name\": layer_name,\n",
    "        \"ecapa\": ecapa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "90e5de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_cav_accuracy_df(tcav, positive_concepts, random_concept, float_precision=3):\n",
    "    rows = []\n",
    "\n",
    "    experimental_sets = [[c, random_concept] for c in positive_concepts]\n",
    "    cavs = tcav.compute_cavs(experimental_sets)\n",
    "\n",
    "    for key, layer_dict in cavs.items():\n",
    "        pos_id = int(str(key).split(\"-\")[0])\n",
    "        concept_name = positive_concepts[pos_id].name\n",
    "\n",
    "        for layer_name, cav_obj in layer_dict.items():\n",
    "            if cav_obj is None or cav_obj.stats is None:\n",
    "                continue\n",
    "\n",
    "            acc = cav_obj.stats[\"accs\"]\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.item()\n",
    "\n",
    "            rows.append({\n",
    "                \"concept name\": concept_name,\n",
    "                \"layer name\": layer_name,\n",
    "                \"cav acc\": round(acc, float_precision)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4975fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tcav_dict_per_sample(tcav_raw, df, model_path, label_2_index):\n",
    "    tcav = tcav_raw[\"tcav\"]\n",
    "    pos = tcav_raw[\"positive-concepts\"]\n",
    "    rnd = tcav_raw[\"random-concept\"]\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        wav_path = row[\"path\"]\n",
    "        label_name = row[\"predicted_label\"]\n",
    "\n",
    "        if label_name not in label_2_index:\n",
    "            continue\n",
    "\n",
    "        mel = audio_to_mel_spectrogram(Path(wav_path))   # shape [80, T]\n",
    "        x = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)   # shape [1, 80, T]\n",
    "\n",
    "\n",
    "        scores = tcav.interpret(\n",
    "            inputs=x,\n",
    "            experimental_sets=[[c, rnd] for c in pos],\n",
    "            target=label_2_index[label_name]\n",
    "        )\n",
    "\n",
    "        output[wav_path] = scores\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "becd5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tcav_dict_per_sample_to_df(tcav_raw, scores_by_sample, concept_names):\n",
    "    rows = []\n",
    "\n",
    "    for wav_path, exp_sets in scores_by_sample.items():\n",
    "        for key, layer_dict in exp_sets.items():\n",
    "            pos_id = int(key.split(\"-\")[0])\n",
    "            concept_name = concept_names[pos_id]\n",
    "\n",
    "            for layer_name, metrics in layer_dict.items():\n",
    "                sc = metrics[\"sign_count\"]\n",
    "                mg = metrics[\"magnitude\"]\n",
    "\n",
    "                sc = sc[0].item() if isinstance(sc, torch.Tensor) else sc[0]\n",
    "                mg = mg[0].item() if isinstance(mg, torch.Tensor) else mg[0]\n",
    "\n",
    "                rows.append({\n",
    "                    \"path\": wav_path,\n",
    "                    \"concept name\": concept_name,\n",
    "                    \"layer name\": layer_name,\n",
    "                    \"positive percentage\": sc,\n",
    "                    \"magnitude\": mg\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ca9bad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tcav_per_sample(attribute_csv, model_path, label_2_index, target_layer_path=None):\n",
    "    df = pd.read_csv(attribute_csv)\n",
    "\n",
    "    df.drop(columns=df.filter(regex=\"^prob \").columns, inplace=True)\n",
    "\n",
    "    tcav_raw = init_tcav_with_pamalia_dict(model_path=model_path)\n",
    "\n",
    "    scores = _get_tcav_dict_per_sample(tcav_raw, df, model_path, label_2_index)\n",
    "\n",
    "    df_tcav = _tcav_dict_per_sample_to_df(tcav_raw, scores, CONCEPT_UNIQUE_NAMES)\n",
    "\n",
    "    df_merged = df_tcav.merge(df, on=\"path\", how=\"left\")\n",
    "\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0e55ca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\utils\\checkpoints.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(path, map_location=device), strict=False\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\processing\\features.py:1218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "c:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\concept\\_utils\\classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[1, 1, 80, 680], n_fft=400, hop_length=160, win_length=400, window=torch.FloatTensor{[400]}, normalized=0, onesided=1, return_complex=0) : expected a 1D or 2D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create output directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m      6\u001b[0m OUTPUT\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m df_result \u001b[38;5;241m=\u001b[39m \u001b[43mget_tcav_per_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mATTRIBUTE_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_2_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_2_index\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m df_result\u001b[38;5;241m.\u001b[39mto_csv(OUTPUT, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m df_result\n",
      "Cell \u001b[1;32mIn[211], line 8\u001b[0m, in \u001b[0;36mget_tcav_per_sample\u001b[1;34m(attribute_csv, model_path, label_2_index, target_layer_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^prob \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m tcav_raw \u001b[38;5;241m=\u001b[39m init_tcav_with_pamalia_dict(model_path\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m----> 8\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tcav_dict_per_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtcav_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_2_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df_tcav \u001b[38;5;241m=\u001b[39m _tcav_dict_per_sample_to_df(tcav_raw, scores, CONCEPT_UNIQUE_NAMES)\n\u001b[0;32m     12\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m df_tcav\u001b[38;5;241m.\u001b[39mmerge(df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[209], line 19\u001b[0m, in \u001b[0;36m_get_tcav_dict_per_sample\u001b[1;34m(tcav_raw, df, model_path, label_2_index)\u001b[0m\n\u001b[0;32m     15\u001b[0m     mel \u001b[38;5;241m=\u001b[39m audio_to_mel_spectrogram(Path(wav_path))   \u001b[38;5;66;03m# shape [80, T]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mel, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# shape [1, 80, T]\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mtcav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperimental_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_2_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     output[wav_path] \u001b[38;5;241m=\u001b[39m scores\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\concept\\_core\\tcav.py:662\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[1;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03mThis method computes magnitude and sign-based TCAV scores for each\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03mexperimental sets in `experimental_sets` list.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    655\u001b[0m \n\u001b[0;32m    656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_to_layer_input\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs, (\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, set `attribute_to_layer_input` flag as a constructor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument to TCAV class. In that case it will be applied \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsistently to both layer activation and layer attribution methods.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    661\u001b[0m )\n\u001b[1;32m--> 662\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cavs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperimental_sets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocesses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m scores: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]]] \u001b[38;5;241m=\u001b[39m defaultdict(\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict()\n\u001b[0;32m    666\u001b[0m )\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# Retrieves the lengths of the experimental sets so that we can sort\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# them by the length and compute TCAV scores in batches.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\concept\\_core\\tcav.py:519\u001b[0m, in \u001b[0;36mTCAV.compute_cavs\u001b[1;34m(self, experimental_sets, force_train, processes)\u001b[0m\n\u001b[0;32m    517\u001b[0m     concept_key_to_layers[concepts_key] \u001b[38;5;241m=\u001b[39m layers\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# Generate activations for missing (concept, layers)\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcept_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m     concept_key_to_layers[concepts_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\concept\\_core\\tcav.py:393\u001b[0m, in \u001b[0;36mTCAV.generate_activations\u001b[1;34m(self, concept_layers)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03mComputes layer activations for the concepts and layers specified in\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m`concept_layers` dictionary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m            {\"striped\": ['inception4c', 'inception4d']}\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m concept_layers:\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcept_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconcept\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcept\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\concept\\_core\\tcav.py:365\u001b[0m, in \u001b[0;36mTCAV.generate_activation\u001b[1;34m(self, layers, concept)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m concept\u001b[38;5;241m.\u001b[39mdata_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData iterator for concept id:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(concept\u001b[38;5;241m.\u001b[39mid),\n\u001b[0;32m    363\u001b[0m )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, examples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(concept\u001b[38;5;241m.\u001b[39mdata_iter):\n\u001b[1;32m--> 365\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_act\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m activation, layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(activations, layers):\n\u001b[0;32m    371\u001b[0m         activation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(activation, (activation\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\attr\\_core\\layer\\layer_activation.py:118\u001b[0m, in \u001b[0;36mLayerActivation.attribute\u001b[1;34m(self, inputs, additional_forward_args, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    >>> attribution = layer_act.attribute(input)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 118\u001b[0m     layer_eval \u001b[38;5;241m=\u001b[39m \u001b[43m_forward_layer_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer, Module):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _format_output(\u001b[38;5;28mlen\u001b[39m(layer_eval) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m, layer_eval)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\_utils\\gradient.py:182\u001b[0m, in \u001b[0;36m_forward_layer_eval\u001b[1;34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_layer_eval\u001b[39m(\n\u001b[0;32m    174\u001b[0m     forward_fn: Callable,\n\u001b[0;32m    175\u001b[0m     inputs: Union[Tensor, Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m     grad_enabled: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], List[Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_forward_layer_eval_with_neuron_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_neuron_selector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\_utils\\gradient.py:445\u001b[0m, in \u001b[0;36m_forward_layer_eval_with_neuron_grads\u001b[1;34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    442\u001b[0m grad_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gradient_neuron_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_enabled\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(grad_enabled):\n\u001b[1;32m--> 445\u001b[0m     saved_layer \u001b[38;5;241m=\u001b[39m \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m device_ids \u001b[38;5;241m=\u001b[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\_utils\\gradient.py:294\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[1;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m             all_hooks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    292\u001b[0m                 single_layer\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001b[0;32m    293\u001b[0m             )\n\u001b[1;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m all_hooks:\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\pretrained\\interfaces.py:1019\u001b[0m, in \u001b[0;36mEncoderClassifier.forward\u001b[1;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavs, wav_lens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the classification\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\pretrained\\interfaces.py:981\u001b[0m, in \u001b[0;36mEncoderClassifier.classify_batch\u001b[1;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclassify_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavs, wav_lens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    953\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs classification on the top of the encoded features.\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \n\u001b[0;32m    955\u001b[0m \u001b[38;5;124;03m    It returns the posterior probabilities, the index and, if the label\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;124;03m        (label encoder should be provided).\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     out_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mclassifier(emb)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    983\u001b[0m     score, index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out_prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\pretrained\\interfaces.py:943\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[1;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[0;32m    940\u001b[0m wavs \u001b[38;5;241m=\u001b[39m wavs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    942\u001b[0m \u001b[38;5;66;03m# Computing features and embeddings\u001b[39;00m\n\u001b[1;32m--> 943\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[0;32m    945\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39membedding_model(feats, wav_lens)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\lobes\\features.py:138\u001b[0m, in \u001b[0;36mFbank.forward\u001b[1;34m(self, wav)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wav):\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a set of features generated from the input waveforms.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m        A batch of audio signals to transform to features.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     STFT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_STFT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     mag \u001b[38;5;241m=\u001b[39m spectral_magnitude(STFT)\n\u001b[0;32m    140\u001b[0m     fbanks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_fbanks(mag)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\speechbrain\\processing\\features.py:147\u001b[0m, in \u001b[0;36mSTFT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    145\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(or_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m or_shape[\u001b[38;5;241m2\u001b[39m], or_shape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 147\u001b[0m stft \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_stft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Retrieving the original dimensionality (batch,time, channels)\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(or_shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yoav1\\OneDrive\\Desktop\\לימודים\\year4\\final\\BioVoice\\venv\\lib\\site-packages\\torch\\functional.py:666\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stft(torch.FloatTensor[1, 1, 80, 680], n_fft=400, hop_length=160, win_length=400, window=torch.FloatTensor{[400]}, normalized=0, onesided=1, return_complex=0) : expected a 1D or 2D tensor"
     ]
    }
   ],
   "source": [
    "ATTRIBUTE_CSV = Path(\"./user_final_predictions.csv\")\n",
    "MODEL_PATH = Path(\"../ecapa_pretrained\")  # SpeechBrain format directory\n",
    "OUTPUT = Path(\"./outputs/tcav_per_sample_output.csv\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_result = get_tcav_per_sample(\n",
    "    attribute_csv=ATTRIBUTE_CSV,\n",
    "    model_path=MODEL_PATH,\n",
    "    label_2_index=label_2_index\n",
    ")\n",
    "\n",
    "df_result.to_csv(OUTPUT, index=False)\n",
    "df_result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
