{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ASVspoof5 100-Speakers Plan + Selective Extraction (train/val/test)\n",
    "\n",
    "Design in this notebook:\n",
    "\n",
    "- `100` speakers total\n",
    "- split groups: `train/val/test = 60/20/20`\n",
    "- partition mix per split:\n",
    "  - `train`: 15(train partition) / 15(dev partition) / 30(eval partition)\n",
    "  - `val`: 5 / 5 / 10\n",
    "  - `test`: 5 / 5 / 10\n",
    "- per speaker quota: `32 bonafide + 48 spoof`\n",
    "- spoof quota by partition:\n",
    "  - `train` partition (`A01-A08`): `6` per system\n",
    "  - `dev` partition (`A09-A16`): `6` per system\n",
    "  - `eval` partition (`A17-A32`): `3` per system\n",
    "\n",
    "This notebook does two things:\n",
    "1. Builds the selection manifest CSV used by the 100-speaker logistic notebooks\n",
    "2. Selectively extracts only needed audio from tar shards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "OUT_DIR = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_protocols/subset_100_speakers_outputs\n",
      "NOTEBOOK_DIR = /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers\n",
      "MANIFEST_OUT = /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers/asvspoof5_100_speakers_selected_utterances_plan.csv\n",
      "EXTRACT_DIR = /home/SpeakerRec/BioVoice/data/datasets/asvspoof5_100_speakers_32_real_48_spoof\n",
      "PROTOCOL[train] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_protocols/ASVspoof5.train.tsv | exists = True\n",
      "PROTOCOL[dev] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_protocols/ASVspoof5.dev.track_1.tsv | exists = True\n",
      "PROTOCOL[eval] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_protocols/ASVspoof5.eval.track_1.tsv | exists = True\n",
      "TAR_DIR[train] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_audio_train_tars | exists = True\n",
      "TAR_DIR[dev] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_audio_dev_tars | exists = True\n",
      "TAR_DIR[eval] = /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_audio_eval_tars | exists = True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _find_project_root_from_cwd() -> Path | None:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for cand in [cwd, *cwd.parents]:\n",
    "        if (cand / 'ASVspoof5_protocols').exists() and (cand / 'redimnet').exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "_detected_root = _find_project_root_from_cwd()\n",
    "PROJECT_ROOT = _detected_root if _detected_root is not None else Path('/home/SpeakerRec/BioVoice')\n",
    "DATA_PATH = (\n",
    "    _detected_root\n",
    "    if _detected_root is not None\n",
    "    else Path(\"/home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars\")\n",
    ")\n",
    "\n",
    "PROTOCOL_PATHS = {\n",
    "    'train': DATA_PATH / 'ASVspoof5_protocols' / 'ASVspoof5.train.tsv',\n",
    "    'dev': DATA_PATH / 'ASVspoof5_protocols' / 'ASVspoof5.dev.track_1.tsv',\n",
    "    'eval': DATA_PATH / 'ASVspoof5_protocols' / 'ASVspoof5.eval.track_1.tsv',\n",
    "}\n",
    "\n",
    "# Adjust these to your local tar folders if needed.\n",
    "TAR_DIRS = {\n",
    "    'train': DATA_PATH / 'ASVspoof5_audio_train_tars',\n",
    "    'dev': DATA_PATH / 'ASVspoof5_audio_dev_tars',\n",
    "    'eval': DATA_PATH / 'ASVspoof5_audio_eval_tars',\n",
    "}\n",
    "\n",
    "OUT_DIR = DATA_PATH / \"ASVspoof5_protocols\" / \"subset_100_speakers_outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR = PROJECT_ROOT / 'redimnet' / 'logistic_regression' / 'asvspoof5' / '100_speakers'\n",
    "NOTEBOOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selected_utterances_plan.csv'\n",
    "SPEAKER_PLAN_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selected_speakers_plan.csv'\n",
    "AUDIT_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selection_audit.csv'\n",
    "SUMMARY_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_plan_summary.json'\n",
    "\n",
    "EXTRACT_DIR = PROJECT_ROOT / 'data' / 'datasets' / 'asvspoof5_100_speakers_32_real_48_spoof'\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "BONA_PER_SPK = 32\n",
    "\n",
    "GROUP_PARTITION_SPEAKERS = {\n",
    "    'train': {'train': 15, 'dev': 15, 'eval': 30},\n",
    "    'val':   {'train': 5,  'dev': 5,  'eval': 10},\n",
    "    'test':  {'train': 5,  'dev': 5,  'eval': 10},\n",
    "}\n",
    "\n",
    "SYSTEMS_BY_PARTITION = {\n",
    "    'train': [f'A{i:02d}' for i in range(1, 9)],\n",
    "    'dev': [f'A{i:02d}' for i in range(9, 17)],\n",
    "    'eval': [f'A{i:02d}' for i in range(17, 33)],\n",
    "}\n",
    "\n",
    "SPOOF_QUOTA_BY_PARTITION = {\n",
    "    'train': 6,\n",
    "    'dev': 6,\n",
    "    'eval': 3,\n",
    "}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for lbl in ['bonafide', 'spoof']:\n",
    "        (EXTRACT_DIR / split / lbl).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('PROJECT_ROOT =', PROJECT_ROOT)\n",
    "print('OUT_DIR =', OUT_DIR)\n",
    "print('NOTEBOOK_DIR =', NOTEBOOK_DIR)\n",
    "print('MANIFEST_OUT =', MANIFEST_OUT)\n",
    "print('EXTRACT_DIR =', EXTRACT_DIR)\n",
    "for part, p in PROTOCOL_PATHS.items():\n",
    "    print(f'PROTOCOL[{part}] =', p, '| exists =', p.exists())\n",
    "for part, p in TAR_DIRS.items():\n",
    "    print(f'TAR_DIR[{part}] =', p, '| exists =', p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1004081\n",
      "Total unique speakers: 1922\n",
      "partition\n",
      "dev      785\n",
      "eval     737\n",
      "train    400\n",
      "Name: speakers, dtype: int64\n",
      "Label counts by partition:\n",
      "label      bonafide   spoof\n",
      "partition                  \n",
      "dev           31334  109616\n",
      "eval         138688  542086\n",
      "train         18797  163560\n",
      "Spoof systems by partition:\n",
      "train ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08']\n",
      "dev ['A09', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16']\n",
      "eval ['A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols = ['speaker_id','utt_id','gender','codec_id','codec_q','source_utt_id','attack_codec_id','system_id','label','unused']\n",
    "\n",
    "\n",
    "def load_protocol(partition: str, path: Path) -> pd.DataFrame:\n",
    "    assert path.exists(), f'Missing protocol file: {path}'\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            t = line.strip().split()\n",
    "            if len(t) != 10:\n",
    "                continue\n",
    "            rows.append(dict(zip(cols, t)))\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['partition'] = partition\n",
    "    return df\n",
    "\n",
    "all_parts = []\n",
    "for part in ['train', 'dev', 'eval']:\n",
    "    d = load_protocol(part, PROTOCOL_PATHS[part])\n",
    "    all_parts.append(d)\n",
    "\n",
    "df = pd.concat(all_parts, ignore_index=True)\n",
    "\n",
    "print('Total rows:', len(df))\n",
    "print('Total unique speakers:', df['speaker_id'].nunique())\n",
    "print(df.groupby('partition')['speaker_id'].nunique().rename('speakers'))\n",
    "print('Label counts by partition:')\n",
    "print(df.groupby(['partition','label']).size().unstack(fill_value=0))\n",
    "print('Spoof systems by partition:')\n",
    "for part in ['train', 'dev', 'eval']:\n",
    "    systems = sorted(df[(df['partition']==part) & (df['label']=='spoof')]['system_id'].unique().tolist())\n",
    "    print(part, systems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required speakers by partition: {'train': 25, 'dev': 25, 'eval': 50}\n",
      "[train] eligible speakers: 367 | need: 25\n",
      "[dev] eligible speakers: 330 | need: 25\n",
      "[eval] eligible speakers: 367 | need: 50\n",
      "Selected speakers total: 100\n",
      "By group: {'test': 20, 'train': 60, 'val': 20}\n",
      "By partition: {'dev': 25, 'eval': 50, 'train': 25}\n",
      "By group+partition:\n",
      "partition  dev  eval  train\n",
      "group                      \n",
      "test         5    10      5\n",
      "train       15    30     15\n",
      "val          5    10      5\n",
      "Gender by group:\n",
      "gender   F   M\n",
      "group         \n",
      "test    12   8\n",
      "train   29  31\n",
      "val      8  12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>partition</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>bonafide_n</th>\n",
       "      <th>min_system_n</th>\n",
       "      <th>n_systems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_1612</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_3545</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_4485</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_5388</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_0820</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>86.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_0829</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>68.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_1029</td>\n",
       "      <td>F</td>\n",
       "      <td>124</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_2068</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>74.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_2803</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>90.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_3440</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>86.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_3750</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_4512</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>96.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_4954</td>\n",
       "      <td>F</td>\n",
       "      <td>200</td>\n",
       "      <td>98.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test</td>\n",
       "      <td>eval</td>\n",
       "      <td>E_5193</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>86.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>T_0696</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>T_3526</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>T_3839</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>T_4231</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test</td>\n",
       "      <td>train</td>\n",
       "      <td>T_5051</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group partition speaker_id gender  bonafide_n  min_system_n  n_systems\n",
       "0   test       dev     D_0913      F          50          43.0          8\n",
       "1   test       dev     D_1612      F          50          29.0          8\n",
       "2   test       dev     D_3545      M          50          32.0          8\n",
       "3   test       dev     D_4485      M          50          25.0          8\n",
       "4   test       dev     D_5388      F          50          40.0          8\n",
       "5   test      eval     E_0820      F         200          86.0         16\n",
       "6   test      eval     E_0829      M         200          68.0         16\n",
       "7   test      eval     E_1029      F         124          32.0         16\n",
       "8   test      eval     E_2068      F         200          74.0         16\n",
       "9   test      eval     E_2803      F         200          90.0         16\n",
       "10  test      eval     E_3440      M         200          86.0         16\n",
       "11  test      eval     E_3750      F         200          58.0         16\n",
       "12  test      eval     E_4512      F         200          96.0         16\n",
       "13  test      eval     E_4954      F         200          98.0         16\n",
       "14  test      eval     E_5193      M         200          86.0         16\n",
       "15  test     train     T_0696      F          50          30.0          8\n",
       "16  test     train     T_3526      M          50          60.0          8\n",
       "17  test     train     T_3839      M          50          60.0          8\n",
       "18  test     train     T_4231      F          50          60.0          8\n",
       "19  test     train     T_5051      M          50          60.0          8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sample_gender_balanced(eligible_df: pd.DataFrame, n_pick: int, seed: int) -> pd.DataFrame:\n",
    "    eligible_df = eligible_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    f = eligible_df[eligible_df['gender'].eq('F')].copy()\n",
    "    m = eligible_df[eligible_df['gender'].eq('M')].copy()\n",
    "\n",
    "    target_f = n_pick // 2\n",
    "    target_m = n_pick - target_f\n",
    "\n",
    "    if len(f) < target_f:\n",
    "        target_f = len(f)\n",
    "        target_m = n_pick - target_f\n",
    "    if len(m) < target_m:\n",
    "        target_m = len(m)\n",
    "        target_f = n_pick - target_m\n",
    "\n",
    "    picked = pd.concat([f.head(target_f), m.head(target_m)], ignore_index=True)\n",
    "    if len(picked) < n_pick:\n",
    "        used = set(picked['speaker_id'])\n",
    "        extra = eligible_df[~eligible_df['speaker_id'].isin(used)].head(n_pick - len(picked))\n",
    "        picked = pd.concat([picked, extra], ignore_index=True)\n",
    "\n",
    "    return picked.drop_duplicates('speaker_id').head(n_pick).copy()\n",
    "\n",
    "\n",
    "required_by_partition = {\n",
    "    part: sum(GROUP_PARTITION_SPEAKERS[g][part] for g in ['train','val','test'])\n",
    "    for part in ['train','dev','eval']\n",
    "}\n",
    "print('Required speakers by partition:', required_by_partition)\n",
    "\n",
    "selected_rows = []\n",
    "\n",
    "for part in ['train','dev','eval']:\n",
    "    dfp = df[df['partition'].eq(part)].copy()\n",
    "    systems = SYSTEMS_BY_PARTITION[part]\n",
    "    spoof_q = SPOOF_QUOTA_BY_PARTITION[part]\n",
    "\n",
    "    bona_counts = dfp[dfp['label'].eq('bonafide')].groupby('speaker_id').size().rename('bonafide_n')\n",
    "    spoof_piv = (\n",
    "        dfp[dfp['label'].eq('spoof')]\n",
    "        .groupby(['speaker_id','system_id']).size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    for s in systems:\n",
    "        if s not in spoof_piv.columns:\n",
    "            spoof_piv[s] = 0\n",
    "    spoof_piv = spoof_piv[systems]\n",
    "\n",
    "    gender_ser = dfp.groupby('speaker_id')['gender'].agg(lambda x: x.iloc[0]).rename('gender')\n",
    "    summary = pd.concat([bona_counts, spoof_piv, gender_ser], axis=1).fillna(0)\n",
    "    summary['n_systems'] = (summary[systems] > 0).sum(axis=1)\n",
    "    summary['min_system_n'] = summary[systems].min(axis=1)\n",
    "\n",
    "    eligible = summary[\n",
    "        (summary['bonafide_n'] >= BONA_PER_SPK) &\n",
    "        (summary['n_systems'] == len(systems)) &\n",
    "        (summary['min_system_n'] >= spoof_q)\n",
    "    ].reset_index()\n",
    "\n",
    "    need = required_by_partition[part]\n",
    "    print(f'[{part}] eligible speakers:', len(eligible), '| need:', need)\n",
    "    assert len(eligible) >= need, f'Not enough eligible speakers in {part}'\n",
    "\n",
    "    picked = sample_gender_balanced(eligible, need, seed=SEED + {'train': 11, 'dev': 22, 'eval': 33}[part]).copy()\n",
    "    picked['partition'] = part\n",
    "\n",
    "    # assign split groups inside each partition according to plan\n",
    "    picked = picked.sample(frac=1.0, random_state=SEED + {'train': 101, 'dev': 202, 'eval': 303}[part]).reset_index(drop=True)\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    for g in ['train','val','test']:\n",
    "        k = GROUP_PARTITION_SPEAKERS[g][part]\n",
    "        chunk = picked.iloc[start:start+k].copy()\n",
    "        chunk['group'] = g\n",
    "        chunks.append(chunk)\n",
    "        start += k\n",
    "    part_sel = pd.concat(chunks, ignore_index=True)\n",
    "    selected_rows.append(part_sel)\n",
    "\n",
    "sel = pd.concat(selected_rows, ignore_index=True)\n",
    "sel = sel[['group','partition','speaker_id','gender','bonafide_n','min_system_n','n_systems']]\n",
    "sel = sel.sort_values(['group','partition','speaker_id']).reset_index(drop=True)\n",
    "\n",
    "print('Selected speakers total:', sel['speaker_id'].nunique())\n",
    "print('By group:', sel['group'].value_counts().sort_index().to_dict())\n",
    "print('By partition:', sel['partition'].value_counts().sort_index().to_dict())\n",
    "print('By group+partition:')\n",
    "print(sel.groupby(['group','partition']).size().unstack(fill_value=0))\n",
    "print('Gender by group:')\n",
    "print(sel.groupby(['group','gender']).size().unstack(fill_value=0))\n",
    "\n",
    "display(sel.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers/asvspoof5_100_speakers_selected_utterances_plan.csv\n",
      "Saved: /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers/asvspoof5_100_speakers_selected_speakers_plan.csv\n",
      "Saved: /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers/asvspoof5_100_speakers_selection_audit.csv\n",
      "Saved: /home/SpeakerRec/BioVoice/redimnet/logistic_regression/asvspoof5/100_speakers/asvspoof5_100_speakers_plan_summary.json\n",
      "Rows: 8000 | Speakers: 100\n",
      "By group: {'test': 1600, 'train': 4800, 'val': 1600}\n",
      "By class: {'bonafide': 3200, 'spoof': 4800}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_rows = []\n",
    "audit_rows = []\n",
    "\n",
    "for idx, r in sel.reset_index(drop=True).iterrows():\n",
    "    split_group = r['group']\n",
    "    part = r['partition']\n",
    "    spk = r['speaker_id']\n",
    "    systems = SYSTEMS_BY_PARTITION[part]\n",
    "    spoof_q = SPOOF_QUOTA_BY_PARTITION[part]\n",
    "\n",
    "    pool_spk = df[(df['partition'].eq(part)) & (df['speaker_id'].eq(spk))].copy()\n",
    "\n",
    "    bona_pool = pool_spk[pool_spk['label'].eq('bonafide')].copy()\n",
    "    bona_pick = bona_pool.sample(n=BONA_PER_SPK, random_state=SEED + 5000 + idx)\n",
    "    bona_pick = bona_pick.copy()\n",
    "    bona_pick['group'] = split_group\n",
    "    bona_pick['selected_reason'] = 'bonafide_quota'\n",
    "    selected_rows.append(bona_pick)\n",
    "    audit_rows.append({\n",
    "        'group': split_group,\n",
    "        'partition': part,\n",
    "        'speaker_id': spk,\n",
    "        'label': 'bonafide',\n",
    "        'system_id': 'bonafide',\n",
    "        'target_n': BONA_PER_SPK,\n",
    "        'selected_n': len(bona_pick),\n",
    "        'availability_n': len(bona_pool),\n",
    "    })\n",
    "\n",
    "    for j, sysid in enumerate(systems):\n",
    "        pool = pool_spk[(pool_spk['label'].eq('spoof')) & (pool_spk['system_id'].eq(sysid))].copy()\n",
    "        pick = pool.sample(n=spoof_q, random_state=SEED + 7000 + idx * 100 + j)\n",
    "        pick = pick.copy()\n",
    "        pick['group'] = split_group\n",
    "        pick['selected_reason'] = 'spoof_system_quota'\n",
    "        selected_rows.append(pick)\n",
    "        audit_rows.append({\n",
    "            'group': split_group,\n",
    "            'partition': part,\n",
    "            'speaker_id': spk,\n",
    "            'label': 'spoof',\n",
    "            'system_id': sysid,\n",
    "            'target_n': spoof_q,\n",
    "            'selected_n': len(pick),\n",
    "            'availability_n': len(pool),\n",
    "        })\n",
    "\n",
    "manifest = pd.concat(selected_rows, ignore_index=True)\n",
    "manifest = manifest[['group','partition','speaker_id','utt_id','gender','label','system_id','codec_id','codec_q','source_utt_id','attack_codec_id','selected_reason']]\n",
    "manifest = manifest.sort_values(['group','partition','speaker_id','label','system_id','utt_id']).reset_index(drop=True)\n",
    "\n",
    "audit_df = pd.DataFrame(audit_rows)\n",
    "\n",
    "# Sanity checks\n",
    "assert manifest['speaker_id'].nunique() == 100\n",
    "assert len(manifest) == 100 * 80\n",
    "assert manifest['utt_id'].nunique() == len(manifest)\n",
    "\n",
    "per_spk = manifest.groupby(['group','partition','speaker_id','label']).size().unstack(fill_value=0)\n",
    "assert (per_spk['bonafide'] == 32).all()\n",
    "assert (per_spk['spoof'] == 48).all()\n",
    "assert (audit_df['selected_n'] == audit_df['target_n']).all()\n",
    "\n",
    "manifest.to_csv(MANIFEST_OUT, index=False)\n",
    "sel.to_csv(SPEAKER_PLAN_OUT, index=False)\n",
    "audit_df.to_csv(AUDIT_OUT, index=False)\n",
    "\n",
    "summary = {\n",
    "    'seed': SEED,\n",
    "    'design': {\n",
    "        'n_speakers_total': 100,\n",
    "        'group_partition_speakers': GROUP_PARTITION_SPEAKERS,\n",
    "        'bonafide_per_speaker': BONA_PER_SPK,\n",
    "        'spoof_quota_by_partition': SPOOF_QUOTA_BY_PARTITION,\n",
    "        'systems_by_partition': SYSTEMS_BY_PARTITION,\n",
    "    },\n",
    "    'selected_speakers': int(sel['speaker_id'].nunique()),\n",
    "    'selected_utterances_total': int(len(manifest)),\n",
    "    'class_counts': {k:int(v) for k,v in manifest['label'].value_counts().sort_index().to_dict().items()},\n",
    "    'group_counts': {k:int(v) for k,v in manifest['group'].value_counts().sort_index().to_dict().items()},\n",
    "    'group_partition_counts': {\n",
    "        g: {p: int(v) for p, v in sel[sel['group'].eq(g)]['partition'].value_counts().sort_index().to_dict().items()}\n",
    "        for g in ['train','val','test']\n",
    "    },\n",
    "    'spoof_counts_by_system': {k:int(v) for k,v in manifest[manifest['label'].eq('spoof')]['system_id'].value_counts().sort_index().to_dict().items()},\n",
    "}\n",
    "SUMMARY_OUT.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "\n",
    "# Convenience copies in OUT_DIR\n",
    "manifest.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selected_utterances_plan.csv', index=False)\n",
    "sel.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selected_speakers_plan.csv', index=False)\n",
    "audit_df.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selection_audit.csv', index=False)\n",
    "(OUT_DIR / 'asvspoof5_100_speakers_plan_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "\n",
    "print('Saved:', MANIFEST_OUT)\n",
    "print('Saved:', SPEAKER_PLAN_OUT)\n",
    "print('Saved:', AUDIT_OUT)\n",
    "print('Saved:', SUMMARY_OUT)\n",
    "print('Rows:', len(manifest), '| Speakers:', manifest['speaker_id'].nunique())\n",
    "print('By group:', manifest['group'].value_counts().sort_index().to_dict())\n",
    "print('By class:', manifest['label'].value_counts().sort_index().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] tar files: 5\n",
      " - flac_T_aa.tar\n",
      " - flac_T_ab.tar\n",
      " - flac_T_ac.tar\n",
      " - flac_T_ad.tar\n",
      " - flac_T_ae.tar\n",
      "[dev] tar files: 3\n",
      " - flac_D_aa.tar\n",
      " - flac_D_ab.tar\n",
      " - flac_D_ac.tar\n",
      "[eval] tar files: 10\n",
      " - flac_E_aa.tar\n",
      " - flac_E_ab.tar\n",
      " - flac_E_ac.tar\n",
      " - flac_E_ad.tar\n",
      " - flac_E_ae.tar\n",
      " - flac_E_af.tar\n",
      " - flac_E_ag.tar\n",
      " - flac_E_ah.tar\n",
      " - flac_E_ai.tar\n",
      " - flac_E_aj.tar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect tar shards by partition\n",
    "\n",
    "partition_tars = {}\n",
    "for part, tar_dir in TAR_DIRS.items():\n",
    "    assert tar_dir.exists(), f'Missing TAR_DIR[{part}]: {tar_dir}'\n",
    "    tars = sorted(tar_dir.glob('flac_*.tar'))\n",
    "    partition_tars[part] = tars\n",
    "    print(f'[{part}] tar files:', len(tars))\n",
    "    for p in tars[:10]:\n",
    "        print(' -', p.name)\n",
    "    assert len(tars) > 0, f'No tar files found for partition {part} in {tar_dir}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2000, 'dev': 2000, 'eval': 4000}\n",
      "Indexing [train] flac_T_aa.tar\n",
      "  found so far [train]: 407 / 2000\n",
      "Indexing [train] flac_T_ab.tar\n",
      "  found so far [train]: 808 / 2000\n",
      "Indexing [train] flac_T_ac.tar\n",
      "  found so far [train]: 1211 / 2000\n",
      "Indexing [train] flac_T_ad.tar\n",
      "  found so far [train]: 1627 / 2000\n",
      "Indexing [train] flac_T_ae.tar\n",
      "  found so far [train]: 2000 / 2000\n",
      "Indexing [dev] flac_D_aa.tar\n",
      "  found so far [dev]: 680 / 2000\n",
      "Indexing [dev] flac_D_ab.tar\n",
      "  found so far [dev]: 1343 / 2000\n",
      "Indexing [dev] flac_D_ac.tar\n",
      "  found so far [dev]: 2000 / 2000\n",
      "Indexing [eval] flac_E_aa.tar\n",
      "  found so far [eval]: 392 / 4000\n",
      "Indexing [eval] flac_E_ab.tar\n",
      "  found so far [eval]: 763 / 4000\n",
      "Indexing [eval] flac_E_ac.tar\n",
      "  found so far [eval]: 1200 / 4000\n",
      "Indexing [eval] flac_E_ad.tar\n",
      "  found so far [eval]: 1602 / 4000\n",
      "Indexing [eval] flac_E_ae.tar\n",
      "  found so far [eval]: 2002 / 4000\n",
      "Indexing [eval] flac_E_af.tar\n",
      "  found so far [eval]: 2403 / 4000\n",
      "Indexing [eval] flac_E_ag.tar\n",
      "  found so far [eval]: 2808 / 4000\n",
      "Indexing [eval] flac_E_ah.tar\n",
      "  found so far [eval]: 3200 / 4000\n",
      "Indexing [eval] flac_E_ai.tar\n",
      "  found so far [eval]: 3586 / 4000\n",
      "Indexing [eval] flac_E_aj.tar\n",
      "  found so far [eval]: 4000 / 4000\n",
      "Mapped rows: 8000 / 8000\n",
      "Unmatched rows: 0\n",
      "Ambiguous utt_ids: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build tar member index for selected utt_ids\n",
    "manifest = pd.read_csv(MANIFEST_OUT)\n",
    "needed_by_part = {\n",
    "    part: set(manifest[manifest['partition'].eq(part)]['utt_id'].astype(str).tolist())\n",
    "    for part in ['train','dev','eval']\n",
    "}\n",
    "print({k: len(v) for k, v in needed_by_part.items()})\n",
    "\n",
    "index_rows = []\n",
    "found_by_part = {k: set() for k in ['train','dev','eval']}\n",
    "\n",
    "for part in ['train','dev','eval']:\n",
    "    needed = needed_by_part[part]\n",
    "    for tar_path in partition_tars[part]:\n",
    "        print(f'Indexing [{part}]', tar_path.name)\n",
    "        with tarfile.open(tar_path, 'r') as tf:\n",
    "            for m in tf.getmembers():\n",
    "                if not m.isfile():\n",
    "                    continue\n",
    "                stem = Path(Path(m.name).name).stem\n",
    "                if stem in needed:\n",
    "                    index_rows.append({\n",
    "                        'partition': part,\n",
    "                        'utt_id': stem,\n",
    "                        'tar_file': str(tar_path),\n",
    "                        'member_name': m.name,\n",
    "                        'member_size': int(m.size),\n",
    "                    })\n",
    "                    found_by_part[part].add(stem)\n",
    "        print(f'  found so far [{part}]:', len(found_by_part[part]), '/', len(needed))\n",
    "\n",
    "index_df = pd.DataFrame(index_rows)\n",
    "idx_counts = index_df.groupby('utt_id').size().reset_index(name='n_matches') if not index_df.empty else pd.DataFrame(columns=['utt_id','n_matches'])\n",
    "ambiguous = idx_counts[idx_counts['n_matches'] > 1].copy()\n",
    "if not index_df.empty:\n",
    "    index_df = index_df.sort_values(['partition','utt_id','tar_file','member_name']).drop_duplicates('utt_id', keep='first').reset_index(drop=True)\n",
    "\n",
    "mapped = manifest.merge(index_df.drop(columns=['partition']), on='utt_id', how='left')\n",
    "unmatched = mapped[mapped['tar_file'].isna()].copy()\n",
    "\n",
    "index_csv = OUT_DIR / 'asvspoof5_100_speakers_tar_member_index.csv'\n",
    "mapped_csv = OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv'\n",
    "unmatched_csv = OUT_DIR / 'asvspoof5_100_speakers_unmatched_utts.csv'\n",
    "ambiguous_csv = OUT_DIR / 'asvspoof5_100_speakers_ambiguous_utts.csv'\n",
    "\n",
    "index_df.to_csv(index_csv, index=False)\n",
    "mapped.to_csv(mapped_csv, index=False)\n",
    "unmatched.to_csv(unmatched_csv, index=False)\n",
    "ambiguous.to_csv(ambiguous_csv, index=False)\n",
    "\n",
    "print('Mapped rows:', mapped['tar_file'].notna().sum(), '/', len(mapped))\n",
    "print('Unmatched rows:', len(unmatched))\n",
    "print('Ambiguous utt_ids:', len(ambiguous))\n",
    "assert len(unmatched) == 0, 'Some selected utt_ids were not found in the provided tar files.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from flac_D_aa.tar | rows = 680\n",
      "Extracting from flac_D_ab.tar | rows = 663\n",
      "Extracting from flac_D_ac.tar | rows = 657\n",
      "Extracting from flac_E_aa.tar | rows = 392\n",
      "Extracting from flac_E_ab.tar | rows = 371\n",
      "Extracting from flac_E_ac.tar | rows = 437\n",
      "Extracting from flac_E_ad.tar | rows = 402\n",
      "Extracting from flac_E_ae.tar | rows = 400\n",
      "Extracting from flac_E_af.tar | rows = 401\n",
      "Extracting from flac_E_ag.tar | rows = 405\n",
      "Extracting from flac_E_ah.tar | rows = 392\n",
      "Extracting from flac_E_ai.tar | rows = 386\n",
      "Extracting from flac_E_aj.tar | rows = 414\n",
      "Extracting from flac_T_aa.tar | rows = 407\n",
      "Extracting from flac_T_ab.tar | rows = 401\n",
      "Extracting from flac_T_ac.tar | rows = 403\n",
      "Extracting from flac_T_ad.tar | rows = 416\n",
      "Extracting from flac_T_ae.tar | rows = 373\n",
      "Extraction complete.\n",
      "Extracted flac count: 8000\n",
      "Extracted wav count: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selective extraction to EXTRACT_DIR/{group}/{label}/{utt_id}.flac\n",
    "mapped = pd.read_csv(OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv')\n",
    "assert mapped['tar_file'].notna().all(), 'Run tar indexing cell first and resolve unmatched rows.'\n",
    "\n",
    "for tar_file, g in mapped.groupby('tar_file'):\n",
    "    tar_path = Path(tar_file)\n",
    "    print('Extracting from', tar_path.name, '| rows =', len(g))\n",
    "    lookup = {row.member_name: row for row in g.itertuples(index=False)}\n",
    "    with tarfile.open(tar_path, 'r') as tf:\n",
    "        for m in tf.getmembers():\n",
    "            if m.name not in lookup:\n",
    "                continue\n",
    "            row = lookup[m.name]\n",
    "            ext = Path(m.name).suffix or '.flac'\n",
    "            out_dir = EXTRACT_DIR / row.group / row.label\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = out_dir / f'{row.utt_id}{ext}'\n",
    "            if out_path.exists():\n",
    "                continue\n",
    "            fobj = tf.extractfile(m)\n",
    "            assert fobj is not None\n",
    "            with open(out_path, 'wb') as w:\n",
    "                w.write(fobj.read())\n",
    "\n",
    "print('Extraction complete.')\n",
    "print('Extracted flac count:', len(list(EXTRACT_DIR.rglob('*.flac'))))\n",
    "print('Extracted wav count:', len(list(EXTRACT_DIR.rglob('*.wav'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/SpeakerRec/BioVoice/data/datasets/ASVspoof5_tars/ASVspoof5_protocols/subset_100_speakers_outputs/asvspoof5_100_speakers_subset_manifest_with_local_paths.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>partition</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>utt_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "      <th>system_id</th>\n",
       "      <th>codec_id</th>\n",
       "      <th>codec_q</th>\n",
       "      <th>source_utt_id</th>\n",
       "      <th>attack_codec_id</th>\n",
       "      <th>selected_reason</th>\n",
       "      <th>tar_file</th>\n",
       "      <th>member_name</th>\n",
       "      <th>member_size</th>\n",
       "      <th>local_audio_path</th>\n",
       "      <th>label_binary_spoof_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000032257</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000032257.flac</td>\n",
       "      <td>179894</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000042883</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000042883.flac</td>\n",
       "      <td>88870</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000144943</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000144943.flac</td>\n",
       "      <td>85752</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000345661</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000345661.flac</td>\n",
       "      <td>182807</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000371743</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000371743.flac</td>\n",
       "      <td>94894</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000634579</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000634579.flac</td>\n",
       "      <td>173849</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000711985</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000711985.flac</td>\n",
       "      <td>77263</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000795019</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000795019.flac</td>\n",
       "      <td>168638</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000798547</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000798547.flac</td>\n",
       "      <td>87691</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>dev</td>\n",
       "      <td>D_0913</td>\n",
       "      <td>D_0000862954</td>\n",
       "      <td>F</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>bonafide</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bonafide_quota</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/ASVspo...</td>\n",
       "      <td>flac_D/D_0000862954.flac</td>\n",
       "      <td>148693</td>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/datasets/asvspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group partition speaker_id        utt_id gender     label system_id  \\\n",
       "0  test       dev     D_0913  D_0000032257      F  bonafide  bonafide   \n",
       "1  test       dev     D_0913  D_0000042883      F  bonafide  bonafide   \n",
       "2  test       dev     D_0913  D_0000144943      F  bonafide  bonafide   \n",
       "3  test       dev     D_0913  D_0000345661      F  bonafide  bonafide   \n",
       "4  test       dev     D_0913  D_0000371743      F  bonafide  bonafide   \n",
       "5  test       dev     D_0913  D_0000634579      F  bonafide  bonafide   \n",
       "6  test       dev     D_0913  D_0000711985      F  bonafide  bonafide   \n",
       "7  test       dev     D_0913  D_0000795019      F  bonafide  bonafide   \n",
       "8  test       dev     D_0913  D_0000798547      F  bonafide  bonafide   \n",
       "9  test       dev     D_0913  D_0000862954      F  bonafide  bonafide   \n",
       "\n",
       "  codec_id codec_q source_utt_id attack_codec_id selected_reason  \\\n",
       "0        -       -             -               -  bonafide_quota   \n",
       "1        -       -             -               -  bonafide_quota   \n",
       "2        -       -             -               -  bonafide_quota   \n",
       "3        -       -             -               -  bonafide_quota   \n",
       "4        -       -             -               -  bonafide_quota   \n",
       "5        -       -             -               -  bonafide_quota   \n",
       "6        -       -             -               -  bonafide_quota   \n",
       "7        -       -             -               -  bonafide_quota   \n",
       "8        -       -             -               -  bonafide_quota   \n",
       "9        -       -             -               -  bonafide_quota   \n",
       "\n",
       "                                            tar_file  \\\n",
       "0  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "1  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "2  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "3  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "4  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "5  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "6  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "7  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "8  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "9  /home/SpeakerRec/BioVoice/data/datasets/ASVspo...   \n",
       "\n",
       "                member_name  member_size  \\\n",
       "0  flac_D/D_0000032257.flac       179894   \n",
       "1  flac_D/D_0000042883.flac        88870   \n",
       "2  flac_D/D_0000144943.flac        85752   \n",
       "3  flac_D/D_0000345661.flac       182807   \n",
       "4  flac_D/D_0000371743.flac        94894   \n",
       "5  flac_D/D_0000634579.flac       173849   \n",
       "6  flac_D/D_0000711985.flac        77263   \n",
       "7  flac_D/D_0000795019.flac       168638   \n",
       "8  flac_D/D_0000798547.flac        87691   \n",
       "9  flac_D/D_0000862954.flac       148693   \n",
       "\n",
       "                                    local_audio_path  label_binary_spoof_1  \n",
       "0  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "1  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "2  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "3  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "4  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "5  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "6  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "7  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "8  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  \n",
       "9  /home/SpeakerRec/BioVoice/data/datasets/asvspo...                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Optional: write manifest with resolved local paths\n",
    "manifest = pd.read_csv(OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv')\n",
    "ext_guess = manifest['member_name'].apply(lambda s: Path(str(s)).suffix if pd.notna(s) else '.flac')\n",
    "manifest['local_audio_path'] = [str(EXTRACT_DIR / g / lbl / f'{u}{e}') for g,lbl,u,e in zip(manifest['group'], manifest['label'], manifest['utt_id'], ext_guess)]\n",
    "manifest['label_binary_spoof_1'] = (manifest['label'] == 'spoof').astype(int)\n",
    "\n",
    "out_csv = OUT_DIR / 'asvspoof5_100_speakers_subset_manifest_with_local_paths.csv'\n",
    "manifest.to_csv(out_csv, index=False)\n",
    "print('Saved:', out_csv)\n",
    "display(manifest.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
