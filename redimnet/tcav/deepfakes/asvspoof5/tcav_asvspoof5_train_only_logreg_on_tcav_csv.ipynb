{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# ASVspoof5 Train-Only Logistic Regression on TCAV CSV (Best-Practice Split)\n",
        "\n",
        "This notebook trains a logistic regression **on the TCAV CSV outputs** (concept features), using the fixed train-only subset design:\n",
        "\n",
        "- `A/B/C = 30/15/5` speakers (already selected)\n",
        "- Train logistic regression on `group == 'B'`\n",
        "- Test on `group == 'C'`\n",
        "- Keep `A` for optional inspection / later analysis\n",
        "\n",
        "Main goals:\n",
        "- Which concepts are important for `bonafide` vs `spoof`\n",
        "- Which concepts are important per user (`speaker_id`)\n",
        "- Which concepts are important for each spoof `system_id` (`A01-A08`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "PROJECT_ROOT = Path('/home/SpeakerRec/BioVoice')\n",
        "\n",
        "TCAV_CSV = PROJECT_ROOT / 'data' / 'tcav' / 'ASVspoof5_train_only_stage4_spoofwrapper' / 'ASVspoof5_train_only_stage4_spoofwrapper.csv'\n",
        "OUT_DIR = PROJECT_ROOT / 'data' / 'tcav' / 'ASVspoof5_train_only_logreg_on_tcav'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Feature options: choose one or both metrics from the TCAV CSV\n",
        "USE_METRICS = ['positive_percentage']   # options: ['positive_percentage'], ['magnitude'], ['positive_percentage','magnitude']\n",
        "\n",
        "# Fixed split by group (best practice for your design)\n",
        "TRAIN_GROUPS = ['B']\n",
        "TEST_GROUPS = ['C']\n",
        "OPTIONAL_HOLDOUT_GROUPS = ['A']\n",
        "\n",
        "print('TCAV_CSV =', TCAV_CSV)\n",
        "print('OUT_DIR =', OUT_DIR)\n",
        "print('USE_METRICS =', USE_METRICS)\n",
        "print('TRAIN_GROUPS =', TRAIN_GROUPS, '| TEST_GROUPS =', TEST_GROUPS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load TCAV CSV and validate schema\n",
        "assert TCAV_CSV.exists(), f'Missing TCAV CSV: {TCAV_CSV}'\n",
        "df_long = pd.read_csv(TCAV_CSV)\n",
        "\n",
        "required_cols = {\n",
        "    'utt_id','group','partition','speaker_id','label_str','label_id','system_id',\n",
        "    'concept_name','positive_percentage','magnitude','layer_key'\n",
        "}\n",
        "missing = sorted(required_cols - set(df_long.columns))\n",
        "assert not missing, f'Missing columns in TCAV CSV: {missing}'\n",
        "\n",
        "# Normalize types\n",
        "for c in ['positive_percentage','magnitude','cav_acc']:\n",
        "    if c in df_long.columns:\n",
        "        df_long[c] = pd.to_numeric(df_long[c], errors='coerce')\n",
        "df_long['utt_id'] = df_long['utt_id'].astype(str)\n",
        "df_long['speaker_id'] = df_long['speaker_id'].astype(str)\n",
        "df_long['group'] = df_long['group'].astype(str)\n",
        "df_long['partition'] = df_long['partition'].astype(str)\n",
        "df_long['label_str'] = df_long['label_str'].astype(str)\n",
        "df_long['label_id'] = pd.to_numeric(df_long['label_id'], errors='coerce').astype(int)\n",
        "df_long['system_id'] = df_long['system_id'].astype(str)\n",
        "\n",
        "# Basic checks\n",
        "print('Rows:', len(df_long))\n",
        "print('Unique utt_id:', df_long['utt_id'].nunique())\n",
        "print('Groups:', df_long[['utt_id','group']].drop_duplicates()['group'].value_counts().sort_index().to_dict())\n",
        "print('Labels:', df_long[['utt_id','label_str']].drop_duplicates()['label_str'].value_counts().sort_index().to_dict())\n",
        "print('Label IDs:', df_long[['utt_id','label_id']].drop_duplicates()['label_id'].value_counts().sort_index().to_dict())\n",
        "print('Layer keys:', sorted(df_long['layer_key'].astype(str).unique().tolist()))\n",
        "print('Concepts:', df_long['concept_name'].nunique())\n",
        "\n",
        "# Expect one row per (utt_id, concept_name, layer_key)\n",
        "per_utt = df_long.groupby('utt_id').size()\n",
        "print('Rows per utt_id (value counts):')\n",
        "print(per_utt.value_counts().sort_index())\n",
        "\n",
        "display(df_long.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Long -> wide (one row per utterance, concept metrics as features)\n",
        "meta_cols = ['utt_id','group','partition','speaker_id','gender','label_str','label_id','system_id','target_class_name','target_class_id']\n",
        "meta_cols = [c for c in meta_cols if c in df_long.columns]\n",
        "\n",
        "# Keep one metadata row per utterance\n",
        "meta_df = df_long[meta_cols].drop_duplicates(subset=['utt_id']).copy()\n",
        "assert meta_df['utt_id'].nunique() == len(meta_df)\n",
        "\n",
        "feature_frames = []\n",
        "feature_cols = []\n",
        "for metric in USE_METRICS:\n",
        "    piv = df_long.pivot_table(index='utt_id', columns='concept_name', values=metric, aggfunc='first')\n",
        "    piv = piv.sort_index(axis=1)\n",
        "    piv.columns = [f'{metric}__{c}' for c in piv.columns]\n",
        "    piv = piv.reset_index()\n",
        "    feature_frames.append(piv)\n",
        "    feature_cols.extend([c for c in piv.columns if c != 'utt_id'])\n",
        "\n",
        "df_wide = meta_df.copy()\n",
        "for fdf in feature_frames:\n",
        "    df_wide = df_wide.merge(fdf, on='utt_id', how='inner')\n",
        "\n",
        "print('Sample-level rows:', len(df_wide))\n",
        "print('Feature count:', len(feature_cols))\n",
        "print('Groups:', df_wide['group'].value_counts().sort_index().to_dict())\n",
        "print('Labels:', df_wide['label_str'].value_counts().sort_index().to_dict())\n",
        "display(df_wide.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Fixed B/C split (speaker-disjoint by design from your subset plan)\n",
        "train_df = df_wide[df_wide['group'].isin(TRAIN_GROUPS)].copy()\n",
        "test_df = df_wide[df_wide['group'].isin(TEST_GROUPS)].copy()\n",
        "holdout_A_df = df_wide[df_wide['group'].isin(OPTIONAL_HOLDOUT_GROUPS)].copy()\n",
        "\n",
        "assert len(train_df) > 0 and len(test_df) > 0, 'Train/Test groups are empty. Check group labels in TCAV CSV.'\n",
        "\n",
        "# Sanity: speaker disjointness between B and C\n",
        "train_speakers = set(train_df['speaker_id'].astype(str).unique().tolist())\n",
        "test_speakers = set(test_df['speaker_id'].astype(str).unique().tolist())\n",
        "assert len(train_speakers & test_speakers) == 0, 'Speaker leakage between train and test groups!'\n",
        "\n",
        "print('Train rows:', len(train_df), '| speakers:', len(train_speakers), '| labels:', train_df['label_id'].value_counts().sort_index().to_dict())\n",
        "print('Test rows :', len(test_df),  '| speakers:', len(test_speakers),  '| labels:', test_df['label_id'].value_counts().sort_index().to_dict())\n",
        "print('Holdout A rows:', len(holdout_A_df), '| speakers:', holdout_A_df['speaker_id'].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train logistic regression on TCAV features (B -> C)\n",
        "X_train = train_df[feature_cols].copy()\n",
        "X_test = test_df[feature_cols].copy()\n",
        "\n",
        "# Impute with training medians only\n",
        "medians = X_train.median(numeric_only=True)\n",
        "X_train = X_train.fillna(medians)\n",
        "X_test = X_test.fillna(medians)\n",
        "\n",
        "y_train = train_df['label_id'].astype(int).to_numpy()  # 0=bonafide, 1=spoof\n",
        "y_test = test_df['label_id'].astype(int).to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=3000, class_weight='balanced', solver='liblinear', random_state=0)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "y_prob_spoof = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': float(accuracy_score(y_test, y_pred)),\n",
        "    'precision_spoof_1': float(precision_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'recall_spoof_1': float(recall_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'f1_spoof_1': float(f1_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'confusion_matrix_labels_[0_bonafide,1_spoof]': confusion_matrix(y_test, y_pred, labels=[0,1]).tolist(),\n",
        "    'classification_report': classification_report(y_test, y_pred, labels=[0,1], zero_division=0),\n",
        "}\n",
        "print(json.dumps({k:v for k,v in metrics.items() if k != 'classification_report'}, indent=2))\n",
        "print(metrics['classification_report'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Global concept importance (coefficients)\n",
        "coef = clf.coef_.ravel()\n",
        "coef_rows = []\n",
        "for feature, c in zip(feature_cols, coef):\n",
        "    metric, concept = feature.split('__', 1)\n",
        "    coef_rows.append({\n",
        "        'feature': feature,\n",
        "        'metric': metric,\n",
        "        'concept': concept,\n",
        "        'coefficient': float(c),\n",
        "        'abs_coefficient': float(abs(c)),\n",
        "        'direction': 'spoof' if c > 0 else 'bonafide' if c < 0 else 'neutral'\n",
        "    })\n",
        "coef_df = pd.DataFrame(coef_rows).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
        "display(coef_df.head(30))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Class-wise concept summary (bonafide vs spoof means) on B+C (analysis set)\n",
        "analysis_df = df_wide[df_wide['group'].isin(TRAIN_GROUPS + TEST_GROUPS)].copy()\n",
        "summary_rows = []\n",
        "for feature in feature_cols:\n",
        "    metric, concept = feature.split('__', 1)\n",
        "    grp = analysis_df.groupby('label_id')[feature].agg(['mean', 'median'])\n",
        "    bona_mean = float(grp.loc[0, 'mean']) if 0 in grp.index and pd.notna(grp.loc[0, 'mean']) else np.nan\n",
        "    spoof_mean = float(grp.loc[1, 'mean']) if 1 in grp.index and pd.notna(grp.loc[1, 'mean']) else np.nan\n",
        "    bona_median = float(grp.loc[0, 'median']) if 0 in grp.index and pd.notna(grp.loc[0, 'median']) else np.nan\n",
        "    spoof_median = float(grp.loc[1, 'median']) if 1 in grp.index and pd.notna(grp.loc[1, 'median']) else np.nan\n",
        "    summary_rows.append({\n",
        "        'feature': feature,\n",
        "        'metric': metric,\n",
        "        'concept': concept,\n",
        "        'bonafide_mean_label0': bona_mean,\n",
        "        'spoof_mean_label1': spoof_mean,\n",
        "        'bonafide_median_label0': bona_median,\n",
        "        'spoof_median_label1': spoof_median,\n",
        "        'mean_diff_spoof_minus_bonafide': spoof_mean - bona_mean if pd.notna(spoof_mean) and pd.notna(bona_mean) else np.nan,\n",
        "    })\n",
        "class_summary_df = pd.DataFrame(summary_rows)\n",
        "print('Top spoof-leaning by mean difference:')\n",
        "display(class_summary_df.sort_values('mean_diff_spoof_minus_bonafide', ascending=False).head(15))\n",
        "print('Top bonafide-leaning by mean difference:')\n",
        "display(class_summary_df.sort_values('mean_diff_spoof_minus_bonafide', ascending=True).head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Per-sample contributions, per-user importance (test group C), and per-system analysis (A01-A08)\n",
        "contrib_matrix = X_test_scaled * clf.coef_.ravel().reshape(1, -1)\n",
        "contrib_df = pd.DataFrame(contrib_matrix, columns=feature_cols)\n",
        "\n",
        "pred_df = test_df[['utt_id','group','speaker_id','system_id','label_str','label_id']].reset_index(drop=True).copy()\n",
        "pred_df['pred_label_id'] = y_pred\n",
        "pred_df['pred_label_str'] = pred_df['pred_label_id'].map({0:'bonafide',1:'spoof'})\n",
        "pred_df['pred_prob_spoof'] = y_prob_spoof\n",
        "pred_df['correct'] = (pred_df['pred_label_id'] == pred_df['label_id']).astype(int)\n",
        "\n",
        "sample_contrib_long = pd.concat([\n",
        "    pred_df[['utt_id','group','speaker_id','system_id','label_str','label_id']],\n",
        "    contrib_df\n",
        "], axis=1).melt(\n",
        "    id_vars=['utt_id','group','speaker_id','system_id','label_str','label_id'],\n",
        "    var_name='feature', value_name='contribution'\n",
        ")\n",
        "sample_contrib_long[['metric','concept']] = sample_contrib_long['feature'].str.split('__', n=1, expand=True)\n",
        "\n",
        "# Per-user mean contributions (test speakers)\n",
        "user_contrib_df = (\n",
        "    sample_contrib_long.groupby(['speaker_id','label_id','label_str','feature','metric','concept'], as_index=False)['contribution']\n",
        "    .mean().rename(columns={'contribution':'mean_contribution'})\n",
        ")\n",
        "\n",
        "# Top concepts per user (test set)\n",
        "def top_concepts_per_user(user_df, top_k=5):\n",
        "    rows = []\n",
        "    for (spk, label_id, label_str), g in user_df.groupby(['speaker_id','label_id','label_str']):\n",
        "        g_desc = g.sort_values('mean_contribution', ascending=False)\n",
        "        g_asc = g.sort_values('mean_contribution', ascending=True)\n",
        "        for rank, (_, r) in enumerate(g_desc.head(top_k).iterrows(), start=1):\n",
        "            rows.append({'speaker_id':spk,'label_id':int(label_id),'label_str':label_str,'list_type':'top_spoof_supporting','rank':rank,\n",
        "                         'feature':r['feature'],'metric':r['metric'],'concept':r['concept'],'mean_contribution':float(r['mean_contribution'])})\n",
        "        for rank, (_, r) in enumerate(g_asc.head(top_k).iterrows(), start=1):\n",
        "            rows.append({'speaker_id':spk,'label_id':int(label_id),'label_str':label_str,'list_type':'top_bonafide_supporting','rank':rank,\n",
        "                         'feature':r['feature'],'metric':r['metric'],'concept':r['concept'],'mean_contribution':float(r['mean_contribution'])})\n",
        "    return pd.DataFrame(rows).sort_values(['speaker_id','label_id','list_type','rank']).reset_index(drop=True)\n",
        "\n",
        "top_user_df = top_concepts_per_user(user_contrib_df, top_k=5)\n",
        "\n",
        "# Per-system summaries on test set (A01-A08 + bonafide)\n",
        "system_feature_summary_df = (\n",
        "    test_df.groupby(['system_id','label_id','label_str'])[feature_cols]\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Per-system contributions (test set)\n",
        "system_contrib_df = (\n",
        "    sample_contrib_long.groupby(['system_id','label_id','label_str','feature','metric','concept'], as_index=False)['contribution']\n",
        "    .mean().rename(columns={'contribution':'mean_contribution'})\n",
        ")\n",
        "\n",
        "# Per-system performance (spoof systems + bonafide)\n",
        "system_perf_rows = []\n",
        "for sysid, g in pred_df.groupby('system_id'):\n",
        "    y_true_sys = g['label_id'].to_numpy()\n",
        "    y_pred_sys = g['pred_label_id'].to_numpy()\n",
        "    system_perf_rows.append({\n",
        "        'system_id': sysid,\n",
        "        'n_samples': int(len(g)),\n",
        "        'label_id_mode': int(g['label_id'].mode().iloc[0]),\n",
        "        'label_str_mode': str(g['label_str'].mode().iloc[0]),\n",
        "        'accuracy': float((y_true_sys == y_pred_sys).mean()),\n",
        "        'mean_pred_prob_spoof': float(g['pred_prob_spoof'].mean()),\n",
        "    })\n",
        "system_perf_df = pd.DataFrame(system_perf_rows).sort_values(['label_str_mode','system_id']).reset_index(drop=True)\n",
        "\n",
        "print('pred_df:'); display(pred_df.head(10))\n",
        "print('top_user_df:'); display(top_user_df.head(20))\n",
        "print('system_perf_df:'); display(system_perf_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save outputs (ready for visualization / reporting)\n",
        "# Save prepared wide table too, since it's useful for re-running analysis quickly.\n",
        "df_wide.to_csv(OUT_DIR / 'tcav_sample_level_features_all.csv', index=False)\n",
        "train_df.to_csv(OUT_DIR / 'train_group_B_samples.csv', index=False)\n",
        "test_df.to_csv(OUT_DIR / 'test_group_C_samples.csv', index=False)\n",
        "holdout_A_df.to_csv(OUT_DIR / 'holdout_group_A_samples.csv', index=False)\n",
        "\n",
        "coef_df.to_csv(OUT_DIR / 'global_concept_coefficients.csv', index=False)\n",
        "class_summary_df.to_csv(OUT_DIR / 'classwise_concept_summary_BplusC.csv', index=False)\n",
        "pred_df.to_csv(OUT_DIR / 'test_group_C_predictions.csv', index=False)\n",
        "sample_contrib_long.to_csv(OUT_DIR / 'test_group_C_sample_contributions_long.csv', index=False)\n",
        "user_contrib_df.to_csv(OUT_DIR / 'test_group_C_user_mean_contributions.csv', index=False)\n",
        "top_user_df.to_csv(OUT_DIR / 'test_group_C_top_concepts_per_user.csv', index=False)\n",
        "system_feature_summary_df.to_csv(OUT_DIR / 'test_group_C_system_feature_means.csv', index=False)\n",
        "system_contrib_df.to_csv(OUT_DIR / 'test_group_C_system_mean_contributions.csv', index=False)\n",
        "system_perf_df.to_csv(OUT_DIR / 'test_group_C_system_performance.csv', index=False)\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(scaler, OUT_DIR / 'scaler_tcav_features.joblib')\n",
        "    joblib.dump(clf, OUT_DIR / 'logreg_tcav_features.joblib')\n",
        "except Exception as e:\n",
        "    print('[WARN] Could not save model artifacts:', e)\n",
        "\n",
        "run_meta = {\n",
        "    'tcav_csv': str(TCAV_CSV),\n",
        "    'out_dir': str(OUT_DIR),\n",
        "    'use_metrics': USE_METRICS,\n",
        "    'split_policy': {'train_groups': TRAIN_GROUPS, 'test_groups': TEST_GROUPS, 'holdout_groups': OPTIONAL_HOLDOUT_GROUPS},\n",
        "    'label_mapping': {'0':'bonafide', '1':'spoof'},\n",
        "    'counts': {\n",
        "        'total_samples': int(len(df_wide)),\n",
        "        'train_samples_B': int(len(train_df)),\n",
        "        'test_samples_C': int(len(test_df)),\n",
        "        'holdout_samples_A': int(len(holdout_A_df)),\n",
        "        'n_features': int(len(feature_cols)),\n",
        "        'n_concepts': int(df_long['concept_name'].nunique()),\n",
        "    },\n",
        "    'metrics': metrics,\n",
        "}\n",
        "(OUT_DIR / 'run_metadata.json').write_text(json.dumps(run_meta, indent=2), encoding='utf-8')\n",
        "print('Saved outputs to', OUT_DIR)\n",
        "print(json.dumps({k:v for k,v in metrics.items() if k != 'classification_report'}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and test diagnostics plots (saved to OUT_DIR/plots)\n",
        "PLOTS_DIR = OUT_DIR / 'plots'\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Compute train predictions for diagnostics\n",
        "X_train_diag = train_df[feature_cols].copy().fillna(medians)\n",
        "X_train_diag_scaled = scaler.transform(X_train_diag)\n",
        "y_pred_train = clf.predict(X_train_diag_scaled)\n",
        "y_prob_train_spoof = clf.predict_proba(X_train_diag_scaled)[:, 1]\n",
        "\n",
        "train_acc = float((y_pred_train == y_train).mean())\n",
        "test_acc = float((y_pred == y_test).mean())\n",
        "print('Train accuracy:', train_acc)\n",
        "print('Test accuracy :', test_acc)\n",
        "\n",
        "# 1) Train vs Test accuracy bar chart\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(['Train (B)', 'Test (C)'], [train_acc, test_acc], color=['#1f77b4', '#ff7f0e'])\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('TCAV-Feature Logistic Regression Accuracy')\n",
        "for x, yv in zip(['Train (B)', 'Test (C)'], [train_acc, test_acc]):\n",
        "    plt.text(x, yv + 0.02, f'{yv:.3f}', ha='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'train_vs_test_accuracy.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 2) Confusion matrix (test)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
        "plt.figure(figsize=(4.8,4.2))\n",
        "plt.imshow(cm, cmap='Blues')\n",
        "plt.title('Test Confusion Matrix (C)')\n",
        "plt.xticks([0,1], ['pred bona', 'pred spoof'])\n",
        "plt.yticks([0,1], ['true bona', 'true spoof'])\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, str(cm[i,j]), ha='center', va='center', color='black')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'test_confusion_matrix.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 3) Probability distributions (train vs test, by class)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,4), sharey=True)\n",
        "axes[0].hist(y_prob_train_spoof[y_train==0], bins=20, alpha=0.7, label='bonafide (0)')\n",
        "axes[0].hist(y_prob_train_spoof[y_train==1], bins=20, alpha=0.7, label='spoof (1)')\n",
        "axes[0].set_title('Train (B): Predicted P(spoof)')\n",
        "axes[0].set_xlabel('Predicted P(spoof)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].legend(fontsize=8)\n",
        "\n",
        "axes[1].hist(y_prob_spoof[y_test==0], bins=20, alpha=0.7, label='bonafide (0)')\n",
        "axes[1].hist(y_prob_spoof[y_test==1], bins=20, alpha=0.7, label='spoof (1)')\n",
        "axes[1].set_title('Test (C): Predicted P(spoof)')\n",
        "axes[1].set_xlabel('Predicted P(spoof)')\n",
        "axes[1].legend(fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'train_test_prob_spoof_hist.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print('Saved diagnostic plots in', PLOTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: global concept importance and class differences\n",
        "PLOTS_DIR = OUT_DIR / 'plots'\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Global coefficients top 25\n",
        "coef_plot = coef_df.head(25).copy().iloc[::-1]\n",
        "colors = ['#d62728' if d == 'spoof' else '#1f77b4' if d == 'bonafide' else '#7f7f7f' for d in coef_plot['direction']]\n",
        "plt.figure(figsize=(10, max(6, 0.32 * len(coef_plot))))\n",
        "plt.barh(coef_plot['feature'], coef_plot['coefficient'], color=colors)\n",
        "plt.axvline(0, color='black', linewidth=1)\n",
        "plt.title('Global Concept Coefficients (positive=spoof, negative=bonafide)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'global_concept_coefficients_top25.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Mean difference (spoof - bonafide) top/bottom\n",
        "top_spoof = class_summary_df.sort_values('mean_diff_spoof_minus_bonafide', ascending=False).head(15).copy().iloc[::-1]\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.barh(top_spoof['feature'], top_spoof['mean_diff_spoof_minus_bonafide'], color='#d62728')\n",
        "plt.axvline(0, color='black', linewidth=1)\n",
        "plt.title('Top Spoof-Leaning Concepts by Mean Difference (B+C)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'class_diff_top_spoof_15.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "top_bona = class_summary_df.sort_values('mean_diff_spoof_minus_bonafide', ascending=True).head(15).copy().iloc[::-1]\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.barh(top_bona['feature'], top_bona['mean_diff_spoof_minus_bonafide'], color='#1f77b4')\n",
        "plt.axvline(0, color='black', linewidth=1)\n",
        "plt.title('Top Bonafide-Leaning Concepts by Mean Difference (B+C)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'class_diff_top_bonafide_15.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: per-user and per-system analyses (test group C)\n",
        "PLOTS_DIR = OUT_DIR / 'plots'\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Per-user heatmap (fake/spoof samples only, test C)\n",
        "spoof_user = user_contrib_df[user_contrib_df['label_id'] == 1].copy()\n",
        "if spoof_user.empty:\n",
        "    print('No spoof rows in user_contrib_df for test group C')\n",
        "else:\n",
        "    heat = spoof_user.pivot_table(index='speaker_id', columns='feature', values='mean_contribution', aggfunc='mean').fillna(0)\n",
        "    plt.figure(figsize=(max(10, 0.35 * heat.shape[1]), max(4, 0.6 * heat.shape[0])))\n",
        "    im = plt.imshow(heat.to_numpy(), aspect='auto', cmap='coolwarm')\n",
        "    plt.colorbar(im, label='Mean contribution')\n",
        "    plt.xticks(range(heat.shape[1]), heat.columns, rotation=90, fontsize=7)\n",
        "    plt.yticks(range(heat.shape[0]), heat.index, fontsize=9)\n",
        "    plt.title('Test Group C: Per-user Mean Concept Contributions (Spoof samples)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PLOTS_DIR / 'test_group_C_user_heatmap_spoof.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# Per-system performance bar chart on test C\n",
        "sys_perf_plot = system_perf_df.copy().sort_values(['label_str_mode', 'system_id'])\n",
        "plt.figure(figsize=(9, 4.5))\n",
        "colors = ['#1f77b4' if x == 'bonafide' else '#d62728' for x in sys_perf_plot['label_str_mode']]\n",
        "plt.bar(sys_perf_plot['system_id'], sys_perf_plot['accuracy'], color=colors)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Accuracy on test C')\n",
        "plt.title('Per-system Accuracy (Test Group C)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / 'test_group_C_system_accuracy.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Top concepts per spoof system (mean contributions) table export\n",
        "top_system_rows = []\n",
        "for sysid, g in system_contrib_df[system_contrib_df['label_id'] == 1].groupby('system_id'):\n",
        "    gs = g.sort_values('mean_contribution', ascending=False).head(5)\n",
        "    for rank, (_, r) in enumerate(gs.iterrows(), start=1):\n",
        "        top_system_rows.append({\n",
        "            'system_id': sysid,\n",
        "            'rank': rank,\n",
        "            'feature': r['feature'],\n",
        "            'metric': r['metric'],\n",
        "            'concept': r['concept'],\n",
        "            'mean_contribution': float(r['mean_contribution']),\n",
        "        })\n",
        "\n",
        "top_system_df = pd.DataFrame(top_system_rows).sort_values(['system_id','rank']).reset_index(drop=True)\n",
        "top_system_df.to_csv(OUT_DIR / 'test_group_C_top_concepts_per_system.csv', index=False)\n",
        "print('Saved:', OUT_DIR / 'test_group_C_top_concepts_per_system.csv')\n",
        "display(top_system_df.head(40))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}