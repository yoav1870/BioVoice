{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SpeakerRec/.cache/torch/hub/IDRnD_ReDimNet_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ReDimNet successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[2]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "redim_model = torch.hub.load(\n",
    "    \"IDRnD/ReDimNet\",\n",
    "    \"ReDimNet\",\n",
    "    model_name=\"b5\",\n",
    "    train_type=\"ptn\",\n",
    "    dataset=\"vox2\",\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "print(\"Loaded ReDimNet successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded speaker head from: /home/SpeakerRec/BioVoice/redimnet/grad_cam/2.0/output/redim_speaker_head_linear.pt\n",
      "Speakers: ['eden', 'idan', 'yoav']\n",
      "L2-normalized embeddings: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3385382/4104975373.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "# another head model - YOAV!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "HEAD_PATH = Path.cwd() / \"output\" / \"redim_speaker_head_linear.pt\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "\n",
    "# Restore speaker mappings\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "\n",
    "# Define head with correct dimensions\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim=192, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "head = SpeakerHead(\n",
    "    in_dim=192,\n",
    "    num_classes=len(SPEAKERS)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n",
    "\n",
    "print(\"Loaded speaker head from:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"L2-normalized embeddings:\", ckpt.get(\"l2_norm_emb\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float01(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def upsample_hw(arr: np.ndarray, size_hw: tuple[int, int],mode: str = \"bilinear\") -> np.ndarray:\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        t = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()  # [1,1,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0, 0].cpu().numpy()\n",
    "\n",
    "    if arr.ndim == 3 and arr.shape[2] == 3:\n",
    "        t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0).float()  # [1,3,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    raise ValueError(f\"Unsupported shape: {arr.shape}\")\n",
    "\n",
    "def plot_mel(mel, save_path=None, title=None):\n",
    "    if isinstance(mel, torch.Tensor):\n",
    "        mel = mel.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    mel_norm = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_norm, aspect=\"auto\", cmap=\"magma\", interpolation=\"bilinear\",origin=\"lower\")\n",
    "    plt.colorbar(label=\"Normalized Energy\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Frequency Bins\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def wav_to_model_mel(wav_path: str):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)  # [1, 80, T]\n",
    "\n",
    "    mel4d = mel.unsqueeze(0)  # [1, 1, 80, T]\n",
    "\n",
    "    mel_np = mel.squeeze(0).cpu().numpy()  # [80, T]\n",
    "    mel_norm = (mel_np - mel_np.min()) / (mel_np.max() - mel_np.min() + 1e-8)\n",
    "    rgb_base = np.stack([mel_norm] * 3, axis=-1)  # [80, T, 3]\n",
    "    rgb_base = np.flipud(rgb_base)  # keep your old convention\n",
    "\n",
    "    return mel4d, rgb_base\n",
    "\n",
    "\n",
    "def magma_rgb(img2d: np.ndarray) -> np.ndarray:\n",
    "    img2d = img2d.astype(np.float32)\n",
    "    img2d = (img2d - img2d.min()) / (img2d.max() - img2d.min() + 1e-8)\n",
    "    return cm.get_cmap(\"magma\")(img2d)[..., :3].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper (logits) ready.\n"
     ]
    }
   ],
   "source": [
    "class ReDimNetMelLogitsWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  mel4d [B, 1, 80, T]\n",
    "    Output: logits [B, 3]\n",
    "    \"\"\"\n",
    "    def __init__(self, redim_model, head, l2_norm_emb=True):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone\n",
    "        self.pool     = redim_model.pool\n",
    "        self.bn       = redim_model.bn\n",
    "        self.linear   = redim_model.linear  # [B, 192]\n",
    "        self.head     = head                # [B, 3]\n",
    "        self.l2_norm_emb = l2_norm_emb\n",
    "\n",
    "    def forward(self, mel4d):\n",
    "        x = self.backbone(mel4d)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn(x)\n",
    "        emb = self.linear(x)  # [B, 192]\n",
    "\n",
    "        # match head training: L2-normalized embeddings\n",
    "        if self.l2_norm_emb:\n",
    "            emb = emb / (emb.norm(p=2, dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        logits = self.head(emb)  # [B, 3]\n",
    "        return logits\n",
    "\n",
    "wrapped_model = ReDimNetMelLogitsWrapper(redim_model, head, l2_norm_emb=True).to(DEVICE).eval()\n",
    "print(\"Wrapper (logits) ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layers:\n",
      "stem -> Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "stage0 -> Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage1 -> Conv2d(32, 128, kernel_size=(2, 1), stride=(2, 1))\n",
      "stage2 -> Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage3 -> Conv2d(64, 128, kernel_size=(2, 1), stride=(2, 1))\n",
      "stage4 -> Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage5 -> Conv2d(128, 256, kernel_size=(2, 1), stride=(2, 1))\n"
     ]
    }
   ],
   "source": [
    "target_layers = [\n",
    "    wrapped_model.backbone.stem[0],\n",
    "    wrapped_model.backbone.stage0[2],\n",
    "    wrapped_model.backbone.stage1[2],\n",
    "    wrapped_model.backbone.stage2[2],\n",
    "    wrapped_model.backbone.stage3[2],\n",
    "    wrapped_model.backbone.stage4[2],\n",
    "    wrapped_model.backbone.stage5[2],\n",
    "]\n",
    "layer_names = [\"stem\", \"stage0\", \"stage1\", \"stage2\", \"stage3\", \"stage4\", \"stage5\"]\n",
    "\n",
    "print(\"Target layers:\")\n",
    "for n, l in zip(layer_names, target_layers):\n",
    "    print(n, \"->\", l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassLogitTarget:\n",
    "    def __init__(self, class_idx: int):\n",
    "        self.class_idx = int(class_idx)\n",
    "\n",
    "    def __call__(self, model_output: torch.Tensor) -> torch.Tensor:\n",
    "        if model_output.ndim == 1:\n",
    "            return model_output[self.class_idx]\n",
    "        if model_output.ndim == 2:\n",
    "            return model_output[:, self.class_idx].sum()\n",
    "        raise ValueError(f\"Unexpected model_output shape: {tuple(model_output.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim_model.eval()\n",
    "head.eval()\n",
    "\n",
    "for p in redim_model.parameters():\n",
    "    p.requires_grad = True   # needed for Grad-CAM\n",
    "\n",
    "for p in head.parameters():\n",
    "    p.requires_grad = False  # optional (head not needed to have grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam_speaker(\n",
    "    wav_path: str,\n",
    "    target_speaker: str,\n",
    "    save_dir: str = \"gradcam_results_cls\",\n",
    "    upscale: int = 10,\n",
    "    cam_quantile: float = 0.85,\n",
    "):\n",
    "    assert target_speaker in speaker_to_id, f\"Unknown speaker: {target_speaker}\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fname = Path(wav_path).stem\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Compute model Mel (input to backbone)\n",
    "    # ----------------------------\n",
    "    mel4d, _ = wav_to_model_mel(wav_path)  # expected: [1,1,80,T]\n",
    "    mel4d = mel4d.detach().to(DEVICE)\n",
    "    mel4d.requires_grad_(True)  # important for Grad-CAM\n",
    "\n",
    "    mel2d = mel4d.squeeze(0).squeeze(0).detach().cpu().numpy()  # [80, T]\n",
    "    mel2d = mel2d.astype(np.float32)\n",
    "\n",
    "    mel_norm = (mel2d - mel2d.min()) / (mel2d.max() - mel2d.min() + 1e-8)\n",
    "    mel_norm = np.clip(mel_norm.astype(np.float32), 0.0, 1.0)\n",
    "\n",
    "    # Save original mel\n",
    "    originals_dir = save_dir / \"original\"\n",
    "    originals_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plot_mel(\n",
    "        mel2d,\n",
    "        save_path=originals_dir / f\"{fname}_mel.png\",\n",
    "        title=f\"{fname} (target={target_speaker})\",\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Target definition\n",
    "    # ----------------------------\n",
    "    target = ClassLogitTarget(speaker_to_id[target_speaker])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Iterate over ALL layers\n",
    "    # ----------------------------\n",
    "\n",
    "    for layer, lname in zip(target_layers, layer_names):\n",
    "        print(f\"[Grad-CAM] Layer: {lname}\")\n",
    "\n",
    "        layer_dir = save_dir / lname\n",
    "        layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        wrapped_model.zero_grad(set_to_none=True)\n",
    "\n",
    "        cam = GradCAM(\n",
    "            model=wrapped_model,\n",
    "            target_layers=[layer],\n",
    "        )\n",
    "\n",
    "        grayscale_cam = cam(\n",
    "            input_tensor=mel4d,\n",
    "            targets=[target],\n",
    "            aug_smooth=False,\n",
    "            eigen_smooth=True,\n",
    "        )[0]  # [H', W']\n",
    "\n",
    "        # Flip to match mel orientation (your convention)\n",
    "        grayscale_cam = np.flipud(grayscale_cam).astype(np.float32)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 4) Align CAM to Mel space\n",
    "        # ----------------------------\n",
    "        cam_on_mel = upsample_hw(grayscale_cam, mel2d.shape, mode=\"bicubic\").astype(np.float32)\n",
    "\n",
    "        # Normalize CAM to [0,1]\n",
    "        cam_norm = (cam_on_mel - cam_on_mel.min()) / (cam_on_mel.max() - cam_on_mel.min() + 1e-8)\n",
    "        cam_norm = np.clip(cam_norm.astype(np.float32), 0.0, 1.0)\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # 6) High-activation focus (relative threshold)\n",
    "        # ----------------------------\n",
    "        thr = float(np.quantile(cam_norm, cam_quantile))\n",
    "        mask = cam_norm >= thr\n",
    "\n",
    "        mel_focus = np.where(mask, mel2d, mel2d.min())\n",
    "        plot_mel(\n",
    "            mel_focus,\n",
    "            save_path=layer_dir / f\"{fname}_mel_focus_q{cam_quantile}.png\",\n",
    "            title=f\"{fname} | {lname} | focus (q={cam_quantile})\",\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 7) Visualization overlay (needs float32 in [0,1])\n",
    "        # ----------------------------\n",
    "        H, W = mel_norm.shape\n",
    "        H2, W2 = H * upscale, W * upscale\n",
    "\n",
    "        rgb_big = upsample_hw(\n",
    "            np.stack([mel_norm] * 3, axis=-1),\n",
    "            (H2, W2),\n",
    "            mode=\"bicubic\",\n",
    "        ).astype(np.float32)\n",
    "        rgb_big = np.clip(rgb_big, 0.0, 1.0)  # IMPORTANT (bicubic can overshoot)\n",
    "\n",
    "        cam_big = upsample_hw(cam_norm, (H2, W2), mode=\"bicubic\").astype(np.float32)\n",
    "        cam_big = np.clip(cam_big, 0.0, 1.0)\n",
    "\n",
    "        overlay = show_cam_on_image(rgb_big, cam_big, use_rgb=True)\n",
    "\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay.png\", overlay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grad-CAM] Layer: stem\n",
      "[Grad-CAM] Layer: stage0\n",
      "[Grad-CAM] Layer: stage1\n",
      "[Grad-CAM] Layer: stage2\n",
      "[Grad-CAM] Layer: stage3\n",
      "[Grad-CAM] Layer: stage4\n",
      "[Grad-CAM] Layer: stage5\n"
     ]
    }
   ],
   "source": [
    "wav_dir = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "\n",
    "test_wav = wav_dir / \"idan_009.wav\"\n",
    "run_gradcam_speaker(\n",
    "    wav_path=str(test_wav),\n",
    "    target_speaker=\"idan\",\n",
    "    save_dir=f\"gradcam_results/idan/{test_wav.stem}\",\n",
    "    upscale=12,\n",
    ")\n",
    "\n",
    "\n",
    "# top3_by_user = {\n",
    "#     \"eden\": [\"eden_017.wav\", \"eden_021.wav\", \"eden_012.wav\"],\n",
    "#     \"idan\": [\"idan_009.wav\", \"idan_004.wav\", \"idan_012.wav\"],\n",
    "#     \"yoav\": [\"yoav_028.wav\", \"yoav_024.wav\", \"yoav_022.wav\"],\n",
    "# }\n",
    "\n",
    "# for spk, fnames in top3_by_user.items():\n",
    "#     for fname in fnames:\n",
    "#         wav_path = wav_dir / fname\n",
    "#         if not wav_path.exists():\n",
    "#             print(f\"[WARN] missing file: {wav_path}\")\n",
    "#             continue\n",
    "\n",
    "#         run_gradcam_speaker(\n",
    "#             wav_path=str(wav_path),\n",
    "#             target_speaker=spk,\n",
    "#             save_dir=f\"gradcam_results_2.0//{spk}/{wav_path.stem}\",\n",
    "#             thr=0.4,      # or 0.3 if you prefer\n",
    "#             upscale=8,\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
