{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from wespeaker.cli.speaker import load_model\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Load WeSpeaker bundle + move backbone to GPU\n",
    "speaker = load_model(PROJECT_ROOT / \"wespeaker-voxceleb-resnet293-LM\")\n",
    "speaker.model = speaker.model.to(DEVICE).eval()\n",
    "\n",
    "print(\"ResNet-293 loaded\")\n",
    "\n",
    "# WeSpeaker mel params\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 80\n",
    "FRAME_LENGTH_MS = 25\n",
    "FRAME_SHIFT_MS = 10\n",
    "N_FFT = int(SAMPLE_RATE * FRAME_LENGTH_MS / 1000)\n",
    "HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT_MS / 1000)\n",
    "\n",
    "print(f\"Mel params: n_fft={N_FFT}, hop_length={HOP_LENGTH}, n_mels={N_MELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== LOAD YOUR TRAINED FC HEAD ==========\n",
    "SPEAKERS = [\"eden\", \"idan\", \"yoav\"]\n",
    "speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "head = SpeakerHead(in_dim=256, num_classes=len(SPEAKERS)).to(DEVICE)\n",
    "\n",
    "head_path = PROJECT_ROOT / \"data\" / \"heads\" / \"resnet_293_speaker_head.pt\"\n",
    "assert head_path.exists(), f\"Missing head checkpoint: {head_path}\"\n",
    "\n",
    "ckpt = torch.load(head_path, map_location=\"cpu\")\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head = head.to(DEVICE).eval()\n",
    "\n",
    "if \"speakers\" in ckpt:\n",
    "    SPEAKERS = ckpt[\"speakers\"]\n",
    "    speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "    id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "print(\"Loaded head from:\", head_path)\n",
    "print(\"Speakers:\", SPEAKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeSpeakerWithHeadForGradCAM(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone: nn.Module, head: nn.Module, try_transpose: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self.try_transpose = try_transpose\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Accept NCHW from Grad-CAM and squeeze channel dim\n",
    "        if feats.ndim == 4 and feats.size(1) == 1:\n",
    "            feats = feats.squeeze(1)\n",
    "\n",
    "        # Run backbone\n",
    "        try:\n",
    "            out = self.backbone(feats)\n",
    "        except Exception:\n",
    "            if not self.try_transpose:\n",
    "                raise\n",
    "            out = self.backbone(feats.transpose(1, 2))\n",
    "\n",
    "        # Unwrap tuple/list (WeSpeaker behavior)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            emb = out[-1]\n",
    "        else:\n",
    "            emb = out\n",
    "\n",
    "        # ---- HARD SHAPE GUARD ----\n",
    "        if emb.ndim == 0:\n",
    "            emb = emb.unsqueeze(0).unsqueeze(0)\n",
    "        elif emb.ndim == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "        elif emb.ndim > 2:\n",
    "            emb = emb.view(emb.size(0), -1)\n",
    "\n",
    "        # ---- HARD DEVICE GUARD (THIS FIXES YOUR ERROR) ----\n",
    "        emb = emb.to(next(self.head.parameters()).device)\n",
    "\n",
    "        logits = self.head(emb)\n",
    "\n",
    "        if logits.ndim == 1:\n",
    "            logits = logits.unsqueeze(0)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "wrapped_model = WeSpeakerWithHeadForGradCAM(speaker.model, head).to(DEVICE).eval()\n",
    "\n",
    "# Enable gradients for backbone (needed for Grad-CAM)\n",
    "for p in wrapped_model.backbone.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# Head grads not required (optional)\n",
    "for p in wrapped_model.head.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"wrapped_model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== HELPERS (your existing ones, unchanged) ==========\n",
    "def upsample_hw(arr: np.ndarray, size_hw: tuple[int, int], mode: str = \"bilinear\") -> np.ndarray:\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        t = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()  # [1,1,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0, 0].cpu().numpy()\n",
    "\n",
    "    if arr.ndim == 3 and arr.shape[2] == 3:\n",
    "        t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0).float()  # [1,3,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    raise ValueError(f\"Unsupported shape: {arr.shape}\")\n",
    "\n",
    "def to_float01(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def plot_mel(mel, save_path=None, title=None):\n",
    "    if isinstance(mel, torch.Tensor):\n",
    "        mel = mel.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    mel_db = 10.0 * np.log10(np.maximum(mel, 1e-10))\n",
    "    mel_db = np.clip(mel_db, mel_db.max() - 80.0, mel_db.max())\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_norm, aspect=\"auto\", cmap=\"magma\", interpolation=\"bilinear\", origin=\"lower\")\n",
    "    plt.colorbar(label=\"Normalized Energy\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Frequency Bins\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_wav_mono_16k(wav_path: str) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav[:1, :]\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "\n",
    "    return wav  # keep on CPU for mel transform stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== FEATURE EXTRACTOR (mel) ==========\n",
    "# Important: mel_transform must be on DEVICE if you want mel tensor on GPU.\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== TARGET LAYERS (same idea, but via wrapped_model.backbone) ==========\n",
    "target_layers = [\n",
    "    # wrapped_model.backbone.layer1[9].conv3,\n",
    "    # wrapped_model.backbone.layer2[19].conv3,\n",
    "    # wrapped_model.backbone.layer3[63].conv3,\n",
    "    wrapped_model.backbone.layer4[2].conv3,\n",
    "]\n",
    "\n",
    "layer_names = [\n",
    "    # \"layer1.9.conv3\",\n",
    "    # \"layer2.19.conv3\",\n",
    "    # \"layer3.63.conv3\",\n",
    "    \"layer4.2.conv3\",\n",
    "]\n",
    "\n",
    "print(\"Target layers:\")\n",
    "for n, l in zip(layer_names, target_layers):\n",
    "    print(n, \"->\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_gradcam_speaker(\n",
    "    wav_path: str,\n",
    "    target_speaker: str,\n",
    "    save_dir: str = \"gradcam_results_cls\",\n",
    "    upscale: int = 10,\n",
    "    cam_quantile: float = 0.85,\n",
    "):\n",
    "    assert target_speaker in speaker_to_id, f\"Unknown speaker: {target_speaker}\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fname = Path(wav_path).stem\n",
    "\n",
    "    # 1) Load wav (CPU), compute mel ON GPU with grad\n",
    "    wav_cpu = load_wav_mono_16k(wav_path)              # [1, T] CPU\n",
    "    wav_gpu = wav_cpu.to(DEVICE)                       # move to GPU for mel_transform\n",
    "    feats = mel_transform(wav_gpu)                     # [1, 80, Tfeat] on GPU\n",
    "    feats = feats.contiguous().float()\n",
    "    feats_cam = feats.unsqueeze(1)                  # [1, 1, 80, Tfeat] for Grad-CAM\n",
    "\n",
    "    # 1b) Detached mel for visualization (numpy)\n",
    "    with torch.no_grad():\n",
    "        mel2d = feats.detach()[0].cpu().numpy()        # [80, Tfeat]\n",
    "    mel_norm = (mel2d - mel2d.min()) / (mel2d.max() - mel2d.min() + 1e-8)\n",
    "\n",
    "    originals_dir = save_dir / \"original\"\n",
    "    originals_dir.mkdir(exist_ok=True)\n",
    "    plot_mel(\n",
    "        mel2d,\n",
    "        save_path=originals_dir / f\"{fname}_mel.png\",\n",
    "        title=f\"{fname} (target={target_speaker})\",\n",
    "    )\n",
    "\n",
    "    # 2) target class\n",
    "    target = ClassifierOutputTarget(speaker_to_id[target_speaker])\n",
    "\n",
    "    layer_stats = {}\n",
    "\n",
    "    for layer, lname in zip(target_layers, layer_names):\n",
    "        print(f\"[Grad-CAM] Layer: {lname}\")\n",
    "\n",
    "        layer_dir = save_dir / lname\n",
    "        layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        wrapped_model.zero_grad(set_to_none=True)\n",
    "        if feats.grad is not None:\n",
    "            feats.grad.zero_()\n",
    "\n",
    "        # Optional sanity check\n",
    "        with torch.no_grad():\n",
    "            tmp = wrapped_model(feats)\n",
    "        print(\"wrapped_model(feats) shape:\", tmp.shape)\n",
    "\n",
    "        try:\n",
    "            cam = GradCAM(model=wrapped_model, target_layers=[layer], use_cuda=(DEVICE == \"cuda\"))\n",
    "        except TypeError:\n",
    "            cam = GradCAM(model=wrapped_model, target_layers=[layer])\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            grayscale_cam = cam(\n",
    "                input_tensor=feats_cam,      # [1,1,80,T] for Grad-CAM\n",
    "                targets=[target],\n",
    "                aug_smooth=False,\n",
    "                eigen_smooth=True,\n",
    "            )[0]  # [H', W'] (feature map space)\n",
    "\n",
    "        # 4) align CAM to mel for plotting\n",
    "        cam_on_mel = upsample_hw(grayscale_cam, mel2d.shape, mode=\"bicubic\")\n",
    "        cam_norm = (cam_on_mel - cam_on_mel.min()) / (cam_on_mel.max() - cam_on_mel.min() + 1e-8)\n",
    "\n",
    "        # 5) weighted mel\n",
    "        mel_weighted = mel2d * cam_norm\n",
    "        plot_mel(\n",
    "            mel_weighted,\n",
    "            save_path=layer_dir / f\"{fname}_mel_weighted.png\",\n",
    "            title=f\"{fname} | {lname} | weighted\",\n",
    "        )\n",
    "\n",
    "        # 6) focus mask\n",
    "        thr = np.quantile(cam_norm, cam_quantile)\n",
    "        mask = cam_norm >= thr\n",
    "        mel_focus = np.where(mask, mel2d, mel2d.min())\n",
    "        plot_mel(\n",
    "            mel_focus,\n",
    "            save_path=layer_dir / f\"{fname}_mel_focus_q{cam_quantile}.png\",\n",
    "            title=f\"{fname} | {lname} | focus\",\n",
    "        )\n",
    "\n",
    "        # 7) overlay\n",
    "        H, W = mel_norm.shape\n",
    "        H2, W2 = H * upscale, W * upscale\n",
    "\n",
    "        rgb_big = upsample_hw(np.stack([mel_norm] * 3, axis=-1), (H2, W2), mode=\"bicubic\")\n",
    "        cam_big = upsample_hw(cam_norm, (H2, W2), mode=\"bicubic\")\n",
    "\n",
    "        rgb_big = to_float01(rgb_big)\n",
    "        cam_big = to_float01(cam_big)\n",
    "        overlay = show_cam_on_image(rgb_big, cam_big, use_rgb=True)\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay.png\", overlay)\n",
    "\n",
    "        # 8) stats\n",
    "        entropy = -np.sum(cam_norm * np.log(cam_norm + 1e-8)) / cam_norm.size\n",
    "        support = (cam_norm > 0.5).mean()\n",
    "        layer_stats[lname] = {\"entropy\": float(entropy), \"support@0.5\": float(support)}\n",
    "\n",
    "    stats_df = pd.DataFrame(layer_stats).T\n",
    "    stats_df.to_csv(save_dir / \"layer_cam_stats.csv\")\n",
    "\n",
    "    print(\"Saved Grad-CAM results to:\", save_dir)\n",
    "    print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "\n",
    "# Use an existing audio file\n",
    "test_wav = wav_dir / \"idan_012.wav\"\n",
    "if not test_wav.exists():\n",
    "    # Fallback to first available idan file\n",
    "    idan_files = list(wav_dir.glob(\"idan_*.wav\"))\n",
    "    if idan_files:\n",
    "        test_wav = sorted(idan_files)[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No audio files found in {wav_dir}\")\n",
    "\n",
    "print(f\"Testing with: {test_wav}\")\n",
    "\n",
    "run_out_dir = Path(f\"gradcam_results/idan/{test_wav.stem}\")\n",
    "originals_dir = run_out_dir / \"original\"\n",
    "\n",
    "run_gradcam_speaker(\n",
    "    wav_path=str(test_wav),\n",
    "    target_speaker=\"idan\",\n",
    "    save_dir=str(run_out_dir),\n",
    "    upscale=12,\n",
    ")\n",
    "\n",
    "# Uncomment to run on multiple speakers\n",
    "# top3_by_speaker = {\n",
    "#     \"eden\": [\"eden_007.wav\", \"eden_010.wav\", \"eden_012.wav\"],\n",
    "#     \"idan\": [\"idan_001.wav\", \"idan_002.wav\", \"idan_005.wav\"],\n",
    "#     \"yoav\": [\"yoav_004.wav\", \"yoav_014.wav\", \"yoav_015.wav\"],\n",
    "# }\n",
    "\n",
    "# for spk, fnames in top3_by_speaker.items():\n",
    "#     for fname in fnames:\n",
    "#         wav_path = wav_dir / fname\n",
    "#         if not wav_path.exists():\n",
    "#             print(f\"[WARN] missing file: {wav_path}\")\n",
    "#             continue\n",
    "\n",
    "#         run_gradcam_speaker(\n",
    "#             wav_path=str(wav_path),\n",
    "#             target_speaker=spk,\n",
    "#             save_dir=f\"gradcam_results/{spk}/{fname.replace('.wav', '')}\",\n",
    "#             upscale=12,\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvResnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
