{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from captum.concept import TCAV, Concept\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from wespeaker.cli.speaker import load_model\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# -------- Project paths / device --------\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "ATTR_CSV_PATH = Path(\n",
    "    PROJECT_ROOT / \"resnet_293\" / \"speaker_similarity_ranking_team.csv\"\n",
    ")\n",
    "\n",
    "WAV_FOLDER = Path(PROJECT_ROOT / \"data\" / \"wavs\")\n",
    "CONCEPT_ROOT = Path(PROJECT_ROOT / \"concept\" / \"temp_concepts\")\n",
    "\n",
    "# Pick one layer key you want TCAV on:\n",
    "# LAYER_KEY = \"stage5\"\n",
    "\n",
    "CONCEPT_SAMPLES = 100\n",
    "RANDOM_SAMPLES = 100\n",
    "BATCH_SIZE_CONCEPT = 1  # keep 1 (safe if variable T)\n",
    "FORCE_TRAIN_CAVS = True  # set True if you want to retrain CAVs\n",
    "\n",
    "OUT_CSV = Path(f\"stage5_temp_concepts_{LAYER_KEY}.csv\")\n",
    "\n",
    "assert ATTR_CSV_PATH.exists(), f\"Missing {ATTR_CSV_PATH}\"\n",
    "assert CONCEPT_ROOT.exists(), f\"Missing {CONCEPT_ROOT}\"\n",
    "HEAD_PATH = Path(PROJECT_ROOT / \"data\" / \"heads\" / \"resnet_293_speaker_head.pt\")\n",
    "\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "# %%\n",
    "speaker = load_model(PROJECT_ROOT / \"wespeaker-voxceleb-resnet293-LM\")\n",
    "net = speaker.model\n",
    "net = net.to(DEVICE)\n",
    "\n",
    "print(\"ResNet-293 loaded from HF\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# -------- Load your speaker head ckpt --------\n",
    "\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "l2_norm_emb = bool(ckpt.get(\"l2_norm_emb\", True))\n",
    "\n",
    "# infer in_dim from checkpoint\n",
    "fc_w = ckpt[\"state_dict\"][\"fc.weight\"]\n",
    "in_dim = int(fc_w.shape[1])\n",
    "num_classes = int(fc_w.shape[0])\n",
    "\n",
    "print(\"Loaded head:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"Head in_dim:\", in_dim, \"num_classes:\", num_classes, \"l2_norm_emb:\", l2_norm_emb)\n",
    "\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "head = SpeakerHead(in_dim=in_dim, num_classes=num_classes).to(DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n",
    "\n",
    "\n",
    "# %%\n",
    "class WeSpeakerWithHeadForGradCAM(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone: nn.Module, head: nn.Module, try_transpose: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self.try_transpose = try_transpose\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Run backbone\n",
    "        try:\n",
    "            out = self.backbone(feats)\n",
    "        except Exception:\n",
    "            if not self.try_transpose:\n",
    "                raise\n",
    "            out = self.backbone(feats.transpose(1, 2))\n",
    "\n",
    "        # Unwrap tuple/list (WeSpeaker behavior)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            emb = out[0]\n",
    "        else:\n",
    "            emb = out\n",
    "\n",
    "        # ---- HARD SHAPE GUARD ----\n",
    "        if emb.ndim == 0:\n",
    "            emb = emb.unsqueeze(0).unsqueeze(0)\n",
    "        elif emb.ndim == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "        elif emb.ndim > 2:\n",
    "            emb = emb.view(emb.size(0), -1)\n",
    "\n",
    "        # ---- HARD DEVICE GUARD (THIS FIXES YOUR ERROR) ----\n",
    "        emb = emb.to(next(self.head.parameters()).device)\n",
    "\n",
    "        logits = self.head(emb)\n",
    "\n",
    "        if logits.ndim == 1:\n",
    "            logits = logits.unsqueeze(0)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "wrapped_model = WeSpeakerWithHeadForGradCAM(speaker.model, head).to(DEVICE).eval()\n",
    "\n",
    "# Enable gradients for backbone (needed for Grad-CAM)\n",
    "for p in wrapped_model.backbone.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# Head grads not required (optional)\n",
    "for p in wrapped_model.head.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"wrapped_model ready\")\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# ========== TARGET LAYERS (same idea, but via wrapped_model.backbone) ==========\n",
    "TARGET_LAYERS = {\n",
    "    \"layer1.9.conv3\": wrapped_model.backbone.layer1[9].conv3,\n",
    "    \"layer2.19.conv3\": wrapped_model.backbone.layer2[19].conv3,\n",
    "    \"layer3.63.conv3\": wrapped_model.backbone.layer3[63].conv3,\n",
    "    \"layer4.2.conv3\": wrapped_model.backbone.layer4[2].conv3,\n",
    "}\n",
    "\n",
    "\n",
    "assert (\n",
    "    LAYER_KEY in TARGET_LAYERS\n",
    "), f\"{LAYER_KEY=} not in TARGET_LAYERS: {list(TARGET_LAYERS.keys())}\"\n",
    "\n",
    "\n",
    "# %%\n",
    "# -------- Resolve layer name string for Captum --------\n",
    "def module_name_in_model(model: torch.nn.Module, target_module: torch.nn.Module) -> str:\n",
    "    for name, mod in model.named_modules():\n",
    "        if mod is target_module:\n",
    "            return name\n",
    "    raise RuntimeError(\n",
    "        \"Could not find the selected layer module in wrapped_model.named_modules()\"\n",
    "    )\n",
    "\n",
    "\n",
    "layer_module = TARGET_LAYERS[LAYER_KEY]\n",
    "LAYER_NAME = module_name_in_model(wrapped_model, layer_module)\n",
    "print(\"Using layer:\", LAYER_KEY, \"->\", LAYER_NAME)\n",
    "\n",
    "\n",
    "# %%\n",
    "TCAV_DEVICE = torch.device(\"cpu\")\n",
    "print(\"TCAV_DEVICE =\", TCAV_DEVICE)\n",
    "\n",
    "redim_model = redim_model.to(TCAV_DEVICE).eval()\n",
    "wrapped_model = wrapped_model.to(TCAV_DEVICE).eval()\n",
    "\n",
    "DEVICE = TCAV_DEVICE\n",
    "\n",
    "\n",
    "# %%\n",
    "class ConceptNPYDataset(Dataset):\n",
    "    def __init__(self, concept_dir: Path, limit: int | None = None):\n",
    "        self.files = sorted(concept_dir.glob(\"*.npy\"))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No .npy found in {concept_dir}\")\n",
    "        if limit is not None:\n",
    "            self.files = self.files[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(self.files[idx]).astype(np.float32)  # (N_MELS, T)\n",
    "        if mel.shape[0] != N_MELS:\n",
    "            raise RuntimeError(\n",
    "                f\"{self.files[idx].name}: expected {N_MELS} bins, got {mel.shape}\"\n",
    "            )\n",
    "        x = torch.from_numpy(mel).unsqueeze(0)  # (1, N_MELS, T) on CPU\n",
    "        return x\n",
    "\n",
    "\n",
    "def infer_frames_for_random(concept_dirs: list[Path]) -> int:\n",
    "    for d in concept_dirs:\n",
    "        f = next(d.glob(\"*.npy\"), None)\n",
    "        if f is not None:\n",
    "            mel = np.load(f)\n",
    "            return int(mel.shape[1])\n",
    "    raise RuntimeError(\"Could not infer frames from concept dirs\")\n",
    "\n",
    "\n",
    "class RandomMelDataset(Dataset):\n",
    "    def __init__(self, n_samples: int, frames: int):\n",
    "        self.n_samples = n_samples\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = torch.randn(N_MELS, self.frames, dtype=torch.float32)\n",
    "        return mel.unsqueeze(0)\n",
    "\n",
    "\n",
    "# %%\n",
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "if not concept_dirs:\n",
    "    raise RuntimeError(f\"No concept folders in {CONCEPT_ROOT}\")\n",
    "\n",
    "concept_names = [d.name for d in concept_dirs]\n",
    "print(\"Concepts:\", concept_names)\n",
    "\n",
    "TARGET_FRAMES = infer_frames_for_random(concept_dirs)\n",
    "print(\"Using fixed frames for TCAV (from concepts):\", TARGET_FRAMES)\n",
    "tcav = TCAV(wrapped_model, [LAYER_NAME], test_split_ratio=0.33)\n",
    "\n",
    "positive_concepts = []\n",
    "for idx, cdir in enumerate(concept_dirs):\n",
    "    ds = ConceptNPYDataset(cdir, limit=CONCEPT_SAMPLES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "    positive_concepts.append(Concept(id=idx, name=cdir.name, data_iter=dl))\n",
    "\n",
    "rand_ds = RandomMelDataset(n_samples=RANDOM_SAMPLES, frames=TARGET_FRAMES)\n",
    "rand_dl = DataLoader(\n",
    "    rand_ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0\n",
    ")\n",
    "random_concept = Concept(id=len(positive_concepts), name=\"random\", data_iter=rand_dl)\n",
    "\n",
    "experimental_sets = [[c, random_concept] for c in positive_concepts]\n",
    "\n",
    "\n",
    "# %%\n",
    "def compute_cav_acc_df(\n",
    "    tcav: TCAV, positive_concepts: list[Concept], random_concept: Concept\n",
    ") -> pd.DataFrame:\n",
    "    cavs_dict = tcav.compute_cavs(\n",
    "        [[c, random_concept] for c in positive_concepts], force_train=FORCE_TRAIN_CAVS\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for concepts_key, layer_map in cavs_dict.items():\n",
    "        try:\n",
    "            pos_id = int(str(concepts_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_id < len(positive_concepts)):\n",
    "            continue\n",
    "        concept_name = positive_concepts[pos_id].name\n",
    "\n",
    "        for layer_name, cav_obj in layer_map.items():\n",
    "            if cav_obj is None or cav_obj.stats is None:\n",
    "                continue\n",
    "            acc = cav_obj.stats.get(\"accs\", None)\n",
    "            if acc is None:\n",
    "                acc = cav_obj.stats.get(\"acc\", None)\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.detach().cpu().item()\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"concept name\": concept_name,\n",
    "                    \"layer name\": layer_name,\n",
    "                    \"cav acc\": float(acc) if acc is not None else np.nan,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows, columns=[\"concept name\", \"layer name\", \"cav acc\"])\n",
    "\n",
    "\n",
    "acc_df = compute_cav_acc_df(tcav, positive_concepts, random_concept)\n",
    "print(acc_df.head())\n",
    "\n",
    "\n",
    "# %%\n",
    "def fix_mel_frames(mel_3d: torch.Tensor, target_frames: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel_3d: (1, N_MELS, T)\n",
    "    returns: (1, N_MELS, target_frames)\n",
    "    \"\"\"\n",
    "    T = int(mel_3d.shape[-1])\n",
    "    if T == target_frames:\n",
    "        return mel_3d\n",
    "    if T > target_frames:\n",
    "        start = (T - target_frames) // 2\n",
    "        return mel_3d[..., start : start + target_frames]\n",
    "    pad = target_frames - T\n",
    "    return F.pad(mel_3d, (0, pad), mode=\"constant\", value=0.0)\n",
    "\n",
    "\n",
    "def wav_path_to_mel4d(path: Path) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)  # (1, N_MELS, T)\n",
    "    mel = fix_mel_frames(mel, TARGET_FRAMES)  # (1, N_MELS, TARGET_FRAMES)\n",
    "    return mel.unsqueeze(0)  # (1, 1, N_MELS, TARGET_FRAMES)\n",
    "\n",
    "\n",
    "def predict_speaker(path: Path) -> tuple[str, float]:\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    with torch.no_grad():\n",
    "        logits = wrapped_model(x)  # (1, num_speakers)\n",
    "        probs = F.softmax(logits, dim=1)[0]\n",
    "        pred_id = int(torch.argmax(probs).item())\n",
    "        pred_name = id_to_speaker[pred_id]\n",
    "        pred_prob = float(probs[pred_id].item())\n",
    "    return pred_name, pred_prob\n",
    "\n",
    "\n",
    "# %%\n",
    "df_attr = pd.read_csv(ATTR_CSV_PATH)\n",
    "\n",
    "\n",
    "if \"path\" not in df_attr.columns or \"speaker\" not in df_attr.columns:\n",
    "    raise RuntimeError(\n",
    "        f\"CSV must contain columns ['path','speaker']. Got: {list(df_attr.columns)}\"\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, r in df_attr.iterrows():\n",
    "    path = Path(r[\"path\"])\n",
    "    true_label = str(r[\"speaker\"])\n",
    "\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    if true_label not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    pred_label, pred_prob = predict_speaker(path)\n",
    "\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    target_idx = speaker_to_id[true_label]\n",
    "\n",
    "    score_for_label = tcav.interpret(\n",
    "        inputs=x,\n",
    "        experimental_sets=experimental_sets,\n",
    "        target=target_idx,\n",
    "    )\n",
    "\n",
    "    for exp_key, layer_dict in score_for_label.items():\n",
    "        try:\n",
    "            pos_idx = int(str(exp_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_idx < len(positive_concepts)):\n",
    "            continue\n",
    "\n",
    "        concept_name = positive_concepts[pos_idx].name\n",
    "\n",
    "        for layer_name, metrics in layer_dict.items():\n",
    "            sc = metrics.get(\"sign_count\")\n",
    "            mg = metrics.get(\"magnitude\")\n",
    "            if sc is None or mg is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(sc, torch.Tensor):\n",
    "                sc = sc.detach().cpu().tolist()\n",
    "            if isinstance(mg, torch.Tensor):\n",
    "                mg = mg.detach().cpu().tolist()\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"path\": str(path),\n",
    "                    \"concept name\": concept_name,\n",
    "                    \"layer name\": layer_name,\n",
    "                    \"positive percentage\": float(sc[0]),\n",
    "                    \"magnitude\": float(mg[0]),\n",
    "                    \"true label\": true_label,\n",
    "                    \"predicted label\": pred_label,\n",
    "                    \"predicted probability\": float(pred_prob),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_tcav = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"path\",\n",
    "        \"concept name\",\n",
    "        \"layer name\",\n",
    "        \"positive percentage\",\n",
    "        \"magnitude\",\n",
    "        \"true label\",\n",
    "        \"predicted label\",\n",
    "        \"predicted probability\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_tcav = df_tcav.merge(acc_df, on=[\"concept name\", \"layer name\"], how=\"left\")\n",
    "\n",
    "df_tcav.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved →\", OUT_CSV)\n",
    "df_tcav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Project paths / device --------\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "ATTR_CSV_PATH = Path(\n",
    "    PROJECT_ROOT\n",
    "    / \"resnet_293\"\n",
    "    / \"speaker_similarity_ranking_team.csv\"\n",
    ")\n",
    "\n",
    "WAV_FOLDER = Path(PROJECT_ROOT / \"data\" / \"wavs\")\n",
    "CONCEPT_ROOT  = Path(PROJECT_ROOT / \"concept\" / \"temp_concepts\")\n",
    "\n",
    "# Pick one layer key you want TCAV on:\n",
    "# LAYER_KEY = \"stage5\"\n",
    "\n",
    "CONCEPT_SAMPLES = 100\n",
    "RANDOM_SAMPLES  = 100\n",
    "BATCH_SIZE_CONCEPT = 1  # keep 1 (safe if variable T)\n",
    "FORCE_TRAIN_CAVS = True  # set True if you want to retrain CAVs\n",
    "\n",
    "OUT_CSV = Path(f\"stage5_temp_concepts_{LAYER_KEY}.csv\")\n",
    "\n",
    "assert ATTR_CSV_PATH.exists(), f\"Missing {ATTR_CSV_PATH}\"\n",
    "assert CONCEPT_ROOT.exists(), f\"Missing {CONCEPT_ROOT}\"\n",
    "HEAD_PATH = Path(PROJECT_ROOT\n",
    "    / \"data\"\n",
    "    / \"heads\"\n",
    "    / \"resnet_293_speaker_head.pt\")\n",
    "\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = load_model(PROJECT_ROOT / \"wespeaker-voxceleb-resnet293-LM\")\n",
    "net = speaker.model\n",
    "net = net.to(DEVICE)\n",
    "\n",
    "print(\"ResNet-293 loaded from HF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Load your speaker head ckpt --------\n",
    "\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "l2_norm_emb = bool(ckpt.get(\"l2_norm_emb\", True))\n",
    "\n",
    "# infer in_dim from checkpoint\n",
    "fc_w = ckpt[\"state_dict\"][\"fc.weight\"]\n",
    "in_dim = int(fc_w.shape[1])\n",
    "num_classes = int(fc_w.shape[0])\n",
    "\n",
    "print(\"Loaded head:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"Head in_dim:\", in_dim, \"num_classes:\", num_classes, \"l2_norm_emb:\", l2_norm_emb)\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "head = SpeakerHead(in_dim=in_dim, num_classes=num_classes).to(DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeSpeakerWithHeadForGradCAM(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone: nn.Module, head: nn.Module, try_transpose: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self.try_transpose = try_transpose\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Run backbone\n",
    "        try:\n",
    "            out = self.backbone(feats)\n",
    "        except Exception:\n",
    "            if not self.try_transpose:\n",
    "                raise\n",
    "            out = self.backbone(feats.transpose(1, 2))\n",
    "\n",
    "        # Unwrap tuple/list (WeSpeaker behavior)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            emb = out[0]\n",
    "        else:\n",
    "            emb = out\n",
    "\n",
    "        # ---- HARD SHAPE GUARD ----\n",
    "        if emb.ndim == 0:\n",
    "            emb = emb.unsqueeze(0).unsqueeze(0)\n",
    "        elif emb.ndim == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "        elif emb.ndim > 2:\n",
    "            emb = emb.view(emb.size(0), -1)\n",
    "\n",
    "        # ---- HARD DEVICE GUARD (THIS FIXES YOUR ERROR) ----\n",
    "        emb = emb.to(next(self.head.parameters()).device)\n",
    "\n",
    "        logits = self.head(emb)\n",
    "\n",
    "        if logits.ndim == 1:\n",
    "            logits = logits.unsqueeze(0)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "wrapped_model = WeSpeakerWithHeadForGradCAM(speaker.model, head).to(DEVICE).eval()\n",
    "\n",
    "# Enable gradients for backbone (needed for Grad-CAM)\n",
    "for p in wrapped_model.backbone.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# Head grads not required (optional)\n",
    "for p in wrapped_model.head.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"wrapped_model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== TARGET LAYERS (same idea, but via wrapped_model.backbone) ==========\n",
    "TARGET_LAYERS = {\n",
    "    \"layer1.9.conv3\": wrapped_model.backbone.layer1[9].conv3,\n",
    "    \"layer2.19.conv3\": wrapped_model.backbone.layer2[19].conv3,\n",
    "    \"layer3.63.conv3\": wrapped_model.backbone.layer3[63].conv3,\n",
    "    \"layer4.2.conv3\": wrapped_model.backbone.layer4[2].conv3,\n",
    "}\n",
    "\n",
    "\n",
    "assert LAYER_KEY in TARGET_LAYERS, f\"{LAYER_KEY=} not in TARGET_LAYERS: {list(TARGET_LAYERS.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Resolve layer name string for Captum --------\n",
    "def module_name_in_model(model: torch.nn.Module, target_module: torch.nn.Module) -> str:\n",
    "    for name, mod in model.named_modules():\n",
    "        if mod is target_module:\n",
    "            return name\n",
    "    raise RuntimeError(\"Could not find the selected layer module in wrapped_model.named_modules()\")\n",
    "\n",
    "\n",
    "    \n",
    "layer_module = TARGET_LAYERS[LAYER_KEY]\n",
    "LAYER_NAME = module_name_in_model(wrapped_model, layer_module)\n",
    "print(\"Using layer:\", LAYER_KEY, \"->\", LAYER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCAV_DEVICE = torch.device(\"cpu\")\n",
    "print(\"TCAV_DEVICE =\", TCAV_DEVICE)\n",
    "\n",
    "redim_model = redim_model.to(TCAV_DEVICE).eval()\n",
    "wrapped_model = wrapped_model.to(TCAV_DEVICE).eval()\n",
    "\n",
    "DEVICE = TCAV_DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptNPYDataset(Dataset):\n",
    "    def __init__(self, concept_dir: Path, limit: int | None = None):\n",
    "        self.files = sorted(concept_dir.glob(\"*.npy\"))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No .npy found in {concept_dir}\")\n",
    "        if limit is not None:\n",
    "            self.files = self.files[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(self.files[idx]).astype(np.float32)  # (N_MELS, T)\n",
    "        if mel.shape[0] != N_MELS:\n",
    "            raise RuntimeError(f\"{self.files[idx].name}: expected {N_MELS} bins, got {mel.shape}\")\n",
    "        x = torch.from_numpy(mel).unsqueeze(0)  # (1, N_MELS, T) on CPU\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def infer_frames_for_random(concept_dirs: list[Path]) -> int:\n",
    "    for d in concept_dirs:\n",
    "        f = next(d.glob(\"*.npy\"), None)\n",
    "        if f is not None:\n",
    "            mel = np.load(f)\n",
    "            return int(mel.shape[1])\n",
    "    raise RuntimeError(\"Could not infer frames from concept dirs\")\n",
    "\n",
    "class RandomMelDataset(Dataset):\n",
    "    def __init__(self, n_samples: int, frames: int):\n",
    "        self.n_samples = n_samples\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = torch.randn(N_MELS, self.frames, dtype=torch.float32)  \n",
    "        return mel.unsqueeze(0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "if not concept_dirs:\n",
    "    raise RuntimeError(f\"No concept folders in {CONCEPT_ROOT}\")\n",
    "\n",
    "concept_names = [d.name for d in concept_dirs]\n",
    "print(\"Concepts:\", concept_names)\n",
    "\n",
    "TARGET_FRAMES = infer_frames_for_random(concept_dirs)\n",
    "print(\"Using fixed frames for TCAV (from concepts):\", TARGET_FRAMES)\n",
    "tcav = TCAV(wrapped_model, [LAYER_NAME], test_split_ratio=0.33)\n",
    "\n",
    "positive_concepts = []\n",
    "for idx, cdir in enumerate(concept_dirs):\n",
    "    ds = ConceptNPYDataset(cdir, limit=CONCEPT_SAMPLES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "    positive_concepts.append(Concept(id=idx, name=cdir.name, data_iter=dl))\n",
    "\n",
    "rand_ds = RandomMelDataset(n_samples=RANDOM_SAMPLES, frames=TARGET_FRAMES)\n",
    "rand_dl = DataLoader(rand_ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "random_concept = Concept(id=len(positive_concepts), name=\"random\", data_iter=rand_dl)\n",
    "\n",
    "experimental_sets = [[c, random_concept] for c in positive_concepts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cav_acc_df(tcav: TCAV, positive_concepts: list[Concept], random_concept: Concept) -> pd.DataFrame:\n",
    "    cavs_dict = tcav.compute_cavs([[c, random_concept] for c in positive_concepts], force_train=FORCE_TRAIN_CAVS)\n",
    "\n",
    "    rows = []\n",
    "    for concepts_key, layer_map in cavs_dict.items():\n",
    "        try:\n",
    "            pos_id = int(str(concepts_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_id < len(positive_concepts)):\n",
    "            continue\n",
    "        concept_name = positive_concepts[pos_id].name\n",
    "\n",
    "        for layer_name, cav_obj in layer_map.items():\n",
    "            if cav_obj is None or cav_obj.stats is None:\n",
    "                continue\n",
    "            acc = cav_obj.stats.get(\"accs\", None)\n",
    "            if acc is None:\n",
    "                acc = cav_obj.stats.get(\"acc\", None)\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.detach().cpu().item()\n",
    "            rows.append({\n",
    "                \"concept name\": concept_name,\n",
    "                \"layer name\": layer_name,\n",
    "                \"cav acc\": float(acc) if acc is not None else np.nan,\n",
    "            })\n",
    "    return pd.DataFrame(rows, columns=[\"concept name\", \"layer name\", \"cav acc\"])\n",
    "\n",
    "acc_df = compute_cav_acc_df(tcav, positive_concepts, random_concept)\n",
    "print(acc_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mel_frames(mel_3d: torch.Tensor, target_frames: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel_3d: (1, N_MELS, T)\n",
    "    returns: (1, N_MELS, target_frames)\n",
    "    \"\"\"\n",
    "    T = int(mel_3d.shape[-1])\n",
    "    if T == target_frames:\n",
    "        return mel_3d\n",
    "    if T > target_frames:\n",
    "        start = (T - target_frames) // 2\n",
    "        return mel_3d[..., start:start + target_frames]\n",
    "    pad = target_frames - T\n",
    "    return F.pad(mel_3d, (0, pad), mode=\"constant\", value=0.0)\n",
    "\n",
    "def wav_path_to_mel4d(path: Path) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)          # (1, N_MELS, T)\n",
    "    mel = fix_mel_frames(mel, TARGET_FRAMES) # (1, N_MELS, TARGET_FRAMES)\n",
    "    return mel.unsqueeze(0)                  # (1, 1, N_MELS, TARGET_FRAMES)\n",
    "\n",
    "def predict_speaker(path: Path) -> tuple[str, float]:\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    with torch.no_grad():\n",
    "        logits = wrapped_model(x)            # (1, num_speakers)\n",
    "        probs = F.softmax(logits, dim=1)[0]\n",
    "        pred_id = int(torch.argmax(probs).item())\n",
    "        pred_name = id_to_speaker[pred_id]\n",
    "        pred_prob = float(probs[pred_id].item())\n",
    "    return pred_name, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr = pd.read_csv(ATTR_CSV_PATH)\n",
    "\n",
    "\n",
    "if \"path\" not in df_attr.columns or \"speaker\" not in df_attr.columns:\n",
    "    raise RuntimeError(f\"CSV must contain columns ['path','speaker']. Got: {list(df_attr.columns)}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, r in df_attr.iterrows():\n",
    "    path = Path(r[\"path\"])\n",
    "    true_label = str(r[\"speaker\"])\n",
    "\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    if true_label not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    pred_label, pred_prob = predict_speaker(path)\n",
    "\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    target_idx = speaker_to_id[true_label]\n",
    "\n",
    "    score_for_label = tcav.interpret(\n",
    "        inputs=x,\n",
    "        experimental_sets=experimental_sets,\n",
    "        target=target_idx,\n",
    "    )\n",
    "\n",
    "    for exp_key, layer_dict in score_for_label.items():\n",
    "        try:\n",
    "            pos_idx = int(str(exp_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_idx < len(positive_concepts)):\n",
    "            continue\n",
    "\n",
    "        concept_name = positive_concepts[pos_idx].name\n",
    "\n",
    "        for layer_name, metrics in layer_dict.items():\n",
    "            sc = metrics.get(\"sign_count\")\n",
    "            mg = metrics.get(\"magnitude\")\n",
    "            if sc is None or mg is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(sc, torch.Tensor):\n",
    "                sc = sc.detach().cpu().tolist()\n",
    "            if isinstance(mg, torch.Tensor):\n",
    "                mg = mg.detach().cpu().tolist()\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(path),\n",
    "                \"concept name\": concept_name,\n",
    "                \"layer name\": layer_name,\n",
    "                \"positive percentage\": float(sc[0]),\n",
    "                \"magnitude\": float(mg[0]),\n",
    "                \"true label\": true_label,\n",
    "                \"predicted label\": pred_label,\n",
    "                \"predicted probability\": float(pred_prob),\n",
    "            })\n",
    "\n",
    "df_tcav = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"path\", \"concept name\", \"layer name\", \"positive percentage\", \"magnitude\",\n",
    "        \"true label\", \"predicted label\", \"predicted probability\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_tcav = df_tcav.merge(acc_df, on=[\"concept name\", \"layer name\"], how=\"left\")\n",
    "\n",
    "df_tcav.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved →\", OUT_CSV)\n",
    "df_tcav.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvResnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
