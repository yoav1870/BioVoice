{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SpeakerRec/.cache/torch/hub/IDRnD_ReDimNet_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ReDimNet successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[2]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from utils.spectogram_player_html import save_spectrogram_player_html\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "redim_model = torch.hub.load(\n",
    "    \"IDRnD/ReDimNet\",\n",
    "    \"ReDimNet\",\n",
    "    model_name=\"b5\",\n",
    "    train_type=\"ptn\",\n",
    "    dataset=\"vox2\",\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "print(\"Loaded ReDimNet successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded speaker head from: /home/SpeakerRec/BioVoice/redimnet/grad_cam/2.0/output/redim_speaker_head_linear.pt\n",
      "Speakers: ['eden', 'idan', 'yoav']\n",
      "L2-normalized embeddings: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1091198/4104975373.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "# another head model - YOAV!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "HEAD_PATH = Path.cwd() / \"output\" / \"redim_speaker_head_linear.pt\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "\n",
    "# Restore speaker mappings\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "\n",
    "# Define head with correct dimensions\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim=192, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "head = SpeakerHead(\n",
    "    in_dim=192,\n",
    "    num_classes=len(SPEAKERS)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n",
    "\n",
    "print(\"Loaded speaker head from:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"L2-normalized embeddings:\", ckpt.get(\"l2_norm_emb\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== 1) LOAD YOUR TRAINED FC HEAD (SPEAKER CLASSIFIER) ==========\n",
    "# SPEAKERS = [\"eden\", \"idan\", \"yoav\"]\n",
    "# speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "# id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "# class SpeakerHead(nn.Module):\n",
    "#     def __init__(self, in_dim=192, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# head = SpeakerHead(in_dim=192, num_classes=len(SPEAKERS)).to(DEVICE)\n",
    "\n",
    "# head_path = PROJECT_ROOT / \"data\" / \"redim_speaker_head.pt\"\n",
    "# assert head_path.exists(), f\"Missing head checkpoint: {head_path}\"\n",
    "\n",
    "# ckpt = torch.load(head_path, map_location=\"cpu\")\n",
    "# head.load_state_dict(ckpt[\"state_dict\"])\n",
    "# head.eval()\n",
    "\n",
    "# if \"speakers\" in ckpt:\n",
    "#     SPEAKERS = ckpt[\"speakers\"]\n",
    "#     speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "#     id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "# print(\"Loaded head from:\", head_path)\n",
    "# print(\"Speakers:\", SPEAKERS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float01(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def upsample_hw(arr: np.ndarray, size_hw: tuple[int, int],mode: str = \"bilinear\") -> np.ndarray:\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        t = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()  # [1,1,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0, 0].cpu().numpy()\n",
    "\n",
    "    if arr.ndim == 3 and arr.shape[2] == 3:\n",
    "        t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0).float()  # [1,3,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    raise ValueError(f\"Unsupported shape: {arr.shape}\")\n",
    "\n",
    "def plot_mel(mel, save_path=None, title=None):\n",
    "    if isinstance(mel, torch.Tensor):\n",
    "        mel = mel.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    mel_norm = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_norm, aspect=\"auto\", cmap=\"magma\", interpolation=\"bilinear\",origin=\"lower\")\n",
    "    plt.colorbar(label=\"Normalized Energy\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Frequency Bins\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def wav_to_model_mel(wav_path: str):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)  # [1, 80, T]\n",
    "\n",
    "    mel4d = mel.unsqueeze(0)  # [1, 1, 80, T]\n",
    "\n",
    "    mel_np = mel.squeeze(0).cpu().numpy()  # [80, T]\n",
    "    mel_norm = (mel_np - mel_np.min()) / (mel_np.max() - mel_np.min() + 1e-8)\n",
    "    rgb_base = np.stack([mel_norm] * 3, axis=-1)  # [80, T, 3]\n",
    "    rgb_base = np.flipud(rgb_base)  \n",
    "\n",
    "    return mel4d, rgb_base, sr\n",
    "\n",
    "\n",
    "def magma_rgb(img2d: np.ndarray) -> np.ndarray:\n",
    "    img2d = img2d.astype(np.float32)\n",
    "    img2d = (img2d - img2d.min()) / (img2d.max() - img2d.min() + 1e-8)\n",
    "    return cm.get_cmap(\"magma\")(img2d)[..., :3].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper (logits) ready.\n"
     ]
    }
   ],
   "source": [
    "class ReDimNetMelLogitsWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  mel4d [B, 1, 80, T]\n",
    "    Output: logits [B, 3]\n",
    "    \"\"\"\n",
    "    def __init__(self, redim_model, head, l2_norm_emb=True):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone\n",
    "        self.pool     = redim_model.pool\n",
    "        self.bn       = redim_model.bn\n",
    "        self.linear   = redim_model.linear  # [B, 192]\n",
    "        self.head     = head                # [B, 3]\n",
    "        self.l2_norm_emb = l2_norm_emb\n",
    "\n",
    "    def forward(self, mel4d):\n",
    "        x = self.backbone(mel4d)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn(x)\n",
    "        emb = self.linear(x)  # [B, 192]\n",
    "\n",
    "        # match head training: L2-normalized embeddings\n",
    "        if self.l2_norm_emb:\n",
    "            emb = emb / (emb.norm(p=2, dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        logits = self.head(emb)  # [B, 3]\n",
    "        return logits\n",
    "\n",
    "wrapped_model = ReDimNetMelLogitsWrapper(redim_model, head, l2_norm_emb=True).to(DEVICE).eval()\n",
    "print(\"Wrapper (logits) ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layers:\n",
      "stem -> Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "stage0 -> Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage1 -> Conv2d(32, 128, kernel_size=(2, 1), stride=(2, 1))\n",
      "stage2 -> Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage3 -> Conv2d(64, 128, kernel_size=(2, 1), stride=(2, 1))\n",
      "stage4 -> Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "stage5 -> Conv2d(128, 256, kernel_size=(2, 1), stride=(2, 1))\n"
     ]
    }
   ],
   "source": [
    "target_layers = [\n",
    "    wrapped_model.backbone.stem[0],\n",
    "    wrapped_model.backbone.stage0[2],\n",
    "    wrapped_model.backbone.stage1[2],\n",
    "    wrapped_model.backbone.stage2[2],\n",
    "    wrapped_model.backbone.stage3[2],\n",
    "    wrapped_model.backbone.stage4[2],\n",
    "    wrapped_model.backbone.stage5[2],\n",
    "]\n",
    "layer_names = [\"stem\", \"stage0\", \"stage1\", \"stage2\", \"stage3\", \"stage4\", \"stage5\"]\n",
    "\n",
    "print(\"Target layers:\")\n",
    "for n, l in zip(layer_names, target_layers):\n",
    "    print(n, \"->\", l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassLogitTarget:\n",
    "    def __init__(self, class_idx: int):\n",
    "        self.class_idx = int(class_idx)\n",
    "\n",
    "    def __call__(self, model_output: torch.Tensor) -> torch.Tensor:\n",
    "        if model_output.ndim == 1:\n",
    "            return model_output[self.class_idx]\n",
    "        if model_output.ndim == 2:\n",
    "            return model_output[:, self.class_idx].sum()\n",
    "        raise ValueError(f\"Unexpected model_output shape: {tuple(model_output.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim_model.eval()\n",
    "head.eval()\n",
    "\n",
    "for p in redim_model.parameters():\n",
    "    p.requires_grad = True   # needed for Grad-CAM\n",
    "\n",
    "for p in head.parameters():\n",
    "    p.requires_grad = False  # optional (head not needed to have grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_gradcam_speaker(\n",
    "#     wav_path: str,\n",
    "#     target_speaker: str,\n",
    "#     save_dir: str = \"gradcam_results_cls\",\n",
    "#     upscale: int = 10,\n",
    "#     cam_quantile: float = 0.85,\n",
    "# ):\n",
    "#     assert target_speaker in speaker_to_id, f\"Unknown speaker: {target_speaker}\"\n",
    "\n",
    "#     save_dir = Path(save_dir)\n",
    "#     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     fname = Path(wav_path).stem\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 1) Compute model Mel\n",
    "#     # ----------------------------\n",
    "#     mel4d, rgb_base = wav_to_model_mel(wav_path)  # mel4d: [1,1,80,T]\n",
    "#     mel4d = mel4d.detach().to(DEVICE)\n",
    "\n",
    "#     mel2d = mel4d.squeeze(0).squeeze(0).cpu().numpy()  # [80, T]\n",
    "#     mel_norm = (mel2d - mel2d.min()) / (mel2d.max() - mel2d.min() + 1e-8)\n",
    "\n",
    "#     # Save original mel\n",
    "#     originals_dir = save_dir / \"original\"\n",
    "#     originals_dir.mkdir(exist_ok=True)\n",
    "#     plot_mel(\n",
    "#         mel2d,\n",
    "#         save_path=originals_dir / f\"{fname}_mel.png\",\n",
    "#         title=f\"{fname} (target={target_speaker})\",\n",
    "#     )\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 2) Target definition\n",
    "#     # ----------------------------\n",
    "#     target = ClassLogitTarget(speaker_to_id[target_speaker])\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 3) Iterate over ALL layers\n",
    "#     # ----------------------------\n",
    "#     layer_stats = {}\n",
    "\n",
    "#     for layer, lname in zip(target_layers, layer_names):\n",
    "#         print(f\"[Grad-CAM] Layer: {lname}\")\n",
    "\n",
    "#         layer_dir = save_dir / lname\n",
    "#         layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         wrapped_model.zero_grad(set_to_none=True)\n",
    "\n",
    "#         cam = GradCAM(\n",
    "#             model=wrapped_model,\n",
    "#             target_layers=[layer],\n",
    "#         )\n",
    "\n",
    "#         grayscale_cam = cam(\n",
    "#             input_tensor=mel4d,\n",
    "#             targets=[target],\n",
    "#             aug_smooth=False,\n",
    "#             eigen_smooth=True,\n",
    "#         )[0]  # [H', W']\n",
    "\n",
    "#         # Flip to match mel orientation\n",
    "#         grayscale_cam = np.flipud(grayscale_cam)\n",
    "\n",
    "#         # ----------------------------\n",
    "#         # 4) Align CAM to Mel space\n",
    "#         # ----------------------------\n",
    "#         cam_on_mel = upsample_hw(grayscale_cam, mel2d.shape, mode=\"bicubic\")\n",
    "\n",
    "#         # Normalize CAM (continuous, NOT binary)\n",
    "#         cam_norm = (cam_on_mel - cam_on_mel.min()) / (\n",
    "#             cam_on_mel.max() - cam_on_mel.min() + 1e-8\n",
    "#         )\n",
    "\n",
    "#         # ----------------------------\n",
    "#         # 5) Continuous CAM-weighted Mel\n",
    "#         # ----------------------------\n",
    "#         mel_weighted = mel2d * cam_norm\n",
    "\n",
    "#         plot_mel(\n",
    "#             mel_weighted,\n",
    "#             save_path=layer_dir / f\"{fname}_mel_weighted.png\",\n",
    "#             title=f\"{fname} | {lname} | weighted\",\n",
    "#         )\n",
    "\n",
    "#         # ----------------------------\n",
    "#         # 6) High-activation focus (relative threshold)\n",
    "#         # ----------------------------\n",
    "#         thr = np.quantile(cam_norm, cam_quantile)\n",
    "#         mask = cam_norm >= thr\n",
    "\n",
    "#         mel_focus = np.where(mask, mel2d, mel2d.min())\n",
    "\n",
    "#         plot_mel(\n",
    "#             mel_focus,\n",
    "#             save_path=layer_dir / f\"{fname}_mel_focus_q{cam_quantile}.png\",\n",
    "#             title=f\"{fname} | {lname} | focus\",\n",
    "#         )\n",
    "\n",
    "#         # ----------------------------\n",
    "#         # 7) Visualization overlay\n",
    "#         # ----------------------------\n",
    "#         H, W = mel_norm.shape\n",
    "#         H2, W2 = H * upscale, W * upscale\n",
    "\n",
    "#         rgb_big = upsample_hw(\n",
    "#             np.stack([mel_norm] * 3, axis=-1),\n",
    "#             (H2, W2),\n",
    "#             mode=\"bicubic\",\n",
    "#         )\n",
    "\n",
    "#         cam_big = upsample_hw(cam_norm, (H2, W2), mode=\"bicubic\")\n",
    "#         overlay = show_cam_on_image(rgb_big, cam_big, use_rgb=True)\n",
    "\n",
    "#         plt.imsave(\n",
    "#             layer_dir / f\"{fname}_overlay.png\",\n",
    "#             overlay,\n",
    "#         )\n",
    "\n",
    "#         # ----------------------------\n",
    "#         # 8) Quantitative layer stats\n",
    "#         # ----------------------------\n",
    "#         entropy = -np.sum(\n",
    "#             cam_norm * np.log(cam_norm + 1e-8)\n",
    "#         ) / cam_norm.size\n",
    "\n",
    "#         support = (cam_norm > 0.5).mean()\n",
    "\n",
    "#         layer_stats[lname] = {\n",
    "#             \"entropy\": float(entropy),\n",
    "#             \"support@0.5\": float(support),\n",
    "#         }\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 9) Save stats summary\n",
    "#     # ----------------------------\n",
    "#     stats_df = pd.DataFrame(layer_stats).T\n",
    "#     stats_df.to_csv(save_dir / \"layer_cam_stats.csv\")\n",
    "\n",
    "#     print(\"Saved Grad-CAM results to:\", save_dir)\n",
    "#     print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hz_to_mel(hz: np.ndarray, mel_scale: str = \"htk\") -> np.ndarray:\n",
    "    hz = np.asarray(hz, dtype=np.float64)\n",
    "    mel_scale = mel_scale.lower()\n",
    "    if mel_scale == \"htk\":\n",
    "        return 2595.0 * np.log10(1.0 + hz / 700.0)\n",
    "\n",
    "    # slaney (librosa-style)\n",
    "    f_sp = 200.0 / 3\n",
    "    brkfrq = 1000.0\n",
    "    brkpt = brkfrq / f_sp  # 15\n",
    "    logstep = np.exp(np.log(6.4) / 27.0)\n",
    "\n",
    "    mel = np.empty_like(hz)\n",
    "    lin = hz < brkfrq\n",
    "    mel[lin] = hz[lin] / f_sp\n",
    "    mel[~lin] = brkpt + np.log(hz[~lin] / brkfrq) / np.log(logstep)\n",
    "    return mel\n",
    "\n",
    "def mel_to_hz(mel: np.ndarray, mel_scale: str = \"htk\") -> np.ndarray:\n",
    "    mel = np.asarray(mel, dtype=np.float64)\n",
    "    mel_scale = mel_scale.lower()\n",
    "    if mel_scale == \"htk\":\n",
    "        return 700.0 * (10.0 ** (mel / 2595.0) - 1.0)\n",
    "\n",
    "    # slaney (librosa-style)\n",
    "    f_sp = 200.0 / 3\n",
    "    brkfrq = 1000.0\n",
    "    brkpt = brkfrq / f_sp  # 15\n",
    "    logstep = np.exp(np.log(6.4) / 27.0)\n",
    "\n",
    "    hz = np.empty_like(mel)\n",
    "    lin = mel < brkpt\n",
    "    hz[lin] = f_sp * mel[lin]\n",
    "    hz[~lin] = brkfrq * (logstep ** (mel[~lin] - brkpt))\n",
    "    return hz\n",
    "\n",
    "def mel_bin_centers_hz(n_mels: int, f_min: float, f_max: float, mel_scale: str = \"htk\") -> np.ndarray:\n",
    "    m_min = hz_to_mel(f_min, mel_scale)\n",
    "    m_max = hz_to_mel(f_max, mel_scale)\n",
    "    mel_points = np.linspace(m_min, m_max, n_mels + 2)  # endpoints + centers\n",
    "    center_mels = mel_points[1:-1]\n",
    "    return mel_to_hz(center_mels, mel_scale)\n",
    "\n",
    "def _get_spec_params(spec, sr_fallback: int):\n",
    "    # Try common attribute names; fall back safely.\n",
    "    f_min = getattr(spec, \"f_min\", None)\n",
    "    if f_min is None:\n",
    "        f_min = getattr(spec, \"fmin\", 0.0)\n",
    "\n",
    "    f_max = getattr(spec, \"f_max\", None)\n",
    "    if f_max is None:\n",
    "        f_max = getattr(spec, \"fmax\", None)\n",
    "\n",
    "    mel_scale = getattr(spec, \"mel_scale\", \"htk\")\n",
    "\n",
    "    # If spec doesn't provide f_max, assume Nyquist.\n",
    "    if f_max is None:\n",
    "        f_max = sr_fallback / 2.0\n",
    "\n",
    "    return float(f_min), float(f_max), str(mel_scale)\n",
    "\n",
    "\n",
    "def plot_mel_with_hz_axis(mel2d: np.ndarray, sr: int, spec=None, save_path=None, title=None):\n",
    "    if isinstance(mel2d, torch.Tensor):\n",
    "        mel2d = mel2d.detach().cpu().numpy()\n",
    "\n",
    "    n_mels, T = mel2d.shape\n",
    "\n",
    "    if spec is None:\n",
    "        f_min, f_max, mel_scale = 0.0, sr / 2.0, \"htk\"\n",
    "    else:\n",
    "        f_min, f_max, mel_scale = _get_spec_params(spec, sr_fallback=sr)\n",
    "\n",
    "    centers_hz = mel_bin_centers_hz(n_mels, f_min, f_max, mel_scale)\n",
    "\n",
    "    mel_norm = (mel2d - mel2d.min()) / (mel2d.max() - mel2d.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(\n",
    "        mel_norm,\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"magma\",\n",
    "        interpolation=\"bilinear\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "\n",
    "    idxs = np.linspace(0, n_mels - 1, 8).round().astype(int)\n",
    "    plt.yticks(idxs, [f\"{centers_hz[i]:.0f}\" for i in idxs])\n",
    "\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=400)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam_speaker(\n",
    "    wav_path: str,\n",
    "    target_speaker: str,\n",
    "    save_dir: str = \"gradcam_results_cls\",\n",
    "    thr: float = 0.3,\n",
    "    upscale: int = 8,\n",
    "):\n",
    "    assert target_speaker in speaker_to_id, f\"Unknown speaker: {target_speaker}\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fname = Path(wav_path).stem\n",
    "\n",
    "    originals_dir = save_dir / \"original\"\n",
    "    originals_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    mel4d, rgb_base, sr = wav_to_model_mel(wav_path)\n",
    "    mel4d = mel4d.detach()\n",
    "\n",
    "    rgb_base01 = to_float01(rgb_base)\n",
    "\n",
    "    \n",
    "    plot_mel(\n",
    "        mel4d.squeeze(0),\n",
    "        save_path=originals_dir / f\"{fname}_original_mel.png\",\n",
    "        title=f\"{fname} (target={target_speaker})\",\n",
    "    )\n",
    "    \n",
    "    target = ClassLogitTarget(speaker_to_id[target_speaker])\n",
    "\n",
    "    for layer, lname in zip(target_layers, layer_names):\n",
    "        layer_dir = save_dir / lname\n",
    "        layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        wrapped_model.zero_grad(set_to_none=True)\n",
    "        cam = GradCAM(model=wrapped_model, target_layers=[layer])\n",
    "\n",
    "        grayscale_cam = cam(\n",
    "            input_tensor=mel4d,\n",
    "            targets=[target],\n",
    "            aug_smooth=False,\n",
    "            eigen_smooth=True,\n",
    "        )[0]\n",
    "\n",
    "        grayscale_cam = np.flipud(grayscale_cam)\n",
    "\n",
    "        h, w = rgb_base01.shape[:2]\n",
    "        H2, W2 = h * upscale, w * upscale\n",
    "\n",
    "        rgb_big = upsample_hw(rgb_base01, (H2, W2), mode=\"bilinear\")\n",
    "        cam_big = upsample_hw(grayscale_cam, (H2, W2))\n",
    "\n",
    "        mask_big = (cam_big >= thr).astype(np.float32)\n",
    "        cam_big_masked = cam_big * mask_big\n",
    "\n",
    "        overlay_big = show_cam_on_image(rgb_big, cam_big_masked, use_rgb=True)\n",
    "\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay_{target_speaker}_thr{thr:.2f}.png\", overlay_big)\n",
    "        \n",
    "        rgb_big = magma_rgb(rgb_big[..., 0]) \n",
    "        mel_only = rgb_big * mask_big[..., None]\n",
    "        mel_only = np.clip(mel_only, 0.0, 1.0)\n",
    "\n",
    "        plt.imsave(\n",
    "            layer_dir / f\"{fname}_mel_masked_{target_speaker}_thr{thr:.2f}.png\",\n",
    "            mel_only\n",
    "        )\n",
    "        save_spectrogram_player_html(\n",
    "            audio_path=wav_path,\n",
    "            spectrogram_png_path=layer_dir / f\"{fname}_mel_masked_{target_speaker}_thr{thr:.2f}.png\",\n",
    "            out_html_path=layer_dir / f\"{fname}_mel_masked_{target_speaker}_thr{thr:.2f}.html\",\n",
    "            total_time_sec=None,\n",
    "            copy_audio=True,\n",
    "            embed_image=True,\n",
    "        )\n",
    "        mel = mel4d.squeeze(0)\n",
    "        mel2d = mel[0] if mel.ndim == 3 else mel  # (80, T)\n",
    "\n",
    "        mel2d_np = mel2d.detach().float().cpu().numpy()\n",
    "        cam_on_mel = upsample_hw(grayscale_cam, mel2d_np.shape)\n",
    "        mel_masked = np.where(cam_on_mel >= thr, mel2d_np, mel2d_np.min())\n",
    "        mel_masked = np.flipud(mel_masked)\n",
    "        plot_mel(\n",
    "            mel_masked,\n",
    "            save_path=layer_dir / f\"{fname}_mel_masked_plot_{target_speaker}_thr{thr:.2f}.png\",\n",
    "            title=f\"{fname} ({lname}, target={target_speaker})\",\n",
    "        )\n",
    "        plt.imsave(originals_dir / f\"{fname}_rgb.png\", rgb_big )\n",
    "\n",
    "        spec_png = originals_dir / f\"{fname}_rgb.png\"\n",
    "\n",
    "        print(f\"Saved: {layer_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] missing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwav_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mrun_gradcam_speaker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwav_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_speaker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgradcam_results_3.0//\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwav_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 39\u001b[0m, in \u001b[0;36mrun_gradcam_speaker\u001b[0;34m(wav_path, target_speaker, save_dir, thr, upscale)\u001b[0m\n\u001b[1;32m     36\u001b[0m wrapped_model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m cam \u001b[38;5;241m=\u001b[39m GradCAM(model\u001b[38;5;241m=\u001b[39mwrapped_model, target_layers\u001b[38;5;241m=\u001b[39m[layer])\n\u001b[0;32m---> 39\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel4d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     46\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflipud(grayscale_cam)\n\u001b[1;32m     48\u001b[0m h, w \u001b[38;5;241m=\u001b[39m rgb_base01\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:209\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:129\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__htcore\u001b[38;5;241m.\u001b[39mmark_step()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:164\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[1;32m    162\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 164\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    166\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:88\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid activation shape. Get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(activations\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eigen_smooth:\n\u001b[0;32m---> 88\u001b[0m     cam \u001b[38;5;241m=\u001b[39m \u001b[43mget_2d_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     cam \u001b[38;5;241m=\u001b[39m weighted_activations\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/pytorch_grad_cam/utils/svd_on_activations.py:16\u001b[0m, in \u001b[0;36mget_2d_projection\u001b[0;34m(activation_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Centering before the SVD seems to be important here,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Otherwise the image returned is negative\u001b[39;00m\n\u001b[1;32m     14\u001b[0m reshaped_activations \u001b[38;5;241m=\u001b[39m reshaped_activations \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     15\u001b[0m     reshaped_activations\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m U, S, VT \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m projection \u001b[38;5;241m=\u001b[39m reshaped_activations \u001b[38;5;241m@\u001b[39m VT[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     18\u001b[0m projection \u001b[38;5;241m=\u001b[39m projection\u001b[38;5;241m.\u001b[39mreshape(activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/BioVoice/.venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:1657\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[1;32m   1656\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1657\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1659\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wav_dir = PROJECT_ROOT / \"data\" / \"augmented_wavs\"\n",
    "\n",
    "# test_wav = wav_dir / \"idan_012.wav\"\n",
    "# run_out_dir = Path(f\"gradcam_results_2.0/idan/{test_wav.stem}\")\n",
    "# originals_dir = run_out_dir / \"original\"\n",
    "\n",
    "# run_gradcam_speaker(\n",
    "#     wav_path=str(test_wav),\n",
    "#     target_speaker=\"idan\",\n",
    "#     save_dir=str(run_out_dir),\n",
    "#     thr=0.2,\n",
    "#     upscale=12,\n",
    "# )\n",
    "\n",
    "# top3_by_user = {\n",
    "#     \"eden\": [\"eden_017.wav\", \"eden_021.wav\", \"eden_012.wav\"],\n",
    "#     \"idan\": [\"idan_009.wav\", \"idan_004.wav\", \"idan_012.wav\"],\n",
    "#     \"yoav\": [\"yoav_028.wav\", \"yoav_024.wav\", \"yoav_022.wav\"],\n",
    "# }\n",
    "\n",
    "wav_by_user: dict[str, list[Path]] = {}\n",
    "for spk in SPEAKERS:  \n",
    "    wav_by_user[spk] = list(wav_dir.glob(f\"{spk}_*.wav\"))\n",
    "    \n",
    "for spk, fnames in wav_by_user.items():\n",
    "    for fname in fnames:\n",
    "        wav_path = wav_dir / fname\n",
    "        if not wav_path.exists():\n",
    "            print(f\"[WARN] missing file: {wav_path}\")\n",
    "            continue\n",
    "\n",
    "        run_gradcam_speaker(\n",
    "            wav_path=str(wav_path),\n",
    "            target_speaker=spk,\n",
    "            save_dir=f\"gradcam_results_3.0//{spk}/{wav_path.stem}\",\n",
    "            thr=0.2,      \n",
    "            upscale=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
