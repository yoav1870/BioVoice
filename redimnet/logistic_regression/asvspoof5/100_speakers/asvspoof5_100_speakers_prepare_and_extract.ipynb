{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ASVspoof5 100-Speakers Plan + Selective Extraction (train/val/test)\n",
    "\n",
    "Design in this notebook:\n",
    "\n",
    "- `100` speakers total\n",
    "- split groups: `train/val/test = 60/20/20`\n",
    "- partition mix per split:\n",
    "  - `train`: 15(train partition) / 15(dev partition) / 30(eval partition)\n",
    "  - `val`: 5 / 5 / 10\n",
    "  - `test`: 5 / 5 / 10\n",
    "- per speaker quota: `32 bonafide + 48 spoof`\n",
    "- spoof quota by partition:\n",
    "  - `train` partition (`A01-A08`): `6` per system\n",
    "  - `dev` partition (`A09-A16`): `6` per system\n",
    "  - `eval` partition (`A17-A32`): `3` per system\n",
    "\n",
    "This notebook does two things:\n",
    "1. Builds the selection manifest CSV used by the 100-speaker logistic notebooks\n",
    "2. Selectively extracts only needed audio from tar shards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _find_project_root_from_cwd() -> Path | None:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for cand in [cwd, *cwd.parents]:\n",
    "        if (cand / 'ASVspoof5_protocols').exists() and (cand / 'redimnet').exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "_detected_root = _find_project_root_from_cwd()\n",
    "PROJECT_ROOT = _detected_root if _detected_root is not None else Path('/home/SpeakerRec/BioVoice')\n",
    "\n",
    "PROTOCOL_PATHS = {\n",
    "    'train': PROJECT_ROOT / 'ASVspoof5_protocols' / 'ASVspoof5.train.tsv',\n",
    "    'dev': PROJECT_ROOT / 'ASVspoof5_protocols' / 'ASVspoof5.dev.track_1.tsv',\n",
    "    'eval': PROJECT_ROOT / 'ASVspoof5_protocols' / 'ASVspoof5.eval.track_1.tsv',\n",
    "}\n",
    "\n",
    "# Adjust these to your local tar folders if needed.\n",
    "TAR_DIRS = {\n",
    "    'train': PROJECT_ROOT / 'ASVspoof5_audio_train_tars',\n",
    "    'dev': PROJECT_ROOT / 'ASVspoof5_audio_dev_tars',\n",
    "    'eval': PROJECT_ROOT / 'ASVspoof5_audio_eval_tars',\n",
    "}\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / 'ASVspoof5_protocols' / 'subset_100_speakers_outputs'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR = PROJECT_ROOT / 'redimnet' / 'logistic_regression' / 'asvspoof5' / '100_speakers'\n",
    "NOTEBOOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selected_utterances_plan.csv'\n",
    "SPEAKER_PLAN_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selected_speakers_plan.csv'\n",
    "AUDIT_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_selection_audit.csv'\n",
    "SUMMARY_OUT = NOTEBOOK_DIR / 'asvspoof5_100_speakers_plan_summary.json'\n",
    "\n",
    "EXTRACT_DIR = PROJECT_ROOT / 'data' / 'datasets' / 'asvspoof5_100_speakers_32_real_48_spoof'\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "BONA_PER_SPK = 32\n",
    "\n",
    "GROUP_PARTITION_SPEAKERS = {\n",
    "    'train': {'train': 15, 'dev': 15, 'eval': 30},\n",
    "    'val':   {'train': 5,  'dev': 5,  'eval': 10},\n",
    "    'test':  {'train': 5,  'dev': 5,  'eval': 10},\n",
    "}\n",
    "\n",
    "SYSTEMS_BY_PARTITION = {\n",
    "    'train': [f'A{i:02d}' for i in range(1, 9)],\n",
    "    'dev': [f'A{i:02d}' for i in range(9, 17)],\n",
    "    'eval': [f'A{i:02d}' for i in range(17, 33)],\n",
    "}\n",
    "\n",
    "SPOOF_QUOTA_BY_PARTITION = {\n",
    "    'train': 6,\n",
    "    'dev': 6,\n",
    "    'eval': 3,\n",
    "}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for lbl in ['bonafide', 'spoof']:\n",
    "        (EXTRACT_DIR / split / lbl).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('PROJECT_ROOT =', PROJECT_ROOT)\n",
    "print('OUT_DIR =', OUT_DIR)\n",
    "print('NOTEBOOK_DIR =', NOTEBOOK_DIR)\n",
    "print('MANIFEST_OUT =', MANIFEST_OUT)\n",
    "print('EXTRACT_DIR =', EXTRACT_DIR)\n",
    "for part, p in PROTOCOL_PATHS.items():\n",
    "    print(f'PROTOCOL[{part}] =', p, '| exists =', p.exists())\n",
    "for part, p in TAR_DIRS.items():\n",
    "    print(f'TAR_DIR[{part}] =', p, '| exists =', p.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['speaker_id','utt_id','gender','codec_id','codec_q','source_utt_id','attack_codec_id','system_id','label','unused']\n",
    "\n",
    "\n",
    "def load_protocol(partition: str, path: Path) -> pd.DataFrame:\n",
    "    assert path.exists(), f'Missing protocol file: {path}'\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            t = line.strip().split()\n",
    "            if len(t) != 10:\n",
    "                continue\n",
    "            rows.append(dict(zip(cols, t)))\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['partition'] = partition\n",
    "    return df\n",
    "\n",
    "all_parts = []\n",
    "for part in ['train', 'dev', 'eval']:\n",
    "    d = load_protocol(part, PROTOCOL_PATHS[part])\n",
    "    all_parts.append(d)\n",
    "\n",
    "df = pd.concat(all_parts, ignore_index=True)\n",
    "\n",
    "print('Total rows:', len(df))\n",
    "print('Total unique speakers:', df['speaker_id'].nunique())\n",
    "print(df.groupby('partition')['speaker_id'].nunique().rename('speakers'))\n",
    "print('Label counts by partition:')\n",
    "print(df.groupby(['partition','label']).size().unstack(fill_value=0))\n",
    "print('Spoof systems by partition:')\n",
    "for part in ['train', 'dev', 'eval']:\n",
    "    systems = sorted(df[(df['partition']==part) & (df['label']=='spoof')]['system_id'].unique().tolist())\n",
    "    print(part, systems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_gender_balanced(eligible_df: pd.DataFrame, n_pick: int, seed: int) -> pd.DataFrame:\n",
    "    eligible_df = eligible_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    f = eligible_df[eligible_df['gender'].eq('F')].copy()\n",
    "    m = eligible_df[eligible_df['gender'].eq('M')].copy()\n",
    "\n",
    "    target_f = n_pick // 2\n",
    "    target_m = n_pick - target_f\n",
    "\n",
    "    if len(f) < target_f:\n",
    "        target_f = len(f)\n",
    "        target_m = n_pick - target_f\n",
    "    if len(m) < target_m:\n",
    "        target_m = len(m)\n",
    "        target_f = n_pick - target_m\n",
    "\n",
    "    picked = pd.concat([f.head(target_f), m.head(target_m)], ignore_index=True)\n",
    "    if len(picked) < n_pick:\n",
    "        used = set(picked['speaker_id'])\n",
    "        extra = eligible_df[~eligible_df['speaker_id'].isin(used)].head(n_pick - len(picked))\n",
    "        picked = pd.concat([picked, extra], ignore_index=True)\n",
    "\n",
    "    return picked.drop_duplicates('speaker_id').head(n_pick).copy()\n",
    "\n",
    "\n",
    "required_by_partition = {\n",
    "    part: sum(GROUP_PARTITION_SPEAKERS[g][part] for g in ['train','val','test'])\n",
    "    for part in ['train','dev','eval']\n",
    "}\n",
    "print('Required speakers by partition:', required_by_partition)\n",
    "\n",
    "selected_rows = []\n",
    "\n",
    "for part in ['train','dev','eval']:\n",
    "    dfp = df[df['partition'].eq(part)].copy()\n",
    "    systems = SYSTEMS_BY_PARTITION[part]\n",
    "    spoof_q = SPOOF_QUOTA_BY_PARTITION[part]\n",
    "\n",
    "    bona_counts = dfp[dfp['label'].eq('bonafide')].groupby('speaker_id').size().rename('bonafide_n')\n",
    "    spoof_piv = (\n",
    "        dfp[dfp['label'].eq('spoof')]\n",
    "        .groupby(['speaker_id','system_id']).size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    for s in systems:\n",
    "        if s not in spoof_piv.columns:\n",
    "            spoof_piv[s] = 0\n",
    "    spoof_piv = spoof_piv[systems]\n",
    "\n",
    "    gender_ser = dfp.groupby('speaker_id')['gender'].agg(lambda x: x.iloc[0]).rename('gender')\n",
    "    summary = pd.concat([bona_counts, spoof_piv, gender_ser], axis=1).fillna(0)\n",
    "    summary['n_systems'] = (summary[systems] > 0).sum(axis=1)\n",
    "    summary['min_system_n'] = summary[systems].min(axis=1)\n",
    "\n",
    "    eligible = summary[\n",
    "        (summary['bonafide_n'] >= BONA_PER_SPK) &\n",
    "        (summary['n_systems'] == len(systems)) &\n",
    "        (summary['min_system_n'] >= spoof_q)\n",
    "    ].reset_index()\n",
    "\n",
    "    need = required_by_partition[part]\n",
    "    print(f'[{part}] eligible speakers:', len(eligible), '| need:', need)\n",
    "    assert len(eligible) >= need, f'Not enough eligible speakers in {part}'\n",
    "\n",
    "    picked = sample_gender_balanced(eligible, need, seed=SEED + {'train': 11, 'dev': 22, 'eval': 33}[part]).copy()\n",
    "    picked['partition'] = part\n",
    "\n",
    "    # assign split groups inside each partition according to plan\n",
    "    picked = picked.sample(frac=1.0, random_state=SEED + {'train': 101, 'dev': 202, 'eval': 303}[part]).reset_index(drop=True)\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    for g in ['train','val','test']:\n",
    "        k = GROUP_PARTITION_SPEAKERS[g][part]\n",
    "        chunk = picked.iloc[start:start+k].copy()\n",
    "        chunk['group'] = g\n",
    "        chunks.append(chunk)\n",
    "        start += k\n",
    "    part_sel = pd.concat(chunks, ignore_index=True)\n",
    "    selected_rows.append(part_sel)\n",
    "\n",
    "sel = pd.concat(selected_rows, ignore_index=True)\n",
    "sel = sel[['group','partition','speaker_id','gender','bonafide_n','min_system_n','n_systems']]\n",
    "sel = sel.sort_values(['group','partition','speaker_id']).reset_index(drop=True)\n",
    "\n",
    "print('Selected speakers total:', sel['speaker_id'].nunique())\n",
    "print('By group:', sel['group'].value_counts().sort_index().to_dict())\n",
    "print('By partition:', sel['partition'].value_counts().sort_index().to_dict())\n",
    "print('By group+partition:')\n",
    "print(sel.groupby(['group','partition']).size().unstack(fill_value=0))\n",
    "print('Gender by group:')\n",
    "print(sel.groupby(['group','gender']).size().unstack(fill_value=0))\n",
    "\n",
    "display(sel.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_rows = []\n",
    "audit_rows = []\n",
    "\n",
    "for idx, r in sel.reset_index(drop=True).iterrows():\n",
    "    split_group = r['group']\n",
    "    part = r['partition']\n",
    "    spk = r['speaker_id']\n",
    "    systems = SYSTEMS_BY_PARTITION[part]\n",
    "    spoof_q = SPOOF_QUOTA_BY_PARTITION[part]\n",
    "\n",
    "    pool_spk = df[(df['partition'].eq(part)) & (df['speaker_id'].eq(spk))].copy()\n",
    "\n",
    "    bona_pool = pool_spk[pool_spk['label'].eq('bonafide')].copy()\n",
    "    bona_pick = bona_pool.sample(n=BONA_PER_SPK, random_state=SEED + 5000 + idx)\n",
    "    bona_pick = bona_pick.copy()\n",
    "    bona_pick['group'] = split_group\n",
    "    bona_pick['selected_reason'] = 'bonafide_quota'\n",
    "    selected_rows.append(bona_pick)\n",
    "    audit_rows.append({\n",
    "        'group': split_group,\n",
    "        'partition': part,\n",
    "        'speaker_id': spk,\n",
    "        'label': 'bonafide',\n",
    "        'system_id': 'bonafide',\n",
    "        'target_n': BONA_PER_SPK,\n",
    "        'selected_n': len(bona_pick),\n",
    "        'availability_n': len(bona_pool),\n",
    "    })\n",
    "\n",
    "    for j, sysid in enumerate(systems):\n",
    "        pool = pool_spk[(pool_spk['label'].eq('spoof')) & (pool_spk['system_id'].eq(sysid))].copy()\n",
    "        pick = pool.sample(n=spoof_q, random_state=SEED + 7000 + idx * 100 + j)\n",
    "        pick = pick.copy()\n",
    "        pick['group'] = split_group\n",
    "        pick['selected_reason'] = 'spoof_system_quota'\n",
    "        selected_rows.append(pick)\n",
    "        audit_rows.append({\n",
    "            'group': split_group,\n",
    "            'partition': part,\n",
    "            'speaker_id': spk,\n",
    "            'label': 'spoof',\n",
    "            'system_id': sysid,\n",
    "            'target_n': spoof_q,\n",
    "            'selected_n': len(pick),\n",
    "            'availability_n': len(pool),\n",
    "        })\n",
    "\n",
    "manifest = pd.concat(selected_rows, ignore_index=True)\n",
    "manifest = manifest[['group','partition','speaker_id','utt_id','gender','label','system_id','codec_id','codec_q','source_utt_id','attack_codec_id','selected_reason']]\n",
    "manifest = manifest.sort_values(['group','partition','speaker_id','label','system_id','utt_id']).reset_index(drop=True)\n",
    "\n",
    "audit_df = pd.DataFrame(audit_rows)\n",
    "\n",
    "# Sanity checks\n",
    "assert manifest['speaker_id'].nunique() == 100\n",
    "assert len(manifest) == 100 * 80\n",
    "assert manifest['utt_id'].nunique() == len(manifest)\n",
    "\n",
    "per_spk = manifest.groupby(['group','partition','speaker_id','label']).size().unstack(fill_value=0)\n",
    "assert (per_spk['bonafide'] == 32).all()\n",
    "assert (per_spk['spoof'] == 48).all()\n",
    "assert (audit_df['selected_n'] == audit_df['target_n']).all()\n",
    "\n",
    "manifest.to_csv(MANIFEST_OUT, index=False)\n",
    "sel.to_csv(SPEAKER_PLAN_OUT, index=False)\n",
    "audit_df.to_csv(AUDIT_OUT, index=False)\n",
    "\n",
    "summary = {\n",
    "    'seed': SEED,\n",
    "    'design': {\n",
    "        'n_speakers_total': 100,\n",
    "        'group_partition_speakers': GROUP_PARTITION_SPEAKERS,\n",
    "        'bonafide_per_speaker': BONA_PER_SPK,\n",
    "        'spoof_quota_by_partition': SPOOF_QUOTA_BY_PARTITION,\n",
    "        'systems_by_partition': SYSTEMS_BY_PARTITION,\n",
    "    },\n",
    "    'selected_speakers': int(sel['speaker_id'].nunique()),\n",
    "    'selected_utterances_total': int(len(manifest)),\n",
    "    'class_counts': {k:int(v) for k,v in manifest['label'].value_counts().sort_index().to_dict().items()},\n",
    "    'group_counts': {k:int(v) for k,v in manifest['group'].value_counts().sort_index().to_dict().items()},\n",
    "    'group_partition_counts': {\n",
    "        g: {p: int(v) for p, v in sel[sel['group'].eq(g)]['partition'].value_counts().sort_index().to_dict().items()}\n",
    "        for g in ['train','val','test']\n",
    "    },\n",
    "    'spoof_counts_by_system': {k:int(v) for k,v in manifest[manifest['label'].eq('spoof')]['system_id'].value_counts().sort_index().to_dict().items()},\n",
    "}\n",
    "SUMMARY_OUT.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "\n",
    "# Convenience copies in OUT_DIR\n",
    "manifest.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selected_utterances_plan.csv', index=False)\n",
    "sel.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selected_speakers_plan.csv', index=False)\n",
    "audit_df.to_csv(OUT_DIR / 'asvspoof5_100_speakers_selection_audit.csv', index=False)\n",
    "(OUT_DIR / 'asvspoof5_100_speakers_plan_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "\n",
    "print('Saved:', MANIFEST_OUT)\n",
    "print('Saved:', SPEAKER_PLAN_OUT)\n",
    "print('Saved:', AUDIT_OUT)\n",
    "print('Saved:', SUMMARY_OUT)\n",
    "print('Rows:', len(manifest), '| Speakers:', manifest['speaker_id'].nunique())\n",
    "print('By group:', manifest['group'].value_counts().sort_index().to_dict())\n",
    "print('By class:', manifest['label'].value_counts().sort_index().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inspect tar shards by partition\n",
    "\n",
    "partition_tars = {}\n",
    "for part, tar_dir in TAR_DIRS.items():\n",
    "    assert tar_dir.exists(), f'Missing TAR_DIR[{part}]: {tar_dir}'\n",
    "    tars = sorted(tar_dir.glob('flac_*.tar'))\n",
    "    partition_tars[part] = tars\n",
    "    print(f'[{part}] tar files:', len(tars))\n",
    "    for p in tars[:10]:\n",
    "        print(' -', p.name)\n",
    "    assert len(tars) > 0, f'No tar files found for partition {part} in {tar_dir}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build tar member index for selected utt_ids\n",
    "manifest = pd.read_csv(MANIFEST_OUT)\n",
    "needed_by_part = {\n",
    "    part: set(manifest[manifest['partition'].eq(part)]['utt_id'].astype(str).tolist())\n",
    "    for part in ['train','dev','eval']\n",
    "}\n",
    "print({k: len(v) for k, v in needed_by_part.items()})\n",
    "\n",
    "index_rows = []\n",
    "found_by_part = {k: set() for k in ['train','dev','eval']}\n",
    "\n",
    "for part in ['train','dev','eval']:\n",
    "    needed = needed_by_part[part]\n",
    "    for tar_path in partition_tars[part]:\n",
    "        print(f'Indexing [{part}]', tar_path.name)\n",
    "        with tarfile.open(tar_path, 'r') as tf:\n",
    "            for m in tf.getmembers():\n",
    "                if not m.isfile():\n",
    "                    continue\n",
    "                stem = Path(Path(m.name).name).stem\n",
    "                if stem in needed:\n",
    "                    index_rows.append({\n",
    "                        'partition': part,\n",
    "                        'utt_id': stem,\n",
    "                        'tar_file': str(tar_path),\n",
    "                        'member_name': m.name,\n",
    "                        'member_size': int(m.size),\n",
    "                    })\n",
    "                    found_by_part[part].add(stem)\n",
    "        print(f'  found so far [{part}]:', len(found_by_part[part]), '/', len(needed))\n",
    "\n",
    "index_df = pd.DataFrame(index_rows)\n",
    "idx_counts = index_df.groupby('utt_id').size().reset_index(name='n_matches') if not index_df.empty else pd.DataFrame(columns=['utt_id','n_matches'])\n",
    "ambiguous = idx_counts[idx_counts['n_matches'] > 1].copy()\n",
    "if not index_df.empty:\n",
    "    index_df = index_df.sort_values(['partition','utt_id','tar_file','member_name']).drop_duplicates('utt_id', keep='first').reset_index(drop=True)\n",
    "\n",
    "mapped = manifest.merge(index_df.drop(columns=['partition']), on='utt_id', how='left')\n",
    "unmatched = mapped[mapped['tar_file'].isna()].copy()\n",
    "\n",
    "index_csv = OUT_DIR / 'asvspoof5_100_speakers_tar_member_index.csv'\n",
    "mapped_csv = OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv'\n",
    "unmatched_csv = OUT_DIR / 'asvspoof5_100_speakers_unmatched_utts.csv'\n",
    "ambiguous_csv = OUT_DIR / 'asvspoof5_100_speakers_ambiguous_utts.csv'\n",
    "\n",
    "index_df.to_csv(index_csv, index=False)\n",
    "mapped.to_csv(mapped_csv, index=False)\n",
    "unmatched.to_csv(unmatched_csv, index=False)\n",
    "ambiguous.to_csv(ambiguous_csv, index=False)\n",
    "\n",
    "print('Mapped rows:', mapped['tar_file'].notna().sum(), '/', len(mapped))\n",
    "print('Unmatched rows:', len(unmatched))\n",
    "print('Ambiguous utt_ids:', len(ambiguous))\n",
    "assert len(unmatched) == 0, 'Some selected utt_ids were not found in the provided tar files.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selective extraction to EXTRACT_DIR/{group}/{label}/{utt_id}.flac\n",
    "mapped = pd.read_csv(OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv')\n",
    "assert mapped['tar_file'].notna().all(), 'Run tar indexing cell first and resolve unmatched rows.'\n",
    "\n",
    "for tar_file, g in mapped.groupby('tar_file'):\n",
    "    tar_path = Path(tar_file)\n",
    "    print('Extracting from', tar_path.name, '| rows =', len(g))\n",
    "    lookup = {row.member_name: row for row in g.itertuples(index=False)}\n",
    "    with tarfile.open(tar_path, 'r') as tf:\n",
    "        for m in tf.getmembers():\n",
    "            if m.name not in lookup:\n",
    "                continue\n",
    "            row = lookup[m.name]\n",
    "            ext = Path(m.name).suffix or '.flac'\n",
    "            out_dir = EXTRACT_DIR / row.group / row.label\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = out_dir / f'{row.utt_id}{ext}'\n",
    "            if out_path.exists():\n",
    "                continue\n",
    "            fobj = tf.extractfile(m)\n",
    "            assert fobj is not None\n",
    "            with open(out_path, 'wb') as w:\n",
    "                w.write(fobj.read())\n",
    "\n",
    "print('Extraction complete.')\n",
    "print('Extracted flac count:', len(list(EXTRACT_DIR.rglob('*.flac'))))\n",
    "print('Extracted wav count:', len(list(EXTRACT_DIR.rglob('*.wav'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: write manifest with resolved local paths\n",
    "manifest = pd.read_csv(OUT_DIR / 'asvspoof5_100_speakers_manifest_with_tar_paths.csv')\n",
    "ext_guess = manifest['member_name'].apply(lambda s: Path(str(s)).suffix if pd.notna(s) else '.flac')\n",
    "manifest['local_audio_path'] = [str(EXTRACT_DIR / g / lbl / f'{u}{e}') for g,lbl,u,e in zip(manifest['group'], manifest['label'], manifest['utt_id'], ext_guess)]\n",
    "manifest['label_binary_spoof_1'] = (manifest['label'] == 'spoof').astype(int)\n",
    "\n",
    "out_csv = OUT_DIR / 'asvspoof5_100_speakers_subset_manifest_with_local_paths.csv'\n",
    "manifest.to_csv(out_csv, index=False)\n",
    "print('Saved:', out_csv)\n",
    "display(manifest.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}