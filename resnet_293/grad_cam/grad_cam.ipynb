{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "Using device: cuda\n",
      "{'data_type': 'shard', 'dataloader_args': {'batch_size': 32, 'drop_last': True, 'num_workers': 16, 'pin_memory': False, 'prefetch_factor': 8}, 'dataset_args': {'aug_prob': 0.6, 'fbank_args': {'dither': 1.0, 'frame_length': 25, 'frame_shift': 10, 'num_mel_bins': 80}, 'num_frms': 200, 'shuffle': True, 'shuffle_args': {'shuffle_size': 2500}, 'spec_aug': False, 'spec_aug_args': {'max_f': 8, 'max_t': 10, 'num_f_mask': 1, 'num_t_mask': 1, 'prob': 0.6}, 'speed_perturb': True}, 'exp_dir': 'exp/ResNet293-TSTP-emb256-fbank80-num_frms200-aug0.6-spTrue-saFalse-ArcMargin-SGD-epoch150', 'gpus': [0, 1], 'log_batch_interval': 100, 'loss': 'CrossEntropyLoss', 'loss_args': {}, 'margin_scheduler': 'MarginScheduler', 'margin_update': {'epoch_iter': 17062, 'final_margin': 0.2, 'fix_start_epoch': 40, 'increase_start_epoch': 20, 'increase_type': 'exp', 'initial_margin': 0.0, 'update_margin': True}, 'model': 'ResNet293', 'model_args': {'embed_dim': 256, 'feat_dim': 80, 'pooling_func': 'TSTP', 'two_emb_layer': False}, 'model_init': None, 'noise_data': 'data/musan/lmdb', 'num_avg': 2, 'num_epochs': 150, 'optimizer': 'SGD', 'optimizer_args': {'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001}, 'projection_args': {'easy_margin': False, 'embed_dim': 256, 'num_class': 17982, 'project_type': 'arc_margin', 'scale': 32.0}, 'reverb_data': 'data/rirs/lmdb', 'save_epoch_interval': 5, 'scheduler': 'ExponentialDecrease', 'scheduler_args': {'epoch_iter': 17062, 'final_lr': 5e-05, 'initial_lr': 0.1, 'num_epochs': 150, 'scale_ratio': 1.0, 'warm_from_zero': True, 'warm_up_epoch': 6}, 'seed': 42, 'train_data': 'data/vox2_dev/shard.list', 'train_label': 'data/vox2_dev/utt2spk'}\n",
      "ResNet-293 loaded\n",
      "Mel params: n_fft=400, hop_length=160, n_mels=80\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from wespeaker.cli.speaker import load_model\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from utils.spectogram_player_html import save_spectrogram_player_html\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Load WeSpeaker bundle + move backbone to GPU\n",
    "speaker = load_model(PROJECT_ROOT / \"wespeaker-voxceleb-resnet293-LM\")\n",
    "speaker.model = speaker.model.to(DEVICE).eval()\n",
    "\n",
    "print(\"ResNet-293 loaded\")\n",
    "\n",
    "# WeSpeaker mel params\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 80\n",
    "FRAME_LENGTH_MS = 25\n",
    "FRAME_SHIFT_MS = 10\n",
    "N_FFT = int(SAMPLE_RATE * FRAME_LENGTH_MS / 1000)\n",
    "HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT_MS / 1000)\n",
    "\n",
    "print(f\"Mel params: n_fft={N_FFT}, hop_length={HOP_LENGTH}, n_mels={N_MELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded head from: /home/SpeakerRec/BioVoice/data/heads/resnet_293_speaker_head.pt\n",
      "Speakers: ['eden', 'idan', 'yoav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655308/2848748702.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(head_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ========== LOAD YOUR TRAINED FC HEAD ==========\n",
    "SPEAKERS = [\"eden\", \"idan\", \"yoav\"]\n",
    "speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "head = SpeakerHead(in_dim=256, num_classes=len(SPEAKERS)).to(DEVICE)\n",
    "\n",
    "head_path = PROJECT_ROOT / \"data\" / \"heads\" / \"resnet_293_speaker_head.pt\"\n",
    "assert head_path.exists(), f\"Missing head checkpoint: {head_path}\"\n",
    "\n",
    "ckpt = torch.load(head_path, map_location=\"cpu\")\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head = head.to(DEVICE).eval()\n",
    "\n",
    "if \"speakers\" in ckpt:\n",
    "    SPEAKERS = ckpt[\"speakers\"]\n",
    "    speaker_to_id = {s: i for i, s in enumerate(SPEAKERS)}\n",
    "    id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "print(\"Loaded head from:\", head_path)\n",
    "print(\"Speakers:\", SPEAKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrapped_model ready\n"
     ]
    }
   ],
   "source": [
    "class WeSpeakerWithHeadForGradCAM(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone: nn.Module, head: nn.Module, try_transpose: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self.try_transpose = try_transpose\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Accept NCHW from Grad-CAM and squeeze channel dim\n",
    "        if feats.ndim == 4 and feats.size(1) == 1:\n",
    "            feats = feats.squeeze(1)\n",
    "\n",
    "        # Run backbone\n",
    "        try:\n",
    "            out = self.backbone(feats)\n",
    "        except Exception:\n",
    "            if not self.try_transpose:\n",
    "                raise\n",
    "            out = self.backbone(feats.transpose(1, 2))\n",
    "\n",
    "        # Unwrap tuple/list (WeSpeaker behavior)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            emb = out[-1]\n",
    "        else:\n",
    "            emb = out\n",
    "\n",
    "        # ---- HARD SHAPE GUARD ----\n",
    "        if emb.ndim == 0:\n",
    "            emb = emb.unsqueeze(0).unsqueeze(0)\n",
    "        elif emb.ndim == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "        elif emb.ndim > 2:\n",
    "            emb = emb.view(emb.size(0), -1)\n",
    "\n",
    "        # ---- HARD DEVICE GUARD (THIS FIXES YOUR ERROR) ----\n",
    "        emb = emb.to(next(self.head.parameters()).device)\n",
    "\n",
    "        logits = self.head(emb)\n",
    "\n",
    "        if logits.ndim == 1:\n",
    "            logits = logits.unsqueeze(0)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "wrapped_model = WeSpeakerWithHeadForGradCAM(speaker.model, head).to(DEVICE).eval()\n",
    "\n",
    "# Enable gradients for backbone (needed for Grad-CAM)\n",
    "for p in wrapped_model.backbone.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# Head grads not required (optional)\n",
    "for p in wrapped_model.head.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"wrapped_model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== HELPERS (your existing ones, unchanged) ==========\n",
    "def magma_rgb(img2d: np.ndarray) -> np.ndarray:\n",
    "    img2d = img2d.astype(np.float32)\n",
    "    img2d = (img2d - img2d.min()) / (img2d.max() - img2d.min() + 1e-8)\n",
    "    return cm.get_cmap(\"magma\")(img2d)[..., :3].astype(np.float32)\n",
    "\n",
    "def normalize_01(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    vmin = float(arr.min())\n",
    "    vmax = float(arr.max())\n",
    "    if vmax - vmin < 1e-8:\n",
    "        return np.zeros_like(arr, dtype=np.float32)\n",
    "    return ((arr - vmin) / (vmax - vmin)).astype(np.float32)\n",
    "\n",
    "def upsample_hw(arr: np.ndarray, size_hw: tuple[int, int], mode: str = \"bilinear\") -> np.ndarray:\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        t = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()  # [1,1,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0, 0].cpu().numpy()\n",
    "\n",
    "    if arr.ndim == 3 and arr.shape[2] == 3:\n",
    "        t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0).float()  # [1,3,H,W]\n",
    "        t = F.interpolate(t, size=size_hw, mode=mode, align_corners=False)\n",
    "        return t[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    raise ValueError(f\"Unsupported shape: {arr.shape}\")\n",
    "\n",
    "def to_float01(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def plot_mel(mel, save_path=None, title=None):\n",
    "    if isinstance(mel, torch.Tensor):\n",
    "        mel = mel.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    mel_db = 10.0 * np.log10(np.maximum(mel, 1e-10))\n",
    "    mel_db = np.clip(mel_db, mel_db.max() - 80.0, mel_db.max())\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-8)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_norm, aspect=\"auto\", cmap=\"magma\", interpolation=\"bilinear\", origin=\"lower\")\n",
    "    plt.colorbar(label=\"Normalized Energy\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Frequency Bins\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_wav_mono_16k(wav_path: str) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav[:1, :]\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "\n",
    "    return wav  # keep on CPU for mel transform stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ========== FEATURE EXTRACTOR (mel) ==========\n",
    "# Important: mel_transform must be on DEVICE if you want mel tensor on GPU.\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layers:\n",
      "layer4.2.conv3 -> Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ========== TARGET LAYERS (same idea, but via wrapped_model.backbone) ==========\n",
    "target_layers = [\n",
    "    # wrapped_model.backbone.layer1[9].conv3,\n",
    "    # wrapped_model.backbone.layer2[19].conv3,\n",
    "    # wrapped_model.backbone.layer3[63].conv3,\n",
    "    wrapped_model.backbone.layer4[2].conv3,\n",
    "]\n",
    "\n",
    "layer_names = [\n",
    "    # \"layer1.9.conv3\",\n",
    "    # \"layer2.19.conv3\",\n",
    "    # \"layer3.63.conv3\",\n",
    "    \"layer4.2.conv3\",\n",
    "]\n",
    "\n",
    "print(\"Target layers:\")\n",
    "for n, l in zip(layer_names, target_layers):\n",
    "    print(n, \"->\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_gradcam_speaker(\n",
    "    wav_path: str,\n",
    "    target_speaker: str,\n",
    "    save_dir: str = \"gradcam_results_cls\",\n",
    "    upscale: int = 10,\n",
    "    cam_quantile: float = 0.85,\n",
    "):\n",
    "    assert target_speaker in speaker_to_id, f\"Unknown speaker: {target_speaker}\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fname = Path(wav_path).stem\n",
    "\n",
    "    # 1) Load wav (CPU), compute mel ON GPU with grad\n",
    "    wav_cpu = load_wav_mono_16k(wav_path)              # [1, T] CPU\n",
    "    wav_gpu = wav_cpu.to(DEVICE)                       # move to GPU for mel_transform\n",
    "    feats = mel_transform(wav_gpu)                     # [1, 80, Tfeat] on GPU\n",
    "    feats = feats.contiguous().float()\n",
    "    feats_cam = feats.unsqueeze(1)                  # [1, 1, 80, Tfeat] for Grad-CAM\n",
    "\n",
    "    # 1b) Detached mel for visualization (numpy)\n",
    "    with torch.no_grad():\n",
    "        mel2d = feats.detach()[0].cpu().numpy()        # [80, Tfeat]\n",
    "    mel_db = 10.0 * np.log10(np.maximum(mel2d, 1e-10))\n",
    "    mel_db = np.clip(mel_db, mel_db.max() - 80.0, mel_db.max())\n",
    "    mel_norm = normalize_01(mel_db)\n",
    "\n",
    "    originals_dir = save_dir / \"original\"\n",
    "    originals_dir.mkdir(exist_ok=True)\n",
    "    plot_mel(\n",
    "        mel2d,\n",
    "        save_path=originals_dir / f\"{fname}_mel.png\",\n",
    "        title=f\"{fname} (target={target_speaker})\",\n",
    "    )\n",
    "\n",
    "    # 2) target class\n",
    "    target = ClassifierOutputTarget(speaker_to_id[target_speaker])\n",
    "\n",
    "    layer_stats = {}\n",
    "\n",
    "    for layer, lname in zip(target_layers, layer_names):\n",
    "        print(f\"[Grad-CAM] Layer: {lname}\")\n",
    "\n",
    "        layer_dir = save_dir / lname\n",
    "        layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        wrapped_model.zero_grad(set_to_none=True)\n",
    "        if feats.grad is not None:\n",
    "            feats.grad.zero_()\n",
    "\n",
    "        # Optional sanity check\n",
    "        with torch.no_grad():\n",
    "            tmp = wrapped_model(feats)\n",
    "        print(\"wrapped_model(feats) shape:\", tmp.shape)\n",
    "\n",
    "        try:\n",
    "            cam = GradCAM(model=wrapped_model, target_layers=[layer], use_cuda=(DEVICE == \"cuda\"))\n",
    "        except TypeError:\n",
    "            cam = GradCAM(model=wrapped_model, target_layers=[layer])\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            grayscale_cam = cam(\n",
    "                input_tensor=feats_cam,      # [1,1,80,T] for Grad-CAM\n",
    "                targets=[target],\n",
    "                aug_smooth=False,\n",
    "                eigen_smooth=True,\n",
    "            )[0]  # [H', W'] (feature map space)\n",
    "\n",
    "        # 4) align CAM to mel for plotting\n",
    "        cam_on_mel = upsample_hw(grayscale_cam, mel2d.shape, mode=\"bicubic\")\n",
    "        cam_norm = normalize_01(cam_on_mel)\n",
    "\n",
    "        # 5) weighted mel\n",
    "        mel_weighted = mel2d * cam_norm\n",
    "        plot_mel(\n",
    "            mel_weighted,\n",
    "            save_path=layer_dir / f\"{fname}_mel_weighted.png\",\n",
    "            title=f\"{fname} | {lname} | weighted\",\n",
    "        )\n",
    "\n",
    "        # 6) focus mask\n",
    "        thr = np.quantile(cam_norm, cam_quantile)\n",
    "        mask = cam_norm >= thr\n",
    "        mel_focus = np.where(mask, mel2d, mel2d.min())\n",
    "        plot_mel(\n",
    "            mel_focus,\n",
    "            save_path=layer_dir / f\"{fname}_mel_focus_q{cam_quantile}.png\",\n",
    "            title=f\"{fname} | {lname} | focus\",\n",
    "        )\n",
    "\n",
    "        # 7) overlay\n",
    "        H, W = mel_norm.shape\n",
    "        H2, W2 = H * upscale, W * upscale\n",
    "\n",
    "        rgb_big = upsample_hw(np.stack([mel_norm] * 3, axis=-1), (H2, W2), mode=\"bicubic\")\n",
    "        cam_big = upsample_hw(cam_norm, (H2, W2), mode=\"bicubic\")\n",
    "\n",
    "        rgb_big = to_float01(rgb_big)\n",
    "        cam_big = normalize_01(cam_big)\n",
    "        overlay = show_cam_on_image(rgb_big, cam_big, use_rgb=True)\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay.png\", overlay)\n",
    "\n",
    "        # 7b) masked mel (no axes) + HTML player\n",
    "        thr_big = np.quantile(cam_big, cam_quantile)\n",
    "        mask_big = (cam_big >= thr_big).astype(np.float32)\n",
    "        mel_only = magma_rgb(rgb_big[..., 0]) * mask_big[..., None]\n",
    "        mel_only = np.clip(mel_only, 0.0, 1.0)\n",
    "        mel_masked_png = layer_dir / f\"{fname}_mel_masked_q{cam_quantile:.2f}.png\"\n",
    "        plt.imsave(mel_masked_png, mel_only)\n",
    "        save_spectrogram_player_html(\n",
    "            audio_path=wav_path,\n",
    "            spectrogram_png_path=mel_masked_png,\n",
    "            out_html_path=layer_dir / f\"{fname}_mel_masked_q{cam_quantile:.2f}.html\",\n",
    "            total_time_sec=None,\n",
    "            copy_audio=True,\n",
    "            embed_image=True,\n",
    "        )\n",
    "\n",
    "        # 8) stats\n",
    "        entropy = -np.sum(cam_norm * np.log(cam_norm + 1e-8)) / cam_norm.size\n",
    "        support = (cam_norm > 0.5).mean()\n",
    "        layer_stats[lname] = {\"entropy\": float(entropy), \"support@0.5\": float(support)}\n",
    "\n",
    "    stats_df = pd.DataFrame(layer_stats).T\n",
    "    stats_df.to_csv(save_dir / \"layer_cam_stats.csv\")\n",
    "\n",
    "    print(\"Saved Grad-CAM results to:\", save_dir)\n",
    "    print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: /home/SpeakerRec/BioVoice/data/wavs/idan_012.wav\n",
      "[Grad-CAM] Layer: layer4.2.conv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7f7ea54ec040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/SpeakerRec/BioVoice/.venvResnet/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrapped_model(feats) shape: torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655308/2664893053.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  return cm.get_cmap(\"magma\")(img2d)[..., :3].astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Grad-CAM results to: gradcam_results/idan/idan_012\n",
      "                 entropy  support@0.5\n",
      "layer4.2.conv3  0.068207     0.030352\n"
     ]
    }
   ],
   "source": [
    "wav_dir = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "\n",
    "# Use an existing audio file\n",
    "test_wav = wav_dir / \"idan_012.wav\"\n",
    "if not test_wav.exists():\n",
    "    # Fallback to first available idan file\n",
    "    idan_files = list(wav_dir.glob(\"idan_*.wav\"))\n",
    "    if idan_files:\n",
    "        test_wav = sorted(idan_files)[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No audio files found in {wav_dir}\")\n",
    "\n",
    "print(f\"Testing with: {test_wav}\")\n",
    "\n",
    "run_out_dir = Path(f\"gradcam_results/idan/{test_wav.stem}\")\n",
    "originals_dir = run_out_dir / \"original\"\n",
    "\n",
    "run_gradcam_speaker(\n",
    "    wav_path=str(test_wav),\n",
    "    target_speaker=\"idan\",\n",
    "    save_dir=str(run_out_dir),\n",
    "    upscale=12,\n",
    ")\n",
    "\n",
    "# Uncomment to run on multiple speakers\n",
    "# top3_by_speaker = {\n",
    "#     \"eden\": [\"eden_007.wav\", \"eden_010.wav\", \"eden_012.wav\"],\n",
    "#     \"idan\": [\"idan_001.wav\", \"idan_002.wav\", \"idan_005.wav\"],\n",
    "#     \"yoav\": [\"yoav_004.wav\", \"yoav_014.wav\", \"yoav_015.wav\"],\n",
    "# }\n",
    "\n",
    "# for spk, fnames in top3_by_speaker.items():\n",
    "#     for fname in fnames:\n",
    "#         wav_path = wav_dir / fname\n",
    "#         if not wav_path.exists():\n",
    "#             print(f\"[WARN] missing file: {wav_path}\")\n",
    "#             continue\n",
    "\n",
    "#         run_gradcam_speaker(\n",
    "#             wav_path=str(wav_path),\n",
    "#             target_speaker=spk,\n",
    "#             save_dir=f\"gradcam_results/{spk}/{fname.replace('.wav', '')}\",\n",
    "#             upscale=12,\n",
    "#         )\n",
    "# Run all wavs per speaker (set RUN_ALL = True to enable)\n",
    "RUN_ALL = False\n",
    "if RUN_ALL:\n",
    "    wav_by_user = {spk: list(wav_dir.glob(f\"{spk}_*.wav\")) for spk in SPEAKERS}\n",
    "    for spk, files in wav_by_user.items():\n",
    "        for wav_path in files:\n",
    "            if not wav_path.exists():\n",
    "                print(f\"[WARN] missing file: {wav_path}\")\n",
    "                continue\n",
    "            run_gradcam_speaker(\n",
    "                wav_path=str(wav_path),\n",
    "                target_speaker=spk,\n",
    "                save_dir=f\"gradcam_results/{spk}/{wav_path.stem}\",\n",
    "                upscale=12,\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvResnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
