{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from captum.concept import TCAV, Concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Project paths / device --------\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "ATTR_CSV_PATH = Path(\n",
    "    PROJECT_ROOT\n",
    "    / \"redimnet\"\n",
    "    / \"grad_cam\"\n",
    "    / \"2.0\"\n",
    "    / \"output\"\n",
    "    / \"speaker_similarity_ranking.csv\"\n",
    ")\n",
    "\n",
    "WAV_FOLDER = Path(PROJECT_ROOT / \"data\" / \"wavs\")\n",
    "CONCEPT_ROOT  = Path(PROJECT_ROOT / \"concept\" / \"temp_concepts\")\n",
    "\n",
    "# Pick one layer key you want TCAV on:\n",
    "LAYER_KEY = \"stage5\"\n",
    "\n",
    "CONCEPT_SAMPLES = 100\n",
    "RANDOM_SAMPLES  = 100\n",
    "BATCH_SIZE_CONCEPT = 1  # keep 1 (safe if variable T)\n",
    "FORCE_TRAIN_CAVS = True  # set True if you want to retrain CAVs\n",
    "\n",
    "OUT_CSV = Path(f\"stage5_temp_concepts_{LAYER_KEY}.csv\")\n",
    "\n",
    "assert ATTR_CSV_PATH.exists(), f\"Missing {ATTR_CSV_PATH}\"\n",
    "assert CONCEPT_ROOT.exists(), f\"Missing {CONCEPT_ROOT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim_model = (\n",
    "    torch.hub.load(\n",
    "        \"IDRnD/ReDimNet\",\n",
    "        \"ReDimNet\",\n",
    "        model_name=\"b5\",\n",
    "        train_type=\"ptn\",\n",
    "        dataset=\"vox2\",\n",
    "    )\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "print(\"Loaded ReDimNet successfully.\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_wav = torch.zeros(1, 16000, device=DEVICE)\n",
    "    dummy_mel = redim_model.spec(dummy_wav)  # (1, N_MELS, T)\n",
    "N_MELS = int(dummy_mel.shape[1])\n",
    "print(\"ReDimNet spec N_MELS =\", N_MELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Load your speaker head ckpt --------\n",
    "HEAD_PATH = Path.cwd() / \"output\" / \"redim_speaker_head_linear.pt\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "l2_norm_emb = bool(ckpt.get(\"l2_norm_emb\", True))\n",
    "\n",
    "# infer in_dim from checkpoint\n",
    "fc_w = ckpt[\"state_dict\"][\"fc.weight\"]\n",
    "in_dim = int(fc_w.shape[1])\n",
    "num_classes = int(fc_w.shape[0])\n",
    "\n",
    "print(\"Loaded head:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"Head in_dim:\", in_dim, \"num_classes:\", num_classes, \"l2_norm_emb:\", l2_norm_emb)\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "head = SpeakerHead(in_dim=in_dim, num_classes=num_classes).to(DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Wrap ReDimNet -> logits --------\n",
    "class ReDimNetMelLogitsWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  mel4d [B, 1, N_MELS, T]\n",
    "    Output: logits [B, num_speakers]\n",
    "    \"\"\"\n",
    "    def __init__(self, redim_model, head: nn.Module, l2_norm_emb: bool):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone\n",
    "        self.pool = redim_model.pool\n",
    "        self.bn = redim_model.bn\n",
    "        self.linear = redim_model.linear\n",
    "        self.head = head\n",
    "        self.l2_norm_emb = l2_norm_emb\n",
    "\n",
    "    def forward(self, mel4d: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(mel4d)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn(x)\n",
    "        emb = self.linear(x)\n",
    "        if self.l2_norm_emb:\n",
    "            emb = emb / (emb.norm(p=2, dim=1, keepdim=True) + 1e-12)\n",
    "        return self.head(emb)\n",
    "\n",
    "wrapped_model = ReDimNetMelLogitsWrapper(redim_model, head, l2_norm_emb=l2_norm_emb).to(DEVICE).eval()\n",
    "print(\"wrapped_model ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAYERS = {\n",
    "    \"stem\":   wrapped_model.backbone.stem[0],\n",
    "    \"stage0\": wrapped_model.backbone.stage0[2],\n",
    "    \"stage1\": wrapped_model.backbone.stage1[2],\n",
    "    \"stage2\": wrapped_model.backbone.stage2[2],\n",
    "    \"stage3\": wrapped_model.backbone.stage3[2],\n",
    "    \"stage4\": wrapped_model.backbone.stage4[2],\n",
    "    \"stage5\": wrapped_model.backbone.stage5[2],\n",
    "}\n",
    "assert LAYER_KEY in TARGET_LAYERS, f\"{LAYER_KEY=} not in TARGET_LAYERS: {list(TARGET_LAYERS.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Resolve layer name string for Captum --------\n",
    "def module_name_in_model(model: torch.nn.Module, target_module: torch.nn.Module) -> str:\n",
    "    for name, mod in model.named_modules():\n",
    "        if mod is target_module:\n",
    "            return name\n",
    "    raise RuntimeError(\"Could not find the selected layer module in wrapped_model.named_modules()\")\n",
    "\n",
    "\n",
    "    \n",
    "layer_module = TARGET_LAYERS[LAYER_KEY]\n",
    "LAYER_NAME = module_name_in_model(wrapped_model, layer_module)\n",
    "print(\"Using layer:\", LAYER_KEY, \"->\", LAYER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCAV_DEVICE = torch.device(\"cpu\")\n",
    "print(\"TCAV_DEVICE =\", TCAV_DEVICE)\n",
    "\n",
    "redim_model = redim_model.to(TCAV_DEVICE).eval()\n",
    "wrapped_model = wrapped_model.to(TCAV_DEVICE).eval()\n",
    "\n",
    "DEVICE = TCAV_DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptNPYDataset(Dataset):\n",
    "    def __init__(self, concept_dir: Path, limit: int | None = None):\n",
    "        self.files = sorted(concept_dir.glob(\"*.npy\"))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No .npy found in {concept_dir}\")\n",
    "        if limit is not None:\n",
    "            self.files = self.files[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(self.files[idx]).astype(np.float32)  # (N_MELS, T)\n",
    "        if mel.shape[0] != N_MELS:\n",
    "            raise RuntimeError(f\"{self.files[idx].name}: expected {N_MELS} bins, got {mel.shape}\")\n",
    "        x = torch.from_numpy(mel).unsqueeze(0)  # (1, N_MELS, T) on CPU\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def infer_frames_for_random(concept_dirs: list[Path]) -> int:\n",
    "    for d in concept_dirs:\n",
    "        f = next(d.glob(\"*.npy\"), None)\n",
    "        if f is not None:\n",
    "            mel = np.load(f)\n",
    "            return int(mel.shape[1])\n",
    "    raise RuntimeError(\"Could not infer frames from concept dirs\")\n",
    "\n",
    "class RandomMelDataset(Dataset):\n",
    "    def __init__(self, n_samples: int, frames: int):\n",
    "        self.n_samples = n_samples\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = torch.randn(N_MELS, self.frames, dtype=torch.float32)  \n",
    "        return mel.unsqueeze(0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "if not concept_dirs:\n",
    "    raise RuntimeError(f\"No concept folders in {CONCEPT_ROOT}\")\n",
    "\n",
    "concept_names = [d.name for d in concept_dirs]\n",
    "print(\"Concepts:\", concept_names)\n",
    "\n",
    "TARGET_FRAMES = infer_frames_for_random(concept_dirs)\n",
    "print(\"Using fixed frames for TCAV (from concepts):\", TARGET_FRAMES)\n",
    "tcav = TCAV(wrapped_model, [LAYER_NAME], test_split_ratio=0.33)\n",
    "\n",
    "positive_concepts = []\n",
    "for idx, cdir in enumerate(concept_dirs):\n",
    "    ds = ConceptNPYDataset(cdir, limit=CONCEPT_SAMPLES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "    positive_concepts.append(Concept(id=idx, name=cdir.name, data_iter=dl))\n",
    "\n",
    "rand_ds = RandomMelDataset(n_samples=RANDOM_SAMPLES, frames=TARGET_FRAMES)\n",
    "rand_dl = DataLoader(rand_ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "random_concept = Concept(id=len(positive_concepts), name=\"random\", data_iter=rand_dl)\n",
    "\n",
    "experimental_sets = [[c, random_concept] for c in positive_concepts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cav_acc_df(tcav: TCAV, positive_concepts: list[Concept], random_concept: Concept) -> pd.DataFrame:\n",
    "    cavs_dict = tcav.compute_cavs([[c, random_concept] for c in positive_concepts], force_train=FORCE_TRAIN_CAVS)\n",
    "\n",
    "    rows = []\n",
    "    for concepts_key, layer_map in cavs_dict.items():\n",
    "        try:\n",
    "            pos_id = int(str(concepts_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_id < len(positive_concepts)):\n",
    "            continue\n",
    "        concept_name = positive_concepts[pos_id].name\n",
    "\n",
    "        for layer_name, cav_obj in layer_map.items():\n",
    "            if cav_obj is None or cav_obj.stats is None:\n",
    "                continue\n",
    "            acc = cav_obj.stats.get(\"accs\", None)\n",
    "            if acc is None:\n",
    "                acc = cav_obj.stats.get(\"acc\", None)\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.detach().cpu().item()\n",
    "            rows.append({\n",
    "                \"concept name\": concept_name,\n",
    "                \"layer name\": layer_name,\n",
    "                \"cav acc\": float(acc) if acc is not None else np.nan,\n",
    "            })\n",
    "    return pd.DataFrame(rows, columns=[\"concept name\", \"layer name\", \"cav acc\"])\n",
    "\n",
    "acc_df = compute_cav_acc_df(tcav, positive_concepts, random_concept)\n",
    "print(acc_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mel_frames(mel_3d: torch.Tensor, target_frames: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel_3d: (1, N_MELS, T)\n",
    "    returns: (1, N_MELS, target_frames)\n",
    "    \"\"\"\n",
    "    T = int(mel_3d.shape[-1])\n",
    "    if T == target_frames:\n",
    "        return mel_3d\n",
    "    if T > target_frames:\n",
    "        start = (T - target_frames) // 2\n",
    "        return mel_3d[..., start:start + target_frames]\n",
    "    pad = target_frames - T\n",
    "    return F.pad(mel_3d, (0, pad), mode=\"constant\", value=0.0)\n",
    "\n",
    "def wav_path_to_mel4d(path: Path) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)          # (1, N_MELS, T)\n",
    "    mel = fix_mel_frames(mel, TARGET_FRAMES) # (1, N_MELS, TARGET_FRAMES)\n",
    "    return mel.unsqueeze(0)                  # (1, 1, N_MELS, TARGET_FRAMES)\n",
    "\n",
    "def predict_speaker(path: Path) -> tuple[str, float]:\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    with torch.no_grad():\n",
    "        logits = wrapped_model(x)            # (1, num_speakers)\n",
    "        probs = F.softmax(logits, dim=1)[0]\n",
    "        pred_id = int(torch.argmax(probs).item())\n",
    "        pred_name = id_to_speaker[pred_id]\n",
    "        pred_prob = float(probs[pred_id].item())\n",
    "    return pred_name, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr = pd.read_csv(ATTR_CSV_PATH)\n",
    "\n",
    "\n",
    "if \"path\" not in df_attr.columns or \"speaker\" not in df_attr.columns:\n",
    "    raise RuntimeError(f\"CSV must contain columns ['path','speaker']. Got: {list(df_attr.columns)}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, r in df_attr.iterrows():\n",
    "    path = Path(r[\"path\"])\n",
    "    true_label = str(r[\"speaker\"])\n",
    "\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    if true_label not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    pred_label, pred_prob = predict_speaker(path)\n",
    "\n",
    "    x = wav_path_to_mel4d(path)\n",
    "    target_idx = speaker_to_id[true_label]\n",
    "\n",
    "    score_for_label = tcav.interpret(\n",
    "        inputs=x,\n",
    "        experimental_sets=experimental_sets,\n",
    "        target=target_idx,\n",
    "    )\n",
    "\n",
    "    for exp_key, layer_dict in score_for_label.items():\n",
    "        try:\n",
    "            pos_idx = int(str(exp_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_idx < len(positive_concepts)):\n",
    "            continue\n",
    "\n",
    "        concept_name = positive_concepts[pos_idx].name\n",
    "\n",
    "        for layer_name, metrics in layer_dict.items():\n",
    "            sc = metrics.get(\"sign_count\")\n",
    "            mg = metrics.get(\"magnitude\")\n",
    "            if sc is None or mg is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(sc, torch.Tensor):\n",
    "                sc = sc.detach().cpu().tolist()\n",
    "            if isinstance(mg, torch.Tensor):\n",
    "                mg = mg.detach().cpu().tolist()\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(path),\n",
    "                \"concept name\": concept_name,\n",
    "                \"layer name\": layer_name,\n",
    "                \"positive percentage\": float(sc[0]),\n",
    "                \"magnitude\": float(mg[0]),\n",
    "                \"true label\": true_label,\n",
    "                \"predicted label\": pred_label,\n",
    "                \"predicted probability\": float(pred_prob),\n",
    "            })\n",
    "\n",
    "df_tcav = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"path\", \"concept name\", \"layer name\", \"positive percentage\", \"magnitude\",\n",
    "        \"true label\", \"predicted label\", \"predicted probability\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_tcav = df_tcav.merge(acc_df, on=[\"concept name\", \"layer name\"], how=\"left\")\n",
    "\n",
    "df_tcav.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved â†’\", OUT_CSV)\n",
    "df_tcav.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
