{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "353736f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from typing import Optional, Tuple, List\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from captum.concept import TCAV, Concept\n",
    "from wespeaker.cli.speaker import load_model\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9535a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "TCAV_DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Project paths / device --------\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "TCAV_DEVICE = torch.device(\"cpu\")\n",
    "print(\"TCAV_DEVICE =\", TCAV_DEVICE)\n",
    "\n",
    "ATTR_CSV_PATH = Path(\n",
    "    PROJECT_ROOT / \"resnet_293\" / \"speaker_similarity_ranking_team.csv\"\n",
    ")\n",
    "WAV_FOLDER = Path(PROJECT_ROOT / \"data\" / \"wavs\")\n",
    "CONCEPT_ROOT = Path(PROJECT_ROOT / \"concept\" / \"concepts_dataset_resnet_293\")\n",
    "\n",
    "HEAD_PATH = Path(PROJECT_ROOT / \"data\" / \"heads\" / \"resnet_293_speaker_head.pt\")\n",
    "\n",
    "assert ATTR_CSV_PATH.exists(), f\"Missing {ATTR_CSV_PATH}\"\n",
    "assert WAV_FOLDER.exists(), f\"Missing {WAV_FOLDER}\"\n",
    "assert CONCEPT_ROOT.exists(), f\"Missing {CONCEPT_ROOT}\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "# ====== choose TCAV layer ======\n",
    "# LAYER_KEY = \"layer3.63.conv3\"  \n",
    "# OUT_CSV = Path(f\"concepts_dataset_resnet_293_{LAYER_KEY}.csv\")\n",
    "\n",
    "CONCEPT_SAMPLES = 100\n",
    "RANDOM_SAMPLES = 100\n",
    "BATCH_SIZE_CONCEPT = 1\n",
    "FORCE_TRAIN_CAVS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71a62042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_type': 'shard', 'dataloader_args': {'batch_size': 32, 'drop_last': True, 'num_workers': 16, 'pin_memory': False, 'prefetch_factor': 8}, 'dataset_args': {'aug_prob': 0.6, 'fbank_args': {'dither': 1.0, 'frame_length': 25, 'frame_shift': 10, 'num_mel_bins': 80}, 'num_frms': 200, 'shuffle': True, 'shuffle_args': {'shuffle_size': 2500}, 'spec_aug': False, 'spec_aug_args': {'max_f': 8, 'max_t': 10, 'num_f_mask': 1, 'num_t_mask': 1, 'prob': 0.6}, 'speed_perturb': True}, 'exp_dir': 'exp/ResNet293-TSTP-emb256-fbank80-num_frms200-aug0.6-spTrue-saFalse-ArcMargin-SGD-epoch150', 'gpus': [0, 1], 'log_batch_interval': 100, 'loss': 'CrossEntropyLoss', 'loss_args': {}, 'margin_scheduler': 'MarginScheduler', 'margin_update': {'epoch_iter': 17062, 'final_margin': 0.2, 'fix_start_epoch': 40, 'increase_start_epoch': 20, 'increase_type': 'exp', 'initial_margin': 0.0, 'update_margin': True}, 'model': 'ResNet293', 'model_args': {'embed_dim': 256, 'feat_dim': 80, 'pooling_func': 'TSTP', 'two_emb_layer': False}, 'model_init': None, 'noise_data': 'data/musan/lmdb', 'num_avg': 2, 'num_epochs': 150, 'optimizer': 'SGD', 'optimizer_args': {'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001}, 'projection_args': {'easy_margin': False, 'embed_dim': 256, 'num_class': 17982, 'project_type': 'arc_margin', 'scale': 32.0}, 'reverb_data': 'data/rirs/lmdb', 'save_epoch_interval': 5, 'scheduler': 'ExponentialDecrease', 'scheduler_args': {'epoch_iter': 17062, 'final_lr': 5e-05, 'initial_lr': 0.1, 'num_epochs': 150, 'scale_ratio': 1.0, 'warm_from_zero': True, 'warm_up_epoch': 6}, 'seed': 42, 'train_data': 'data/vox2_dev/shard.list', 'train_label': 'data/vox2_dev/utt2spk'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SpeakerRec/BioVoice/.venvResnet/lib/python3.10/site-packages/wespeaker/utils/checkpoint.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-293 backbone loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Load WeSpeaker ResNet-293 backbone --------\n",
    "speaker = load_model(PROJECT_ROOT / \"wespeaker-voxceleb-resnet293-LM\")\n",
    "backbone = speaker.model\n",
    "\n",
    "backbone = backbone.to(TCAV_DEVICE).eval()\n",
    "\n",
    "print(\"ResNet-293 backbone loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4409f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded head: /home/SpeakerRec/BioVoice/data/heads/resnet_293_speaker_head.pt\n",
      "Speakers: ['eden', 'idan', 'yoav']\n",
      "Head in_dim: 256 num_classes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_482030/1418539356.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(HEAD_PATH, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpeakerHead(\n",
       "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Load your speaker head ckpt --------\n",
    "ckpt = torch.load(HEAD_PATH, map_location=\"cpu\")\n",
    "\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "SPEAKERS = ckpt.get(\"speakers\", list(speaker_to_id.keys()))\n",
    "\n",
    "# build reverse mapping (THIS IS THE FIX)\n",
    "id_to_speaker = {i: s for s, i in speaker_to_id.items()}\n",
    "\n",
    "fc_w = ckpt[\"state_dict\"][\"fc.weight\"]\n",
    "in_dim = int(fc_w.shape[1])\n",
    "num_classes = int(fc_w.shape[0])\n",
    "\n",
    "print(\"Loaded head:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"Head in_dim:\", in_dim, \"num_classes:\", num_classes)\n",
    "\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "head = SpeakerHead(in_dim=in_dim, num_classes=num_classes).to(TCAV_DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c3adcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrapped_model ready on cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class WeSpeakerForTCAV(nn.Module):\n",
    "    \"\"\"\n",
    "    TCAV-safe wrapper:\n",
    "    - Uses WeSpeaker backbone\n",
    "    - DOES NOT apply the speaker head\n",
    "    - Returns fake logits with correct shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F)\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.backbone(x)\n",
    "\n",
    "        # unwrap wespeaker output\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            emb = out[0]\n",
    "        else:\n",
    "            emb = out\n",
    "\n",
    "        # flatten anything strange\n",
    "        if emb.ndim == 0:\n",
    "            emb = emb.view(1, 1)\n",
    "        elif emb.ndim == 1:\n",
    "            emb = emb.unsqueeze(0)\n",
    "        elif emb.ndim > 2:\n",
    "            emb = emb.reshape(emb.size(0), -1)\n",
    "\n",
    "        B = emb.shape[0]\n",
    "\n",
    "        # ---- CRITICAL PART ----\n",
    "        # TCAV only needs gradients + target index.\n",
    "        # So return dummy logits with correct batch size.\n",
    "        logits = emb[:, :self.num_classes]\n",
    "\n",
    "\n",
    "        return logits\n",
    "\n",
    "wrapped_model = (\n",
    "    WeSpeakerForTCAV(\n",
    "        backbone=speaker.model,\n",
    "        num_classes=len(SPEAKERS),\n",
    "    )\n",
    "    .to(TCAV_DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# For TCAV we DO need gradients through backbone\n",
    "for p in wrapped_model.backbone.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "# # head grads not needed\n",
    "# for p in wrapped_model.head.parameters():\n",
    "#     p.requires_grad_(False)\n",
    "\n",
    "print(\"wrapped_model ready on\", next(wrapped_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f861c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.9.conv3 -> backbone.layer1.9.conv3\n",
      "layer2.19.conv3 -> backbone.layer2.19.conv3\n",
      "layer3.63.conv3 -> backbone.layer3.63.conv3\n",
      "layer4.2.conv3 -> backbone.layer4.2.conv3\n",
      "\n",
      "Using layers for TCAV:\n",
      "  backbone.layer1.9.conv3\n",
      "  backbone.layer2.19.conv3\n",
      "  backbone.layer3.63.conv3\n",
      "  backbone.layer4.2.conv3\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Resolve layer name strings for Captum (ALL layers) --------\n",
    "\n",
    "TARGET_LAYERS = {\n",
    "    \"layer1.9.conv3\": wrapped_model.backbone.layer1[9].conv3,\n",
    "    \"layer2.19.conv3\": wrapped_model.backbone.layer2[19].conv3,\n",
    "    \"layer3.63.conv3\": wrapped_model.backbone.layer3[63].conv3,\n",
    "    \"layer4.2.conv3\": wrapped_model.backbone.layer4[2].conv3,\n",
    "}\n",
    "\n",
    "\n",
    "def module_name_in_model(model: nn.Module, target_module: nn.Module) -> str:\n",
    "    for name, mod in model.named_modules():\n",
    "        if mod is target_module:\n",
    "            return name\n",
    "    raise RuntimeError(\"Could not find target module in wrapped_model.named_modules()\")\n",
    "\n",
    "\n",
    "# Resolve ALL layer names\n",
    "LAYER_NAMES = []\n",
    "for key, module in TARGET_LAYERS.items():\n",
    "    layer_name = module_name_in_model(wrapped_model, module)\n",
    "    LAYER_NAMES.append(layer_name)\n",
    "    print(f\"{key} -> {layer_name}\")\n",
    "\n",
    "print(\"\\nUsing layers for TCAV:\")\n",
    "for ln in LAYER_NAMES:\n",
    "    print(\" \", ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ce77a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred from concepts: N_MELS = 80 TARGET_FRAMES = 304\n",
      "Concepts: ['long_constant_thick', 'long_constant_thick_Vibrato', 'long_dropping_flat_thick', 'long_dropping_flat_thick_Vibrato', 'long_dropping_steep_thick', 'long_dropping_steep_thin', 'long_rising_flat_thick', 'long_rising_steep_thick', 'long_rising_steep_thin', 'short_constant_thick', 'short_dropping_steep_thick', 'short_dropping_steep_thin', 'short_rising_steep_thick', 'short_rising_steep_thin']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Infer N_MELS + TARGET_FRAMES from your concepts (avoid mismatch bugs) --------\n",
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "if not concept_dirs:\n",
    "    raise RuntimeError(f\"No concept folders in {CONCEPT_ROOT}\")\n",
    "\n",
    "\n",
    "def infer_mels_and_frames(concept_dirs: List[Path]) -> Tuple[int, int]:\n",
    "    for d in concept_dirs:\n",
    "        f = next(d.glob(\"*.npy\"), None)\n",
    "        if f is None:\n",
    "            continue\n",
    "        arr = np.load(f)\n",
    "        if arr.ndim != 2:\n",
    "            raise RuntimeError(\n",
    "                f\"Concept file {f} expected 2D [MELS, FRAMES], got {arr.shape}\"\n",
    "            )\n",
    "        return int(arr.shape[0]), int(arr.shape[1])\n",
    "    raise RuntimeError(\"Could not infer mel bins/frames from concept dirs\")\n",
    "\n",
    "\n",
    "N_MELS, TARGET_FRAMES = infer_mels_and_frames(concept_dirs)\n",
    "print(\"Inferred from concepts: N_MELS =\", N_MELS, \"TARGET_FRAMES =\", TARGET_FRAMES)\n",
    "\n",
    "concept_names = [d.name for d in concept_dirs]\n",
    "print(\"Concepts:\", concept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d223cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -------- Audio -> mel pipeline (MUST match concept space) --------\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_LENGTH_MS = 25\n",
    "FRAME_SHIFT_MS = 10\n",
    "WIN_LENGTH = int(SAMPLE_RATE * FRAME_LENGTH_MS / 1000)  # 400\n",
    "HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT_MS / 1000)  # 160\n",
    "N_FFT = WIN_LENGTH  # keep same as before (works for torchaudio)\n",
    "\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    win_length=WIN_LENGTH,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    center=True,  # אם אצלך ב-Activation-CAM זה עבד טוב, תשאיר True\n",
    "    power=2.0,\n",
    ").to(TCAV_DEVICE)\n",
    "\n",
    "\n",
    "def fix_mel_frames(mel_3d: torch.Tensor, target_frames: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel_3d: [1, N_MELS, T]\n",
    "    returns: [1, N_MELS, target_frames]\n",
    "    \"\"\"\n",
    "    Tcur = int(mel_3d.shape[-1])\n",
    "    if Tcur == target_frames:\n",
    "        return mel_3d\n",
    "    if Tcur > target_frames:\n",
    "        start = (Tcur - target_frames) // 2\n",
    "        return mel_3d[..., start : start + target_frames]\n",
    "    pad = target_frames - Tcur\n",
    "    return F.pad(mel_3d, (0, pad), mode=\"constant\", value=0.0)\n",
    "\n",
    "\n",
    "def postprocess_like_concepts(mel: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel: [1, N_MELS, T] (energy)\n",
    "    -> log + CMN over time (per mel bin)\n",
    "    \"\"\"\n",
    "    mel = torch.clamp(mel, min=0.0)\n",
    "    mel = torch.log(mel + eps)\n",
    "    mel = mel - mel.mean(dim=-1, keepdim=True)\n",
    "    return mel\n",
    "\n",
    "\n",
    "def wav_path_to_tcav_input(path: Path) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns input for TCAV / WeSpeaker backbone\n",
    "    Shape: [1, T, F]\n",
    "    \"\"\"\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = wav[:1].float()\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "\n",
    "    wav = wav.to(TCAV_DEVICE)\n",
    "\n",
    "    mel = mel_transform(wav)  # [1, F, T]\n",
    "    mel = postprocess_like_concepts(mel)\n",
    "    mel = fix_mel_frames(mel, TARGET_FRAMES)  # [1, F, T]\n",
    "\n",
    "    x_tf = mel.squeeze(0).transpose(0, 1)  # [T, F]\n",
    "    return x_tf.unsqueeze(0)  # [1, T, F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2d6f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -------- Concept datasets (FIXED for WeSpeaker ResNet293) --------\n",
    "# WeSpeaker expects x shape = (B, T, F)  (time first)\n",
    "# Your saved .npy concepts are (F, T) so we transpose -> (T, F).\n",
    "# Also: DO NOT unsqueeze(0) here, because DataLoader already creates batch dim.\n",
    "\n",
    "\n",
    "class ConceptNPYDataset(Dataset):\n",
    "    def __init__(self, concept_dir: Path, limit: Optional[int] = None):\n",
    "        self.files = sorted(concept_dir.glob(\"*.npy\"))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No .npy found in {concept_dir}\")\n",
    "        if limit is not None:\n",
    "            self.files = self.files[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel_ft = np.load(self.files[idx]).astype(np.float32)  # (F, T)\n",
    "        if mel_ft.ndim != 2:\n",
    "            raise RuntimeError(\n",
    "                f\"{self.files[idx].name}: expected 2D (F,T), got {mel_ft.shape}\"\n",
    "            )\n",
    "\n",
    "        F_bins, T_frames = mel_ft.shape\n",
    "        if F_bins != N_MELS:\n",
    "            raise RuntimeError(\n",
    "                f\"{self.files[idx].name}: expected F={N_MELS} mel bins, got {F_bins}\"\n",
    "            )\n",
    "        if T_frames != TARGET_FRAMES:\n",
    "            raise RuntimeError(\n",
    "                f\"{self.files[idx].name}: expected T={TARGET_FRAMES} frames, got {T_frames}\"\n",
    "            )\n",
    "\n",
    "        mel_tf = mel_ft.T  # (T, F)\n",
    "        x = torch.from_numpy(mel_tf)  # (T, F) on CPU (or TCAV_DEVICE)\n",
    "        return x.to(TCAV_DEVICE)\n",
    "\n",
    "\n",
    "class RandomMelDataset(Dataset):\n",
    "    def __init__(self, n_samples: int):\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # random in (T, F) because model expects (B, T, F)\n",
    "        x = torch.randn(TARGET_FRAMES, N_MELS, dtype=torch.float32)\n",
    "        return x.to(TCAV_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6ecacfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built experimental sets: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SpeakerRec/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/concept/_utils/classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Build TCAV concepts --------\n",
    "tcav = TCAV(wrapped_model, LAYER_NAMES, test_split_ratio=0.33)\n",
    "\n",
    "positive_concepts: List[Concept] = []\n",
    "for idx, cdir in enumerate(concept_dirs):\n",
    "    ds = ConceptNPYDataset(cdir, limit=CONCEPT_SAMPLES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0)\n",
    "    positive_concepts.append(Concept(id=idx, name=cdir.name, data_iter=dl))\n",
    "\n",
    "rand_ds = RandomMelDataset(n_samples=RANDOM_SAMPLES)\n",
    "rand_dl = DataLoader(\n",
    "    rand_ds, batch_size=BATCH_SIZE_CONCEPT, shuffle=False, num_workers=0\n",
    ")\n",
    "random_concept = Concept(id=len(positive_concepts), name=\"random\", data_iter=rand_dl)\n",
    "\n",
    "experimental_sets = [[c, random_concept] for c in positive_concepts]\n",
    "\n",
    "print(\"Built experimental sets:\", len(experimental_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26c81f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SpeakerRec/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/concept/_core/cav.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  save_dict = torch.load(cavs_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        concept name                layer name   cav acc\n",
      "34            long_rising_steep_thin  backbone.layer3.63.conv3  0.480769\n",
      "29           long_rising_steep_thick  backbone.layer2.19.conv3  0.480769\n",
      "27            long_rising_flat_thick   backbone.layer4.2.conv3  0.480769\n",
      "13  long_dropping_flat_thick_Vibrato  backbone.layer2.19.conv3  0.480769\n",
      "10          long_dropping_flat_thick  backbone.layer3.63.conv3  0.480769\n",
      "52           short_rising_steep_thin   backbone.layer1.9.conv3  0.442308\n",
      "51          short_rising_steep_thick   backbone.layer4.2.conv3  0.442308\n",
      "39              short_constant_thick   backbone.layer4.2.conv3  0.442308\n",
      "22          long_dropping_steep_thin  backbone.layer3.63.conv3  0.442308\n",
      "9           long_dropping_flat_thick  backbone.layer2.19.conv3  0.442308\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Compute CAV accuracies (sanity) --------\n",
    "def compute_cav_acc_df(\n",
    "    tcav: TCAV, positive_concepts: List[Concept], random_concept: Concept\n",
    ") -> pd.DataFrame:\n",
    "    cavs_dict = tcav.compute_cavs(\n",
    "        [[c, random_concept] for c in positive_concepts], force_train=FORCE_TRAIN_CAVS\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for concepts_key, layer_map in cavs_dict.items():\n",
    "        try:\n",
    "            pos_id = int(str(concepts_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_id < len(positive_concepts)):\n",
    "            continue\n",
    "        concept_name = positive_concepts[pos_id].name\n",
    "\n",
    "        for layer_name, cav_obj in layer_map.items():\n",
    "            if cav_obj is None or cav_obj.stats is None:\n",
    "                continue\n",
    "            acc = cav_obj.stats.get(\"accs\", None)\n",
    "            if acc is None:\n",
    "                acc = cav_obj.stats.get(\"acc\", None)\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.detach().cpu().item()\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"concept name\": concept_name,\n",
    "                    \"layer name\": layer_name,\n",
    "                    \"cav acc\": float(acc) if acc is not None else np.nan,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"concept name\", \"layer name\", \"cav acc\"])\n",
    "\n",
    "\n",
    "acc_df = compute_cav_acc_df(tcav, positive_concepts, random_concept)\n",
    "print(acc_df.sort_values(\"cav acc\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "63f6ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -------- Prediction helper (optional) --------\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_speaker_from_wav(path: Path) -> Tuple[str, float]:\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    wav = wav[:1].float()\n",
    "\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "    wav = wav.to(TCAV_DEVICE)\n",
    "\n",
    "    # THIS is the correct embedding extraction\n",
    "    emb = speaker.extract_embedding_from_pcm(wav, 16000)\n",
    "\n",
    "    emb = emb / (emb.norm(p=2) + 1e-12)  # same as training\n",
    "    emb = emb.unsqueeze(0) if emb.ndim == 1 else emb  # [1, D]\n",
    "\n",
    "    logits = head(emb)\n",
    "    probs = F.softmax(logits, dim=1)[0]\n",
    "\n",
    "    pred_id = int(torch.argmax(probs).item())\n",
    "    return id_to_speaker[pred_id], float(probs[pred_id].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "858e5074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]/home/SpeakerRec/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/concept/_core/cav.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  save_dict = torch.load(cavs_path)\n",
      "  0%|          | 0/90 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m pred_label, pred_prob \u001b[38;5;241m=\u001b[39m predict_speaker_from_wav(path)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 3) TCAV interpret\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m score_for_label \u001b[38;5;241m=\u001b[39m \u001b[43mtcav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp_key, layer_dict \u001b[38;5;129;01min\u001b[39;00m score_for_label\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/concept/_core/tcav.py:695\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[0;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m layer_module \u001b[38;5;241m=\u001b[39m _get_module_from_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, layer)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_attr_method\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m layer_module\n\u001b[0;32m--> 695\u001b[0m attribs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_attr_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_attr_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# self\u001b[39;49;00m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m attribs \u001b[38;5;241m=\u001b[39m _format_tensor_into_tuples(attribs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# n_inputs x n_features\u001b[39;00m\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/attr/_core/layer/layer_gradient_x_activation.py:170\u001b[0m, in \u001b[0;36mLayerGradientXActivation.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    165\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(\n\u001b[1;32m    166\u001b[0m     additional_forward_args\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Returns gradient of output with respect to\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# hidden layer and hidden layer evaluated at each input.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m layer_gradients, layer_evals \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_layer_gradients_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer, Module):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _format_output(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28mlen\u001b[39m(layer_evals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiply_gradient_acts(layer_gradients, layer_evals),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/captum/_utils/gradient.py:644\u001b[0m, in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[0;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m all_layers: List[Module] \u001b[38;5;241m=\u001b[39m [layer] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Module) \u001b[38;5;28;01melse\u001b[39;00m layer\n\u001b[1;32m    638\u001b[0m grad_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    639\u001b[0m     layer_tensor\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m single_layer \u001b[38;5;129;01min\u001b[39;00m all_layers\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device_id \u001b[38;5;129;01min\u001b[39;00m key_list\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_tensor \u001b[38;5;129;01min\u001b[39;00m saved_layer[single_layer][device_id]\n\u001b[1;32m    643\u001b[0m )\n\u001b[0;32m--> 644\u001b[0m saved_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    647\u001b[0m all_grads: List[Tuple[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/torch/autograd/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    432\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    433\u001b[0m         grad_outputs_\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    447\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    449\u001b[0m     ):\n",
      "File \u001b[0;32m~/BioVoice/.venvResnet/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# -------- Run TCAV on your ranked CSV --------\n",
    "df_attr = pd.read_csv(ATTR_CSV_PATH)\n",
    "\n",
    "if \"path\" not in df_attr.columns or \"speaker\" not in df_attr.columns:\n",
    "    raise RuntimeError(\n",
    "        f\"CSV must contain columns ['path','speaker']. Got: {list(df_attr.columns)}\"\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, r in tqdm(df_attr.iterrows(), total=len(df_attr)):\n",
    "    path = Path(r[\"path\"])\n",
    "    true_label = str(r[\"speaker\"])\n",
    "\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    if true_label not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    # 1) build mel in SAME space as concepts: (B,T,F)\n",
    "    mel_ft = wav_path_to_mel3d(path)  # (1, F, T)\n",
    "    x = mel_ft.transpose(1, 2)  # (1, T, F)\n",
    "\n",
    "    # 2) make it require grad BUT not a leaf (avoid wespeaker in-place error)\n",
    "    x = x.detach().requires_grad_(True)\n",
    "    x = x * 1.0\n",
    "\n",
    "    target_idx = speaker_to_id[true_label]\n",
    "\n",
    "    # optional prediction (not used by TCAV graph)\n",
    "    pred_label, pred_prob = predict_speaker_from_wav(path)\n",
    "\n",
    "    # 3) TCAV interpret\n",
    "    score_for_label = tcav.interpret(\n",
    "        inputs=x,\n",
    "        experimental_sets=experimental_sets,\n",
    "        target=target_idx,\n",
    "    )\n",
    "\n",
    "    for exp_key, layer_dict in score_for_label.items():\n",
    "        try:\n",
    "            pos_idx = int(str(exp_key).split(\"-\")[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not (0 <= pos_idx < len(positive_concepts)):\n",
    "            continue\n",
    "\n",
    "        concept_name = positive_concepts[pos_idx].name\n",
    "\n",
    "        for layer_name, metrics in layer_dict.items():\n",
    "            sc = metrics.get(\"sign_count\")\n",
    "            mg = metrics.get(\"magnitude\")\n",
    "            if sc is None or mg is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(sc, torch.Tensor):\n",
    "                sc = sc.detach().cpu().tolist()\n",
    "            if isinstance(mg, torch.Tensor):\n",
    "                mg = mg.detach().cpu().tolist()\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"path\": str(path),\n",
    "                    \"concept name\": concept_name,\n",
    "                    \"layer name\": layer_name,\n",
    "                    \"positive percentage\": float(sc[0]),\n",
    "                    \"magnitude\": float(mg[0]),\n",
    "                    \"true label\": true_label,\n",
    "                    \"predicted label\": pred_label,\n",
    "                    \"predicted probability\": float(pred_prob),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_tcav = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"path\",\n",
    "        \"concept name\",\n",
    "        \"layer name\",\n",
    "        \"positive percentage\",\n",
    "        \"magnitude\",\n",
    "        \"true label\",\n",
    "        \"predicted label\",\n",
    "        \"predicted probability\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_tcav = df_tcav.merge(acc_df, on=[\"concept name\", \"layer name\"], how=\"left\")\n",
    "df_tcav.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved →\", OUT_CSV)\n",
    "df_tcav.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvResnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
