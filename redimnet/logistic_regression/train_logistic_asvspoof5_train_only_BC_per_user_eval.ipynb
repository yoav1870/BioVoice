{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASVspoof5 Train-Only Probe (B -> C): Bonafide vs Spoof with Per-User Test Analysis\n",
    "\n",
    "This notebook trains one global `B -> C` bonafide/spoof probe, then reports **per-user metrics** on the `C` test speakers.\n",
    "\n",
    "This is the recommended first version of \"per-user\" analysis (global model + user-specific evaluation), instead of training a separate model per user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/SpeakerRec/BioVoice\")\n",
    "MANIFEST_PATH = PROJECT_ROOT / \"redimnet\" / \"tcav\" / \"deepfakes\" / \"asvspoof5\" / \"asvspoof5_train_only_selected_utterances_plan.csv\"\n",
    "SUBSET_AUDIO_ROOT = PROJECT_ROOT / \"data\" / \"datasets\" / \"asvspoof5_train_only_subset_audio\"\n",
    "\n",
    "EMBED_CACHE_DIR = PROJECT_ROOT / \"data\" / \"embeddings\" / \"asvspoof5_train_only_abc\"\n",
    "EMBED_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "print(\"SUBSET_AUDIO_ROOT:\", SUBSET_AUDIO_ROOT)\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / 'data' / 'models' / 'asvspoof5_train_only_probe_BC_per_user'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_NPZ = EMBED_CACHE_DIR / 'embeddings_BC_global.npz'\n",
    "FORCE_RECOMPUTE_EMBEDDINGS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"bonafide\": 0, \"spoof\": 1}\n",
    "\n",
    "\n",
    "def build_audio_path(row, subset_root: Path) -> Path:\n",
    "    # extracted subset layout: <root>/<group>/<label>/<utt_id>.flac\n",
    "    return subset_root / str(row[\"group\"]) / str(row[\"label\"]) / f\"{row['utt_id']}.flac\"\n",
    "\n",
    "\n",
    "def load_manifest(manifest_path: Path, subset_root: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(manifest_path)\n",
    "    req = {\"group\", \"speaker_id\", \"utt_id\", \"label\", \"system_id\"}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Manifest missing columns: {sorted(missing)}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"label_str\"] = df[\"label\"].astype(str)\n",
    "    df[\"label_id\"] = df[\"label_str\"].map(LABEL_MAP).astype(int)\n",
    "    df[\"audio_path\"] = df.apply(lambda r: str(build_audio_path(r, subset_root)), axis=1)\n",
    "    df[\"audio_exists\"] = df[\"audio_path\"].map(lambda p: Path(p).exists())\n",
    "    return df\n",
    "\n",
    "\n",
    "def embed_with_redim(model, wav_path: str, device: str) -> np.ndarray:\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav[:1, :]\n",
    "    wav = wav.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(wav)\n",
    "    return emb.squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_embeddings_for_df(df: pd.DataFrame, model, device: str, cache_npz: Path, force_recompute: bool = False):\n",
    "    if cache_npz.exists() and not force_recompute:\n",
    "        payload = np.load(cache_npz, allow_pickle=True)\n",
    "        emb = payload[\"X\"]\n",
    "        utt_ids = payload[\"utt_ids\"].astype(str)\n",
    "        cache_df = pd.DataFrame({\"utt_id\": utt_ids, \"_emb_idx\": np.arange(len(utt_ids))})\n",
    "        merged = df.merge(cache_df, on=\"utt_id\", how=\"left\", validate=\"one_to_one\")\n",
    "        if merged[\"_emb_idx\"].isna().any():\n",
    "            missing = merged.loc[merged[\"_emb_idx\"].isna(), \"utt_id\"].tolist()[:10]\n",
    "            raise RuntimeError(f\"Cache missing utt_ids, examples: {missing}\")\n",
    "        X = emb[merged[\"_emb_idx\"].astype(int).to_numpy()]\n",
    "        return X\n",
    "\n",
    "    rows = []\n",
    "    vecs = []\n",
    "    for rec in tqdm(df.to_dict(\"records\"), desc=f\"Embedding {len(df)} samples\"):\n",
    "        p = Path(rec[\"audio_path\"])\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing audio: {p}\")\n",
    "        vecs.append(embed_with_redim(model, str(p), device))\n",
    "        rows.append(str(rec[\"utt_id\"]))\n",
    "    X = np.stack(vecs).astype(np.float32)\n",
    "    np.savez_compressed(cache_npz, X=X, utt_ids=np.array(rows, dtype=object))\n",
    "    return X\n",
    "\n",
    "\n",
    "def train_and_eval_logreg(X_train, y_train, X_test, y_test, class_weight=\"balanced\"):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight=class_weight, random_state=42)\n",
    "    clf.fit(X_train_s, y_train)\n",
    "\n",
    "    p_train = clf.predict_proba(X_train_s)[:, 1]\n",
    "    p_test = clf.predict_proba(X_test_s)[:, 1]\n",
    "    yhat_train = (p_train >= 0.5).astype(int)\n",
    "    yhat_test = (p_test >= 0.5).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, yhat_train)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, yhat_test)),\n",
    "        \"train_auc\": float(roc_auc_score(y_train, p_train)) if len(np.unique(y_train)) == 2 else None,\n",
    "        \"test_auc\": float(roc_auc_score(y_test, p_test)) if len(np.unique(y_test)) == 2 else None,\n",
    "        \"train_confusion_matrix\": confusion_matrix(y_train, yhat_train).tolist(),\n",
    "        \"test_confusion_matrix\": confusion_matrix(y_test, yhat_test).tolist(),\n",
    "        \"train_classification_report\": classification_report(y_train, yhat_train, output_dict=True, zero_division=0),\n",
    "        \"test_classification_report\": classification_report(y_test, yhat_test, output_dict=True, zero_division=0),\n",
    "    }\n",
    "    return scaler, clf, X_train_s, X_test_s, p_train, p_test, yhat_train, yhat_test, metrics\n",
    "\n",
    "\n",
    "def plot_confmat(cm, title):\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels([\"bonafide\",\"spoof\"], rotation=30, ha=\"right\")\n",
    "    ax.set_yticklabels([\"bonafide\",\"spoof\"])\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ReDimNet backbone for embedding extraction\n",
    "redim_model = (\n",
    "    torch.hub.load(\n",
    "        \"IDRnD/ReDimNet\",\n",
    "        \"ReDimNet\",\n",
    "        model_name=\"b5\",\n",
    "        train_type=\"ptn\",\n",
    "        dataset=\"vox2\",\n",
    "    )\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "print(\"Loaded ReDimNet on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_df = load_manifest(MANIFEST_PATH, SUBSET_AUDIO_ROOT)\n",
    "bc_df = manifest_df[manifest_df['group'].isin(['B','C'])].copy().reset_index(drop=True)\n",
    "X_bc = extract_embeddings_for_df(bc_df[['utt_id','audio_path']], redim_model, DEVICE, CACHE_NPZ, force_recompute=FORCE_RECOMPUTE_EMBEDDINGS)\n",
    "y_bc = bc_df['label_id'].to_numpy().astype(int)\n",
    "\n",
    "train_mask = bc_df['group'].eq('B').to_numpy()\n",
    "test_mask = bc_df['group'].eq('C').to_numpy()\n",
    "X_train, y_train = X_bc[train_mask], y_bc[train_mask]\n",
    "X_test, y_test = X_bc[test_mask], y_bc[test_mask]\n",
    "\n",
    "scaler, clf, X_train_s, X_test_s, p_train, p_test, yhat_train, yhat_test, metrics = train_and_eval_logreg(X_train, y_train, X_test, y_test)\n",
    "print('Global test accuracy:', metrics['test_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build predictions table for B and C, then compute per-user metrics across all users (with split column)\n",
    "pred_train = bc_df.loc[train_mask, ['group','speaker_id','utt_id','label_str','label_id','system_id']].copy().reset_index(drop=True)\n",
    "pred_train['split'] = 'train_B'\n",
    "pred_train['prob_spoof'] = p_train\n",
    "pred_train['pred_label_id'] = yhat_train\n",
    "pred_train['pred_label_str'] = pred_train['pred_label_id'].map({0:'bonafide',1:'spoof'})\n",
    "\n",
    "pred_test = bc_df.loc[test_mask, ['group','speaker_id','utt_id','label_str','label_id','system_id']].copy().reset_index(drop=True)\n",
    "pred_test['split'] = 'test_C'\n",
    "pred_test['prob_spoof'] = p_test\n",
    "pred_test['pred_label_id'] = yhat_test\n",
    "pred_test['pred_label_str'] = pred_test['pred_label_id'].map({0:'bonafide',1:'spoof'})\n",
    "\n",
    "pred_all = pd.concat([pred_train, pred_test], ignore_index=True)\n",
    "pred_all.to_csv(OUT_DIR / 'predictions_BC_all_users.csv', index=False)\n",
    "pred_test.to_csv(OUT_DIR / 'test_predictions_C.csv', index=False)\n",
    "\n",
    "rows = []\n",
    "for (split, spk), g in pred_all.groupby(['split','speaker_id']):\n",
    "    y_true = g['label_id'].to_numpy()\n",
    "    y_hat = g['pred_label_id'].to_numpy()\n",
    "    p = g['prob_spoof'].to_numpy()\n",
    "    rows.append({\n",
    "        'split': split,\n",
    "        'group': g['group'].iloc[0],\n",
    "        'speaker_id': spk,\n",
    "        'n_samples': int(len(g)),\n",
    "        'n_bonafide': int((y_true==0).sum()),\n",
    "        'n_spoof': int((y_true==1).sum()),\n",
    "        'accuracy': float(accuracy_score(y_true, y_hat)),\n",
    "        'auc': float(roc_auc_score(y_true, p)) if len(np.unique(y_true)) == 2 else np.nan,\n",
    "        'bonafide_recall': float(((y_hat[y_true==0]==0).mean()) if (y_true==0).any() else np.nan),\n",
    "        'spoof_recall': float(((y_hat[y_true==1]==1).mean()) if (y_true==1).any() else np.nan),\n",
    "    })\n",
    "per_user_df = pd.DataFrame(rows).sort_values(['split','accuracy'], ascending=[True, False])\n",
    "per_user_df.to_csv(OUT_DIR / 'per_user_metrics_BC.csv', index=False)\n",
    "per_user_df[per_user_df['split']=='test_C'].to_csv(OUT_DIR / 'per_user_metrics_C.csv', index=False)\n",
    "\n",
    "with open(OUT_DIR / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(OUT_DIR / 'logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "(OUT_DIR / 'global_metrics.json').write_text(json.dumps(metrics, indent=2), encoding='utf-8')\n",
    "\n",
    "print('Per-user metrics across all users (B and C):')\n",
    "print(per_user_df)\n",
    "print('Saved outputs ->', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-user performance for both train users (B) and test users (C)\n",
    "for split_name, color in [('train_B', '#4e79a7'), ('test_C', '#59a14f')]:\n",
    "    plot_df = per_user_df[per_user_df['split'] == split_name].sort_values('accuracy')\n",
    "    if plot_df.empty:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.barh(plot_df['speaker_id'], plot_df['accuracy'], color=color)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_title(f'Per-User Accuracy ({split_name})')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for split_name, marker in [('train_B','o'), ('test_C','s')]:\n",
    "    g = per_user_df[per_user_df['split'] == split_name]\n",
    "    if g.empty:\n",
    "        continue\n",
    "    ax.scatter(g['speaker_id'], g['bonafide_recall'], label=f'{split_name} bonafide recall', marker=marker)\n",
    "    ax.scatter(g['speaker_id'], g['spoof_recall'], label=f'{split_name} spoof recall', marker='x')\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Per-User Class Recall (B and C)')\n",
    "ax.legend(fontsize=8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: inspect one test user confusion matrix\n",
    "candidate = per_user_df[per_user_df['split'] == 'test_C']\n",
    "speaker_to_show = candidate.iloc[0]['speaker_id'] if len(candidate) else None\n",
    "if speaker_to_show is not None:\n",
    "    g = pred_all[(pred_all['split'] == 'test_C') & (pred_all['speaker_id'] == speaker_to_show)]\n",
    "    cm = confusion_matrix(g['label_id'], g['pred_label_id'])\n",
    "    print('Speaker:', speaker_to_show)\n",
    "    display(g.head())\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels(['bonafide','spoof'], rotation=30, ha='right')\n",
    "    ax.set_yticklabels(['bonafide','spoof'])\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "    ax.set_title(f'Confusion Matrix - {speaker_to_show}')\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha='center', va='center')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}