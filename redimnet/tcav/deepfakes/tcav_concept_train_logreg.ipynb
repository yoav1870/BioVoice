{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 2. Train Logistic Regression (15 Users Train / 5 Users Test)\n",
        "\n",
        "This notebook trains a speaker-independent logistic regression on the prepared sample-level CSV.\n",
        "\n",
        "Input: output from `tcav_concept_prepare_csv.ipynb`\n",
        "Output: train/test splits, metrics, coefficients, per-user contributions, predictions.\n",
        "\n",
        "This notebook is self-contained (does not import your `.py` script).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "PROJECT_ROOT = Path('/home/SpeakerRec/BioVoice')\n",
        "PREPARED_CSV = PROJECT_ROOT / 'data' / 'tcav' / 'prepared_csvs' / 'stage4_spoofwrapper_positive_percentage' / 'sample_level_features_all.csv'\n",
        "OUT_DIR = PROJECT_ROOT / 'data' / 'tcav' / 'logreg_concept_analysis' / 'stage4_spoofwrapper_pospct'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "NUM_SPEAKERS = 20\n",
        "TRAIN_SPEAKERS = 15\n",
        "TEST_SPEAKERS = 5\n",
        "RANDOM_SEED = 42\n",
        "PREFER_BALANCED_SPEAKERS = True\n",
        "\n",
        "print('PREPARED_CSV =', PREPARED_CSV)\n",
        "print('OUT_DIR =', OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load prepared sample-level CSV\n",
        "assert PREPARED_CSV.exists(), f'Missing prepared CSV: {PREPARED_CSV}'\n",
        "df_wide = pd.read_csv(PREPARED_CSV)\n",
        "\n",
        "required_meta = ['idx', 'speaker_id', 'system_id', 'key', 'true label']\n",
        "for c in required_meta:\n",
        "    assert c in df_wide.columns, f'Missing column: {c}'\n",
        "\n",
        "feature_cols = [c for c in df_wide.columns if '__' in c]\n",
        "assert feature_cols, 'No feature columns found (expected names like metric__concept)'\n",
        "\n",
        "df_wide['idx'] = df_wide['idx'].astype(str)\n",
        "df_wide['speaker_id'] = df_wide['speaker_id'].astype(str)\n",
        "df_wide['true label'] = pd.to_numeric(df_wide['true label'], errors='coerce').astype(int)\n",
        "\n",
        "print('Rows:', len(df_wide))\n",
        "print('Speakers:', df_wide['speaker_id'].nunique())\n",
        "print('Features:', len(feature_cols))\n",
        "print('Class counts (true label):', df_wide['true label'].value_counts().sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Speaker selection and 15/5 split (speaker-independent)\n",
        "def choose_speakers(df, num_speakers=20, seed=42, prefer_balanced=True):\n",
        "    speakers = sorted(df['speaker_id'].astype(str).unique().tolist())\n",
        "    if len(speakers) < num_speakers:\n",
        "        raise ValueError(f'Requested {num_speakers} speakers, found {len(speakers)}')\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    if prefer_balanced:\n",
        "        counts = df.groupby(['speaker_id', 'true label']).size().unstack(fill_value=0)\n",
        "        has_real = counts[0] > 0 if 0 in counts.columns else pd.Series(False, index=counts.index)\n",
        "        has_fake = counts[1] > 0 if 1 in counts.columns else pd.Series(False, index=counts.index)\n",
        "        balanced = counts.index[(has_real) & (has_fake)].astype(str).tolist()\n",
        "        if len(balanced) >= num_speakers:\n",
        "            return sorted(rng.choice(sorted(balanced), size=num_speakers, replace=False).tolist())\n",
        "\n",
        "    return sorted(rng.choice(speakers, size=num_speakers, replace=False).tolist())\n",
        "\n",
        "selected_speakers = choose_speakers(df_wide, NUM_SPEAKERS, RANDOM_SEED, PREFER_BALANCED_SPEAKERS)\n",
        "train_speakers, test_speakers = train_test_split(\n",
        "    selected_speakers,\n",
        "    train_size=TRAIN_SPEAKERS,\n",
        "    test_size=TEST_SPEAKERS,\n",
        "    random_state=RANDOM_SEED,\n",
        "    shuffle=True,\n",
        ")\n",
        "train_speakers = sorted(train_speakers)\n",
        "test_speakers = sorted(test_speakers)\n",
        "\n",
        "selected_df = df_wide[df_wide['speaker_id'].isin(selected_speakers)].copy()\n",
        "train_df = selected_df[selected_df['speaker_id'].isin(train_speakers)].copy()\n",
        "test_df = selected_df[selected_df['speaker_id'].isin(test_speakers)].copy()\n",
        "\n",
        "print('Train speakers:', train_speakers)\n",
        "print('Test speakers :', test_speakers)\n",
        "print('Train rows:', len(train_df), '| class counts:', train_df['true label'].value_counts().sort_index().to_dict())\n",
        "print('Test rows :', len(test_df),  '| class counts:', test_df['true label'].value_counts().sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train logistic regression\n",
        "X_train = train_df[feature_cols].copy()\n",
        "X_test = test_df[feature_cols].copy()\n",
        "\n",
        "# Fill missing values using training medians\n",
        "medians = X_train.median(numeric_only=True)\n",
        "X_train = X_train.fillna(medians)\n",
        "X_test = X_test.fillna(medians)\n",
        "\n",
        "y_train = train_df['true label'].astype(int).to_numpy()  # 1=fake, 0=real\n",
        "y_test = test_df['true label'].astype(int).to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear', random_state=0)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "y_prob_fake = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': float(accuracy_score(y_test, y_pred)),\n",
        "    'precision_fake_1': float(precision_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'recall_fake_1': float(recall_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'f1_fake_1': float(f1_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "    'confusion_matrix_labels_[0_real,1_fake]': confusion_matrix(y_test, y_pred, labels=[0,1]).tolist(),\n",
        "    'classification_report': classification_report(y_test, y_pred, labels=[0,1], zero_division=0),\n",
        "}\n",
        "print(json.dumps({k:v for k,v in metrics.items() if k != 'classification_report'}, indent=2))\n",
        "print(metrics['classification_report'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Global concept importance (coefficients)\n",
        "coef = clf.coef_.ravel()\n",
        "coef_rows = []\n",
        "for feature, c in zip(feature_cols, coef):\n",
        "    metric, concept = feature.split('__', 1)\n",
        "    coef_rows.append({\n",
        "        'feature': feature,\n",
        "        'metric': metric,\n",
        "        'concept': concept,\n",
        "        'coefficient': float(c),\n",
        "        'abs_coefficient': float(abs(c)),\n",
        "        'direction': 'fake' if c > 0 else 'real' if c < 0 else 'neutral'\n",
        "    })\n",
        "coef_df = pd.DataFrame(coef_rows).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
        "display(coef_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Class-wise concept summary (real vs fake means)\n",
        "summary_rows = []\n",
        "for feature in feature_cols:\n",
        "    metric, concept = feature.split('__', 1)\n",
        "    grp = selected_df.groupby('true label')[feature].agg(['mean', 'median'])\n",
        "    real_mean = float(grp.loc[0, 'mean']) if 0 in grp.index and pd.notna(grp.loc[0, 'mean']) else np.nan\n",
        "    fake_mean = float(grp.loc[1, 'mean']) if 1 in grp.index and pd.notna(grp.loc[1, 'mean']) else np.nan\n",
        "    real_median = float(grp.loc[0, 'median']) if 0 in grp.index and pd.notna(grp.loc[0, 'median']) else np.nan\n",
        "    fake_median = float(grp.loc[1, 'median']) if 1 in grp.index and pd.notna(grp.loc[1, 'median']) else np.nan\n",
        "    summary_rows.append({\n",
        "        'feature': feature,\n",
        "        'metric': metric,\n",
        "        'concept': concept,\n",
        "        'real_mean_true_label_0': real_mean,\n",
        "        'fake_mean_true_label_1': fake_mean,\n",
        "        'real_median_true_label_0': real_median,\n",
        "        'fake_median_true_label_1': fake_median,\n",
        "        'mean_diff_fake_minus_real': fake_mean - real_mean if pd.notna(fake_mean) and pd.notna(real_mean) else np.nan,\n",
        "    })\n",
        "class_summary_df = pd.DataFrame(summary_rows)\n",
        "display(class_summary_df.sort_values('mean_diff_fake_minus_real', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Per-sample and per-user concept contributions on test set\n",
        "contrib_matrix = X_test_scaled * clf.coef_.ravel().reshape(1, -1)\n",
        "contrib_df = pd.DataFrame(contrib_matrix, columns=feature_cols)\n",
        "\n",
        "pred_df = test_df[['idx', 'speaker_id', 'system_id', 'key', 'true label']].reset_index(drop=True).copy()\n",
        "pred_df['pred_label'] = y_pred\n",
        "pred_df['pred_prob_fake'] = y_prob_fake\n",
        "\n",
        "sample_contrib_long = pd.concat([pred_df[['idx', 'speaker_id', 'system_id', 'key', 'true label']], contrib_df], axis=1).melt(\n",
        "    id_vars=['idx', 'speaker_id', 'system_id', 'key', 'true label'],\n",
        "    var_name='feature', value_name='contribution'\n",
        ")\n",
        "sample_contrib_long[['metric', 'concept']] = sample_contrib_long['feature'].str.split('__', n=1, expand=True)\n",
        "\n",
        "user_contrib = (\n",
        "    sample_contrib_long\n",
        "    .groupby(['speaker_id', 'true label', 'feature', 'metric', 'concept'], as_index=False)['contribution']\n",
        "    .mean()\n",
        "    .rename(columns={'contribution': 'mean_contribution'})\n",
        ")\n",
        "\n",
        "def top_concepts_per_user(user_df, top_k=3):\n",
        "    rows = []\n",
        "    for (speaker_id, true_label), g in user_df.groupby(['speaker_id', 'true label']):\n",
        "        g_desc = g.sort_values('mean_contribution', ascending=False)\n",
        "        g_asc = g.sort_values('mean_contribution', ascending=True)\n",
        "        for rank, (_, r) in enumerate(g_desc.head(top_k).iterrows(), start=1):\n",
        "            rows.append({'speaker_id': speaker_id, 'true label': int(true_label), 'list_type': 'top_fake_supporting', 'rank': rank,\n",
        "                         'feature': r['feature'], 'metric': r['metric'], 'concept': r['concept'], 'mean_contribution': float(r['mean_contribution'])})\n",
        "        for rank, (_, r) in enumerate(g_asc.head(top_k).iterrows(), start=1):\n",
        "            rows.append({'speaker_id': speaker_id, 'true label': int(true_label), 'list_type': 'top_real_supporting', 'rank': rank,\n",
        "                         'feature': r['feature'], 'metric': r['metric'], 'concept': r['concept'], 'mean_contribution': float(r['mean_contribution'])})\n",
        "    return pd.DataFrame(rows).sort_values(['speaker_id','true label','list_type','rank']).reset_index(drop=True)\n",
        "\n",
        "top_user_df = top_concepts_per_user(user_contrib, top_k=3)\n",
        "display(top_user_df.head(30))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save training outputs for later visualization notebook\n",
        "train_df.to_csv(OUT_DIR / 'train_samples_15speakers.csv', index=False)\n",
        "test_df.to_csv(OUT_DIR / 'test_samples_5speakers.csv', index=False)\n",
        "selected_df.to_csv(OUT_DIR / 'sample_level_features_selected_20speakers.csv', index=False)\n",
        "coef_df.to_csv(OUT_DIR / 'global_concept_coefficients.csv', index=False)\n",
        "class_summary_df.to_csv(OUT_DIR / 'classwise_concept_summary.csv', index=False)\n",
        "pred_df.to_csv(OUT_DIR / 'test_predictions.csv', index=False)\n",
        "sample_contrib_long.to_csv(OUT_DIR / 'test_sample_contributions_long.csv', index=False)\n",
        "user_contrib.to_csv(OUT_DIR / 'test_user_mean_contributions.csv', index=False)\n",
        "top_user_df.to_csv(OUT_DIR / 'test_user_top_concepts.csv', index=False)\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(scaler, OUT_DIR / 'scaler.joblib')\n",
        "    joblib.dump(clf, OUT_DIR / 'logreg.joblib')\n",
        "except Exception as e:\n",
        "    print('[WARN] Could not save joblib files:', e)\n",
        "\n",
        "run_metadata = {\n",
        "    'prepared_csv': str(PREPARED_CSV),\n",
        "    'out_dir': str(OUT_DIR),\n",
        "    'random_seed': RANDOM_SEED,\n",
        "    'speaker_split': {\n",
        "        'train_speakers': train_speakers,\n",
        "        'test_speakers': test_speakers,\n",
        "    },\n",
        "    'counts': {\n",
        "        'all_rows': int(len(df_wide)),\n",
        "        'selected_rows': int(len(selected_df)),\n",
        "        'train_rows': int(len(train_df)),\n",
        "        'test_rows': int(len(test_df)),\n",
        "    },\n",
        "    'label_mapping': {\n",
        "        'true_label_0': 'real',\n",
        "        'true_label_1': 'fake'\n",
        "    },\n",
        "    'metrics': metrics,\n",
        "}\n",
        "(OUT_DIR / 'run_metadata.json').write_text(json.dumps(run_metadata, indent=2), encoding='utf-8')\n",
        "print('Saved outputs to', OUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}