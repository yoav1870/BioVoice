{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASVspoof5 Train-Only Probe (Tuned) ? B -> C with Per-User Evaluation\n",
    "\n",
    "Trains a tuned global probe (speaker-disjoint validation inside `B`) and reports per-user metrics for all `B` and `C` users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROJECT_ROOT = Path(\"/home/SpeakerRec/BioVoice\")\n",
    "MANIFEST_PATH = PROJECT_ROOT / \"redimnet\" / \"tcav\" / \"deepfakes\" / \"asvspoof5\" / \"asvspoof5_train_only_selected_utterances_plan.csv\"\n",
    "SUBSET_AUDIO_ROOT = PROJECT_ROOT / \"data\" / \"datasets\" / \"asvspoof5_train_only_subset_audio\"\n",
    "EMBED_CACHE_DIR = PROJECT_ROOT / \"data\" / \"embeddings\" / \"asvspoof5_train_only_abc\"\n",
    "EMBED_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tuning config (speaker-disjoint split inside group B)\n",
    "VAL_SPEAKER_COUNT = 3      # from 15 B speakers -> 12 train, 3 val\n",
    "TUNE_SEED = 42\n",
    "C_GRID = [0.01, 0.1, 1.0, 10.0]\n",
    "CLASS_WEIGHT_OPTIONS = [None, 'balanced']\n",
    "THRESH_GRID = np.linspace(0.05, 0.95, 181)\n",
    "FORCE_RECOMPUTE_EMBEDDINGS = False\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "print(\"SUBSET_AUDIO_ROOT:\", SUBSET_AUDIO_ROOT)\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / 'data' / 'models' / 'asvspoof5_train_only_probe_BC_per_user_tuned'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_NPZ = EMBED_CACHE_DIR / 'embeddings_BC_global.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"bonafide\": 0, \"spoof\": 1}\n",
    "\n",
    "\n",
    "def build_audio_path(row, subset_root: Path) -> Path:\n",
    "    return subset_root / str(row[\"group\"]) / str(row[\"label\"]) / f\"{row['utt_id']}.flac\"\n",
    "\n",
    "\n",
    "def load_manifest(manifest_path: Path, subset_root: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(manifest_path).copy()\n",
    "    req = {\"group\",\"speaker_id\",\"utt_id\",\"label\",\"system_id\"}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Manifest missing columns: {sorted(missing)}\")\n",
    "    df[\"label_str\"] = df[\"label\"].astype(str)\n",
    "    df[\"label_id\"] = df[\"label_str\"].map(LABEL_MAP).astype(int)\n",
    "    df[\"audio_path\"] = df.apply(lambda r: str(build_audio_path(r, subset_root)), axis=1)\n",
    "    df[\"audio_exists\"] = df[\"audio_path\"].map(lambda p: Path(p).exists())\n",
    "    return df\n",
    "\n",
    "\n",
    "def embed_with_redim(model, wav_path: str, device: str) -> np.ndarray:\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav[:1, :]\n",
    "    wav = wav.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(wav)\n",
    "    return emb.squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_embeddings_for_df(df_paths: pd.DataFrame, model, device: str, cache_npz: Path, force_recompute: bool=False):\n",
    "    if cache_npz.exists() and not force_recompute:\n",
    "        payload = np.load(cache_npz, allow_pickle=True)\n",
    "        X = payload['X']\n",
    "        utt_ids = payload['utt_ids'].astype(str)\n",
    "        lut = pd.DataFrame({'utt_id': utt_ids, '_idx': np.arange(len(utt_ids))})\n",
    "        m = df_paths[['utt_id']].merge(lut, on='utt_id', how='left', validate='one_to_one')\n",
    "        if m['_idx'].isna().any():\n",
    "            miss = m.loc[m['_idx'].isna(), 'utt_id'].tolist()[:10]\n",
    "            raise RuntimeError(f\"Embedding cache missing utt_ids, examples: {miss}\")\n",
    "        return X[m['_idx'].astype(int).to_numpy()]\n",
    "\n",
    "    vecs, ids = [], []\n",
    "    for rec in tqdm(df_paths.to_dict('records'), desc=f\"Embedding {len(df_paths)}\"):\n",
    "        p = Path(rec['audio_path'])\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing audio: {p}\")\n",
    "        vecs.append(embed_with_redim(model, str(p), device))\n",
    "        ids.append(str(rec['utt_id']))\n",
    "    X = np.stack(vecs).astype(np.float32)\n",
    "    np.savez_compressed(cache_npz, X=X, utt_ids=np.array(ids, dtype=object))\n",
    "    return X\n",
    "\n",
    "\n",
    "def split_B_speakers(df: pd.DataFrame, n_val_speakers=3, seed=42):\n",
    "    b_speakers = sorted(df.loc[df['group'].eq('B'), 'speaker_id'].astype(str).unique().tolist())\n",
    "    if len(b_speakers) <= n_val_speakers:\n",
    "        raise ValueError(f\"Not enough B speakers ({len(b_speakers)}) for val={n_val_speakers}\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    shuffled = b_speakers.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    val = sorted(shuffled[:n_val_speakers])\n",
    "    train = sorted(shuffled[n_val_speakers:])\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, p1, thr):\n",
    "    y_hat = (p1 >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_hat).tolist()\n",
    "    out = {\n",
    "        'threshold': float(thr),\n",
    "        'accuracy': float(accuracy_score(y_true, y_hat)),\n",
    "        'auc': float(roc_auc_score(y_true, p1)) if len(np.unique(y_true)) == 2 else None,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': classification_report(y_true, y_hat, output_dict=True, zero_division=0),\n",
    "    }\n",
    "    # balanced accuracy (manual to avoid extra import)\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    out['balanced_accuracy'] = float((tnr + tpr) / 2.0)\n",
    "    out['bonafide_recall'] = float(tnr)\n",
    "    out['spoof_recall'] = float(tpr)\n",
    "    return out, y_hat\n",
    "\n",
    "\n",
    "def tune_logreg_with_speaker_val(X, y, df_meta, c_grid, class_weight_options, thresh_grid, val_speaker_count=3, seed=42):\n",
    "    b_train_speakers, b_val_speakers = split_B_speakers(df_meta, n_val_speakers=val_speaker_count, seed=seed)\n",
    "    is_b_train = df_meta['group'].eq('B') & df_meta['speaker_id'].astype(str).isin(b_train_speakers)\n",
    "    is_b_val = df_meta['group'].eq('B') & df_meta['speaker_id'].astype(str).isin(b_val_speakers)\n",
    "    is_c_test = df_meta['group'].eq('C')\n",
    "\n",
    "    X_btr, y_btr = X[is_b_train.to_numpy()], y[is_b_train.to_numpy()]\n",
    "    X_bval, y_bval = X[is_b_val.to_numpy()], y[is_b_val.to_numpy()]\n",
    "    X_B, y_B = X[df_meta['group'].eq('B').to_numpy()], y[df_meta['group'].eq('B').to_numpy()]\n",
    "    X_C, y_C = X[is_c_test.to_numpy()], y[is_c_test.to_numpy()]\n",
    "\n",
    "    best = None\n",
    "    tuning_rows = []\n",
    "    for cw in class_weight_options:\n",
    "        for cval in c_grid:\n",
    "            scaler = StandardScaler()\n",
    "            X_btr_s = scaler.fit_transform(X_btr)\n",
    "            X_bval_s = scaler.transform(X_bval)\n",
    "            clf = LogisticRegression(max_iter=2000, C=float(cval), class_weight=cw, random_state=42)\n",
    "            clf.fit(X_btr_s, y_btr)\n",
    "            p_bval = clf.predict_proba(X_bval_s)[:, 1]\n",
    "            for thr in thresh_grid:\n",
    "                m_val, _ = metrics_at_threshold(y_bval, p_bval, float(thr))\n",
    "                row = {\n",
    "                    'C': float(cval),\n",
    "                    'class_weight': str(cw),\n",
    "                    'threshold': float(thr),\n",
    "                    'val_balanced_accuracy': m_val['balanced_accuracy'],\n",
    "                    'val_accuracy': m_val['accuracy'],\n",
    "                    'val_auc': m_val['auc'],\n",
    "                    'val_bonafide_recall': m_val['bonafide_recall'],\n",
    "                    'val_spoof_recall': m_val['spoof_recall'],\n",
    "                }\n",
    "                tuning_rows.append(row)\n",
    "                key = (m_val['balanced_accuracy'], m_val['accuracy'], m_val['auc'] if m_val['auc'] is not None else -1.0)\n",
    "                if best is None or key > best['_key']:\n",
    "                    best = {'_key': key, **row}\n",
    "\n",
    "    # retrain final model on full B using selected hyperparameters\n",
    "    final_scaler = StandardScaler()\n",
    "    X_B_s = final_scaler.fit_transform(X_B)\n",
    "    X_C_s = final_scaler.transform(X_C)\n",
    "    final_clf = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        C=float(best['C']),\n",
    "        class_weight=(None if best['class_weight'] == 'None' else 'balanced'),\n",
    "        random_state=42,\n",
    "    )\n",
    "    final_clf.fit(X_B_s, y_B)\n",
    "    p_B = final_clf.predict_proba(X_B_s)[:, 1]\n",
    "    p_C = final_clf.predict_proba(X_C_s)[:, 1]\n",
    "\n",
    "    chosen_thr = float(best['threshold'])\n",
    "    m_B, yhat_B = metrics_at_threshold(y_B, p_B, chosen_thr)\n",
    "    m_C, yhat_C = metrics_at_threshold(y_C, p_C, chosen_thr)\n",
    "\n",
    "    # default threshold reference (0.5) on test C for comparison\n",
    "    m_C_default, yhat_C_default = metrics_at_threshold(y_C, p_C, 0.5)\n",
    "\n",
    "    result = {\n",
    "        'b_train_speakers': b_train_speakers,\n",
    "        'b_val_speakers': b_val_speakers,\n",
    "        'best_params': {k: best[k] for k in ['C','class_weight','threshold','val_balanced_accuracy','val_accuracy','val_auc','val_bonafide_recall','val_spoof_recall']},\n",
    "        'metrics_B_tuned_threshold': m_B,\n",
    "        'metrics_C_tuned_threshold': m_C,\n",
    "        'metrics_C_default_threshold_0_5': m_C_default,\n",
    "    }\n",
    "\n",
    "    masks = {\n",
    "        'is_B': df_meta['group'].eq('B').to_numpy(),\n",
    "        'is_C': df_meta['group'].eq('C').to_numpy(),\n",
    "    }\n",
    "    tuning_df = pd.DataFrame(tuning_rows).sort_values(['val_balanced_accuracy','val_accuracy','val_auc'], ascending=False)\n",
    "    return final_scaler, final_clf, chosen_thr, p_B, p_C, yhat_B, yhat_C, yhat_C_default, result, tuning_df, masks\n",
    "\n",
    "\n",
    "def plot_confmat(cm, title):\n",
    "    cm = np.array(cm)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels(['bonafide','spoof'], rotation=30, ha='right')\n",
    "    ax.set_yticklabels(['bonafide','spoof'])\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True'); ax.set_title(title)\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(int(v)), ha='center', va='center')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim_model = (\n",
    "    torch.hub.load(\n",
    "        \"IDRnD/ReDimNet\",\n",
    "        \"ReDimNet\",\n",
    "        model_name=\"b5\",\n",
    "        train_type=\"ptn\",\n",
    "        dataset=\"vox2\",\n",
    "    )\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "print(\"Loaded ReDimNet on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_df = load_manifest(MANIFEST_PATH, SUBSET_AUDIO_ROOT)\n",
    "bc_df = manifest_df[manifest_df['group'].isin(['B','C'])].copy().reset_index(drop=True)\n",
    "X_bc = extract_embeddings_for_df(bc_df[['utt_id','audio_path']], redim_model, DEVICE, CACHE_NPZ, force_recompute=FORCE_RECOMPUTE_EMBEDDINGS)\n",
    "y_bc = bc_df['label_id'].to_numpy().astype(int)\n",
    "\n",
    "scaler, clf, thr, p_B, p_C, yhat_B, yhat_C, yhat_C_default, results, tuning_df, masks = tune_logreg_with_speaker_val(\n",
    "    X_bc, y_bc, bc_df, C_GRID, CLASS_WEIGHT_OPTIONS, THRESH_GRID, val_speaker_count=VAL_SPEAKER_COUNT, seed=TUNE_SEED\n",
    ")\n",
    "print('Chosen threshold:', thr)\n",
    "print('Global test accuracy (C, tuned thr):', results['metrics_C_tuned_threshold']['accuracy'])\n",
    "print('Global test accuracy (C, thr=0.5):', results['metrics_C_default_threshold_0_5']['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_B = bc_df.loc[masks['is_B'], ['group','speaker_id','utt_id','label_str','label_id','system_id']].copy().reset_index(drop=True)\n",
    "pred_B['split'] = 'train_B'\n",
    "pred_B['prob_spoof'] = p_B\n",
    "pred_B['pred_label_id'] = yhat_B\n",
    "\n",
    "pred_C = bc_df.loc[masks['is_C'], ['group','speaker_id','utt_id','label_str','label_id','system_id']].copy().reset_index(drop=True)\n",
    "pred_C['split'] = 'test_C'\n",
    "pred_C['prob_spoof'] = p_C\n",
    "pred_C['pred_label_id'] = yhat_C\n",
    "\n",
    "pred_all = pd.concat([pred_B, pred_C], ignore_index=True)\n",
    "pred_all['pred_label_str'] = pred_all['pred_label_id'].map({0:'bonafide',1:'spoof'})\n",
    "\n",
    "rows = []\n",
    "for (split, spk), g in pred_all.groupby(['split','speaker_id']):\n",
    "    y_t = g['label_id'].to_numpy(); p = g['prob_spoof'].to_numpy(); y_h = g['pred_label_id'].to_numpy()\n",
    "    cm = confusion_matrix(y_t, y_h).tolist()\n",
    "    tn, fp = cm[0]; fn, tp = cm[1]\n",
    "    rows.append({\n",
    "        'split': split,\n",
    "        'group': g['group'].iloc[0],\n",
    "        'speaker_id': spk,\n",
    "        'n_samples': int(len(g)),\n",
    "        'n_bonafide': int((y_t==0).sum()),\n",
    "        'n_spoof': int((y_t==1).sum()),\n",
    "        'accuracy': float(accuracy_score(y_t, y_h)),\n",
    "        'auc': float(roc_auc_score(y_t, p)) if len(np.unique(y_t))==2 else np.nan,\n",
    "        'bonafide_recall': float(tn/(tn+fp)) if (tn+fp) else np.nan,\n",
    "        'spoof_recall': float(tp/(tp+fn)) if (tp+fn) else np.nan,\n",
    "        'fp_bonafide_as_spoof': int(fp),\n",
    "        'fn_spoof_as_bonafide': int(fn),\n",
    "    })\n",
    "per_user_df = pd.DataFrame(rows).sort_values(['split','accuracy'], ascending=[True, False])\n",
    "\n",
    "pred_all.to_csv(OUT_DIR / 'predictions_BC_all_users.csv', index=False)\n",
    "per_user_df.to_csv(OUT_DIR / 'per_user_metrics_BC.csv', index=False)\n",
    "per_user_df[per_user_df['split']=='test_C'].to_csv(OUT_DIR / 'per_user_metrics_C.csv', index=False)\n",
    "(OUT_DIR / 'run_summary.json').write_text(json.dumps(results, indent=2), encoding='utf-8')\n",
    "(OUT_DIR / 'tuning_results_top200.csv').write_text(tuning_df.head(200).to_csv(index=False), encoding='utf-8')\n",
    "with open(OUT_DIR / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(OUT_DIR / 'logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "print(per_user_df)\n",
    "print('Saved outputs ->', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrams: global B/C accuracy and per-user accuracy\n",
    "mB = results['metrics_B_tuned_threshold']; mC = results['metrics_C_tuned_threshold']; mC05 = results['metrics_C_default_threshold_0_5']\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "labels = ['Train B tuned', 'Test C @0.5', 'Test C tuned']\n",
    "vals = [mB['accuracy'], mC05['accuracy'], mC['accuracy']]\n",
    "ax.bar(labels, vals, color=['#4e79a7','#9c755f','#59a14f'])\n",
    "ax.set_ylim(0,1); ax.set_ylabel('Accuracy'); ax.set_title('Global Accuracy (Per-User Notebook)')\n",
    "for i,v in enumerate(vals): ax.text(i, v+0.02, f'{v:.3f}', ha='center', fontsize=9)\n",
    "plt.xticks(rotation=20, ha='right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "for split_name, color in [('train_B', '#4e79a7'), ('test_C', '#59a14f')]:\n",
    "    g = per_user_df[per_user_df['split']==split_name].sort_values('accuracy')\n",
    "    if g.empty: continue\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.barh(g['speaker_id'], g['accuracy'], color=color)\n",
    "    ax.set_xlim(0,1); ax.set_xlabel('Accuracy'); ax.set_title(f'Per-User Accuracy ({split_name})')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect weakest/strongest test users quickly\n",
    "c_users = per_user_df[per_user_df['split']=='test_C'].sort_values('accuracy')\n",
    "display(c_users)\n",
    "if len(c_users):\n",
    "    weak_spk = c_users.iloc[0]['speaker_id']\n",
    "    g = pred_all[(pred_all['split']=='test_C') & (pred_all['speaker_id']==weak_spk)]\n",
    "    print('Weakest test user:', weak_spk)\n",
    "    display(g.head())\n",
    "    cm = confusion_matrix(g['label_id'], g['pred_label_id']).tolist()\n",
    "    plot_confmat(cm, f'Confusion Matrix - {weak_spk} (test_C)')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}