{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "WAV_DIR: /home/SpeakerRec/BioVoice/data/wavs\n",
      "CONCEPTS_ROOT: /home/SpeakerRec/BioVoice/concept/new_concepts\n",
      "OUT_CSV: /home/SpeakerRec/BioVoice/output/tcav_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SpeakerRec/.cache/torch/hub/IDRnD_ReDimNet_master\n",
      "/tmp/ipykernel_1394958/167469207.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded head: /home/SpeakerRec/BioVoice/redimnet/tcav/output/redim_speaker_head_linear.pt\n",
      "Head in_dim: 192 num_classes: 3 l2_norm_emb: True\n",
      "wrapped_model ready.\n",
      "Eval wavs: 92\n",
      "[layer] stage4 -> Sequential\n",
      "[concept] constant_long_thick: loaded 1 patches, skipped 120\n",
      "[CAV] concept=constant_long_thick layer=stage4 acc=0.568 dim=2304\n",
      "[concept] constant_long_thin: loaded 1 patches, skipped 120\n",
      "[CAV] concept=constant_long_thin layer=stage4 acc=0.500 dim=2304\n",
      "[concept] constant_short_thick: loaded 1 patches, skipped 120\n",
      "[CAV] concept=constant_short_thick layer=stage4 acc=0.392 dim=2304\n",
      "[concept] constant_short_thin: loaded 1 patches, skipped 120\n",
      "[CAV] concept=constant_short_thin layer=stage4 acc=0.351 dim=2304\n",
      "Wrote: /home/SpeakerRec/BioVoice/output/tcav_results.csv rows: 368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>concept name</th>\n",
       "      <th>layer name</th>\n",
       "      <th>positive percentage</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>true label</th>\n",
       "      <th>predicted label</th>\n",
       "      <th>predicted probability</th>\n",
       "      <th>cav acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/wavs/eden_001.wav</td>\n",
       "      <td>constant_long_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>wavs</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.769236</td>\n",
       "      <td>0.567568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/wavs/eden_001.wav</td>\n",
       "      <td>constant_long_thin</td>\n",
       "      <td>stage4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>wavs</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.769236</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/wavs/eden_001.wav</td>\n",
       "      <td>constant_short_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>wavs</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.769236</td>\n",
       "      <td>0.391892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/wavs/eden_001.wav</td>\n",
       "      <td>constant_short_thin</td>\n",
       "      <td>stage4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>wavs</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.769236</td>\n",
       "      <td>0.351351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/SpeakerRec/BioVoice/data/wavs/eden_002.wav</td>\n",
       "      <td>constant_long_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>wavs</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.789764</td>\n",
       "      <td>0.567568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path          concept name  \\\n",
       "0  /home/SpeakerRec/BioVoice/data/wavs/eden_001.wav   constant_long_thick   \n",
       "1  /home/SpeakerRec/BioVoice/data/wavs/eden_001.wav    constant_long_thin   \n",
       "2  /home/SpeakerRec/BioVoice/data/wavs/eden_001.wav  constant_short_thick   \n",
       "3  /home/SpeakerRec/BioVoice/data/wavs/eden_001.wav   constant_short_thin   \n",
       "4  /home/SpeakerRec/BioVoice/data/wavs/eden_002.wav   constant_long_thick   \n",
       "\n",
       "  layer name  positive percentage  magnitude true label predicted label  \\\n",
       "0     stage4                100.0   0.000030       wavs            eden   \n",
       "1     stage4                100.0   0.000038       wavs            eden   \n",
       "2     stage4                100.0   0.000027       wavs            eden   \n",
       "3     stage4                100.0   0.000018       wavs            eden   \n",
       "4     stage4                100.0   0.000035       wavs            eden   \n",
       "\n",
       "   predicted probability   cav acc  \n",
       "0               0.769236  0.567568  \n",
       "1               0.769236  0.500000  \n",
       "2               0.769236  0.391892  \n",
       "3               0.769236  0.351351  \n",
       "4               0.789764  0.567568  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/SpeakerRec/BioVoice\")\n",
    "WAV_DIR = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "CONCEPTS_ROOT = PROJECT_ROOT / \"concept\" / \"new_concepts\"\n",
    "OUT_DIR = PROJECT_ROOT / \"output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / \"tcav_results.csv\"\n",
    "\n",
    "# Mel windowing\n",
    "N_MELS = 72\n",
    "TARGET_FRAMES = 192\n",
    "WINDOW_STRIDE = 24\n",
    "\n",
    "# CAV training\n",
    "SAMPLES_PER_WAV_FOR_CAV = 2\n",
    "MAX_WAVS_FOR_CAV = 400\n",
    "BATCH_SIZE_CAV = 32\n",
    "\n",
    "# Stamp\n",
    "STAMP_STRENGTH = 1.0      # scaled by background std\n",
    "MAX_ABS_DELTA = 4.0       # clamp\n",
    "\n",
    "# Layer keys you want to probe\n",
    "LAYER_KEYS =  [\"stem\",\"stage0\",\"stage1\",\"stage2\",\"stage3\",\"stage4\",\"stage5\"]\n",
    "\n",
    "EVAL_WAV_GLOB = str(WAV_DIR / \"**\" / \"*.wav\")\n",
    "\n",
    "def infer_true_label_from_path(wav_path: Path) -> str:\n",
    "    return wav_path.parent.name\n",
    "\n",
    "def id_to_name(i: int, id_to_speaker):\n",
    "    if isinstance(id_to_speaker, (list, tuple)):\n",
    "        return str(id_to_speaker[i])\n",
    "    return str(id_to_speaker[i])\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"WAV_DIR:\", WAV_DIR)\n",
    "print(\"CONCEPTS_ROOT:\", CONCEPTS_ROOT)\n",
    "print(\"OUT_CSV:\", OUT_CSV)\n",
    "\n",
    "# ----------------------------\n",
    "# Model + head + wrapper\n",
    "# ----------------------------\n",
    "redim_model = torch.hub.load(\n",
    "    \"IDRnD/ReDimNet\",\n",
    "    \"ReDimNet\",\n",
    "    model_name=\"b5\",\n",
    "    train_type=\"ptn\",\n",
    "    dataset=\"vox2\",\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "HEAD_PATH = Path.cwd() / \"output\" / \"redim_speaker_head_linear.pt\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "speaker_to_id: Dict[str, int] = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "l2_norm_emb = bool(ckpt.get(\"l2_norm_emb\", True))\n",
    "\n",
    "fc_w = ckpt[\"state_dict\"][\"fc.weight\"]\n",
    "in_dim = int(fc_w.shape[1])\n",
    "num_classes = int(fc_w.shape[0])\n",
    "\n",
    "print(\"Loaded head:\", HEAD_PATH)\n",
    "print(\"Head in_dim:\", in_dim, \"num_classes:\", num_classes, \"l2_norm_emb:\", l2_norm_emb)\n",
    "\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "head = SpeakerHead(in_dim=in_dim, num_classes=num_classes).to(DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n",
    "\n",
    "class ReDimNetMelLogitsWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  mel4d [B, 1, N_MELS, T]\n",
    "    Output: logits [B, num_speakers]\n",
    "    \"\"\"\n",
    "    def __init__(self, redim_model, head: nn.Module, l2_norm_emb: bool):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone\n",
    "        self.pool = redim_model.pool\n",
    "        self.bn = redim_model.bn\n",
    "        self.linear = redim_model.linear\n",
    "        self.head = head\n",
    "        self.l2_norm_emb = l2_norm_emb\n",
    "\n",
    "    def forward(self, mel4d: torch.Tensor) -> torch.Tensor:\n",
    "        # device-safe\n",
    "        mel4d = mel4d.to(next(self.parameters()).device).float()\n",
    "\n",
    "        x = self.backbone(mel4d)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn(x)\n",
    "        emb = self.linear(x)\n",
    "        if self.l2_norm_emb:\n",
    "            emb = emb / (emb.norm(p=2, dim=1, keepdim=True) + 1e-12)\n",
    "        return self.head(emb)\n",
    "\n",
    "wrapped_model = ReDimNetMelLogitsWrapper(redim_model, head, l2_norm_emb=l2_norm_emb).to(DEVICE).eval()\n",
    "print(\"wrapped_model ready.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Layer resolution + hooks\n",
    "# ----------------------------\n",
    "def resolve_layer_module(backbone: torch.nn.Module, layer_key: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Resolve a layer module from the *exact backbone instance used in forward*.\n",
    "    \"\"\"\n",
    "    bk = backbone\n",
    "\n",
    "    if hasattr(bk, layer_key):\n",
    "        return getattr(bk, layer_key)\n",
    "\n",
    "    if layer_key.startswith(\"stage\") and layer_key[5:].isdigit():\n",
    "        idx = int(layer_key[5:])\n",
    "        for attr in [\"stages\", \"stage\", \"blocks\", \"layers\"]:\n",
    "            if hasattr(bk, attr):\n",
    "                seq = getattr(bk, attr)\n",
    "                try:\n",
    "                    return seq[idx]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    candidates = []\n",
    "    for name, mod in bk.named_modules():\n",
    "        if name.endswith(layer_key) or (layer_key in name):\n",
    "            candidates.append((name, mod))\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: len(x[0]))\n",
    "        print(f\"[resolve_layer_module] Using fallback match: {candidates[0][0]}\")\n",
    "        return candidates[0][1]\n",
    "\n",
    "    raise ValueError(f\"Could not resolve layer_key={layer_key}\")\n",
    "def pool_activation_to_vec(act: torch.Tensor) -> torch.Tensor:\n",
    "    if act.ndim == 2:      # [B,C]\n",
    "        return act\n",
    "    if act.ndim == 3:      # [B,C,T]\n",
    "        return act.mean(dim=2)\n",
    "    if act.ndim == 4:      # [B,C,H,W]\n",
    "        return act.mean(dim=(2,3))\n",
    "    return act.flatten(start_dim=1)\n",
    "\n",
    "class LayerHook:\n",
    "    def __init__(self, layer: torch.nn.Module):\n",
    "        self.out = None\n",
    "        self.h = layer.register_forward_hook(self._hook)\n",
    "    def _hook(self, module, inp, out):\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            out = out[0]\n",
    "        self.out = out\n",
    "    def close(self):\n",
    "        self.h.remove()\n",
    "\n",
    "# ----------------------------\n",
    "# Mel cache (huge speedup)\n",
    "# ----------------------------\n",
    "class MelStore:\n",
    "    def __init__(self):\n",
    "        self.cache: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get(self, wav_path: Path) -> torch.Tensor:\n",
    "        k = str(wav_path)\n",
    "        if k in self.cache:\n",
    "            return self.cache[k]\n",
    "\n",
    "        wav, _sr = torchaudio.load(k)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav = wav.to(DEVICE).float()                # IMPORTANT: match spec device\n",
    "        mel3d = redim_model.spec(wav)               # [1,N_MELS,T] on DEVICE\n",
    "        mel2d = mel3d[0].detach().cpu().float()     # cache on CPU\n",
    "\n",
    "        if mel2d.shape[0] != N_MELS:\n",
    "            raise ValueError(f\"Expected N_MELS={N_MELS}, got {mel2d.shape[0]} for {wav_path}\")\n",
    "\n",
    "        self.cache[k] = mel2d\n",
    "        return mel2d\n",
    "\n",
    "mel_store = MelStore()\n",
    "\n",
    "# ----------------------------\n",
    "# Windows\n",
    "# ----------------------------\n",
    "def mel2d_to_windows(mel2d: torch.Tensor, target_frames: int, stride: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns windows4d: [B,1,N_MELS,target_frames] on CPU float32.\n",
    "    \"\"\"\n",
    "    n_mels, t = mel2d.shape\n",
    "    if t <= target_frames:\n",
    "        chunk = mel2d\n",
    "        if t < target_frames:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, target_frames - t), value=0.0)\n",
    "        return chunk.unsqueeze(0).unsqueeze(0).contiguous().float()\n",
    "\n",
    "    starts = list(range(0, t - target_frames + 1, stride))\n",
    "    wins = [mel2d[:, s:s+target_frames] for s in starts]\n",
    "    x = torch.stack(wins, dim=0)  # [B,N_MELS,T]\n",
    "    return x.unsqueeze(1).contiguous().float()  # [B,1,N_MELS,T]\n",
    "\n",
    "# ----------------------------\n",
    "# Concept patches (0..1 raw masks only)\n",
    "# ----------------------------\n",
    "def extract_nonzero_patch(mask2d: torch.Tensor, threshold: float = 1e-3) -> torch.Tensor:\n",
    "    m = mask2d > threshold\n",
    "    if not m.any():\n",
    "        raise ValueError(\"Mask has no nonzero region.\")\n",
    "    ys, xs = torch.where(m)\n",
    "    y0, y1 = int(ys.min()), int(ys.max()) + 1\n",
    "    x0, x1 = int(xs.min()), int(xs.max()) + 1\n",
    "    return mask2d[y0:y1, x0:x1].clone()\n",
    "\n",
    "def load_concept_patches(concept_dir: Path) -> List[torch.Tensor]:\n",
    "    npys = sorted(concept_dir.rglob(\"*.npy\"))\n",
    "    if not npys:\n",
    "        raise ValueError(f\"No .npy files found under {concept_dir}\")\n",
    "\n",
    "    patches: List[torch.Tensor] = []\n",
    "    skipped = 0\n",
    "\n",
    "    for p in npys:\n",
    "        arr = np.load(str(p))\n",
    "        if arr.ndim != 2:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        t = torch.from_numpy(arr).float()\n",
    "        if t.shape[0] != N_MELS:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        mn = float(t.min().item())\n",
    "        mx = float(t.max().item())\n",
    "        # accept only 0..1 masks (raw_energy / systematic raw)\n",
    "        if mn < -1e-3 or mx > 1.0 + 1e-3:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        try:\n",
    "            patches.append(extract_nonzero_patch(t, threshold=1e-3))\n",
    "        except Exception:\n",
    "            skipped += 1\n",
    "\n",
    "    if not patches:\n",
    "        raise ValueError(\n",
    "            f\"No usable 0..1 mask npys found under {concept_dir}. \"\n",
    "            f\"Ensure raw_energy/systematic raw masks are saved (0..1).\"\n",
    "        )\n",
    "    print(f\"[concept] {concept_dir.name}: loaded {len(patches)} patches, skipped {skipped}\")\n",
    "    return patches\n",
    "\n",
    "def list_concept_dirs(concepts_root: Path) -> List[Path]:\n",
    "    if (concepts_root / \"raw_energy.npy\").exists():\n",
    "        return [concepts_root]\n",
    "    dirs = [p for p in concepts_root.iterdir() if p.is_dir()]\n",
    "    dirs.sort()\n",
    "    return dirs\n",
    "\n",
    "# ----------------------------\n",
    "# In-distribution datasets\n",
    "# ----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class StampConfig:\n",
    "    strength: float = STAMP_STRENGTH\n",
    "    max_abs_delta: float = MAX_ABS_DELTA\n",
    "    seed: int = 0\n",
    "\n",
    "def random_crop_or_pad(mel2d: torch.Tensor, target_frames: int, rng: random.Random) -> torch.Tensor:\n",
    "    _n_mels, t = mel2d.shape\n",
    "    if t == target_frames:\n",
    "        return mel2d\n",
    "    if t > target_frames:\n",
    "        s = rng.randint(0, t - target_frames)\n",
    "        return mel2d[:, s:s+target_frames]\n",
    "    return torch.nn.functional.pad(mel2d, (0, target_frames - t), value=0.0)\n",
    "class BackgroundWindowDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Negative examples: real background windows.\n",
    "    Returns x3d: [1, N_MELS, T] on CPU.\n",
    "    DataLoader will batch into [B, 1, N_MELS, T] (correct).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav_paths: Sequence[Path],\n",
    "        target_frames: int,\n",
    "        samples_per_wav: int,\n",
    "        seed: int = 0,\n",
    "    ):\n",
    "        self.wav_paths = list(wav_paths)\n",
    "        self.target_frames = int(target_frames)\n",
    "        self.samples_per_wav = int(samples_per_wav)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for wi in range(len(self.wav_paths)):\n",
    "            for ri in range(self.samples_per_wav):\n",
    "                self.index.append((wi, ri))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        wi, ri = self.index[idx]\n",
    "        rng = random.Random(self.seed + idx * 1337 + ri)\n",
    "\n",
    "        mel2d = mel_store.get(self.wav_paths[wi])  # [N_MELS, Tfull] CPU\n",
    "        win2d = random_crop_or_pad(mel2d, self.target_frames, rng)  # [N_MELS, T]\n",
    "\n",
    "        return win2d.unsqueeze(0).float()  # [1, N_MELS, T]\n",
    "\n",
    "\n",
    "class StampedConceptDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Positive examples: background windows with a randomly placed patch stamp.\n",
    "    Returns x3d: [1, N_MELS, T] on CPU.\n",
    "    DataLoader will batch into [B, 1, N_MELS, T] (correct).\n",
    "    \"\"\"\n",
    "    def __init__(self, bg: BackgroundWindowDataset, patches: List[torch.Tensor], stamp: StampConfig):\n",
    "        self.bg = bg\n",
    "        self.patches = patches\n",
    "        self.stamp = stamp\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.bg)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        x3d = self.bg[idx].clone()  # [1, N_MELS, T]\n",
    "        rng = random.Random(self.stamp.seed + idx * 9176)\n",
    "\n",
    "        patch = self.patches[rng.randint(0, len(self.patches) - 1)]  # [H,W], 0..1\n",
    "        ph, pw = patch.shape\n",
    "        _, n_mels, t_frames = x3d.shape\n",
    "\n",
    "        if ph > n_mels or pw > t_frames:\n",
    "            raise ValueError(f\"Patch {patch.shape} too big for window [N_MELS={n_mels},T={t_frames}]\")\n",
    "\n",
    "        y0 = rng.randint(0, n_mels - ph)\n",
    "        x0 = rng.randint(0, t_frames - pw)\n",
    "\n",
    "        bg_std = float(x3d.std().clamp_min(1e-6))\n",
    "        delta = self.stamp.strength * bg_std\n",
    "        delta = float(max(-self.stamp.max_abs_delta, min(self.stamp.max_abs_delta, delta)))\n",
    "\n",
    "        # stamp onto the single channel (index 0)\n",
    "        x3d[0, y0:y0+ph, x0:x0+pw] += delta * patch.to(x3d.dtype)\n",
    "        return x3d.float()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CAV training (manual, robust)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def collect_layer_features(loader: DataLoader, layer: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Collect pooled activations for one layer.\n",
    "    Returns X [N,D] numpy float32.\n",
    "    \"\"\"\n",
    "    hook = LayerHook(layer)\n",
    "    X_list: List[np.ndarray] = []\n",
    "\n",
    "    wrapped_model.eval()\n",
    "    for x4d in loader:\n",
    "        x4d = x4d.to(DEVICE)  # [B,1,N_MELS,T]\n",
    "        hook.out = None\n",
    "        _ = wrapped_model(x4d)\n",
    "        act = hook.out\n",
    "        if act is None:\n",
    "            hook.close()\n",
    "            raise RuntimeError(\"Hook didn't capture activation. Check layer resolution.\")\n",
    "        vec = pool_activation_to_vec(act).detach().cpu().float().numpy()  # [B,D]\n",
    "        X_list.append(vec)\n",
    "\n",
    "    hook.close()\n",
    "    return np.concatenate(X_list, axis=0)\n",
    "\n",
    "def train_cav_logreg(X_pos: np.ndarray, X_neg: np.ndarray, seed: int) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Train linear classifier to separate pos vs neg.\n",
    "    Returns cav_dir [D] (normalized) and heldout acc.\n",
    "    \"\"\"\n",
    "    X = np.concatenate([X_neg, X_pos], axis=0)\n",
    "    y = np.concatenate([np.zeros(len(X_neg), dtype=np.int64), np.ones(len(X_pos), dtype=np.int64)], axis=0)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"liblinear\",\n",
    "        random_state=seed,\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    acc = float(clf.score(X_te, y_te))\n",
    "\n",
    "    w = clf.coef_[0].astype(np.float32)   # direction towards class=1\n",
    "    w = w / (np.linalg.norm(w) + 1e-9)\n",
    "    return w, acc\n",
    "\n",
    "# ----------------------------\n",
    "# Directional derivatives (TCAV-style) for a layer\n",
    "# ----------------------------\n",
    "def directional_derivatives(\n",
    "    windows4d_cpu: torch.Tensor,  # [B,1,N_MELS,T] CPU\n",
    "    target_idx: int,\n",
    "    layer: torch.nn.Module,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns g [B,D] where D matches pooled activation dim for that layer.\n",
    "    Computes grad wrt *hooked activation*, then pools grad the same way as activations.\n",
    "    This avoids pooled.grad == None issues.\n",
    "    \"\"\"\n",
    "    # Hard-fail if inference mode is enabled (enable_grad cannot override it)\n",
    "    if hasattr(torch, \"is_inference_mode_enabled\") and torch.is_inference_mode_enabled():\n",
    "        raise RuntimeError(\n",
    "            \"torch.inference_mode() is enabled; gradients are disabled. \"\n",
    "            \"Restart kernel and ensure you are not inside inference_mode.\"\n",
    "        )\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        hook = LayerHook(layer)\n",
    "        wrapped_model.eval()\n",
    "\n",
    "        x = windows4d_cpu.to(DEVICE).float()\n",
    "        x = x.detach()  # keep clean graph\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        wrapped_model.zero_grad(set_to_none=True)\n",
    "        hook.out = None\n",
    "\n",
    "        logits = wrapped_model(x)  # [B,C]\n",
    "        act = hook.out\n",
    "        if act is None:\n",
    "            hook.close()\n",
    "            raise RuntimeError(\"Hook didn't capture activation. Wrong layer module.\")\n",
    "\n",
    "        if isinstance(act, (tuple, list)):\n",
    "            act = act[0]\n",
    "\n",
    "        if not isinstance(act, torch.Tensor):\n",
    "            hook.close()\n",
    "            raise RuntimeError(f\"Hook output is not a Tensor: {type(act)}\")\n",
    "\n",
    "        if not act.requires_grad:\n",
    "            hook.close()\n",
    "            raise RuntimeError(\n",
    "                \"Hooked activation does not require grad. \"\n",
    "                \"This usually means you're in inference_mode() or grad is globally disabled.\"\n",
    "            )\n",
    "\n",
    "        score = logits[:, target_idx].sum()\n",
    "\n",
    "        grad_act = torch.autograd.grad(\n",
    "            outputs=score,\n",
    "            inputs=act,\n",
    "            retain_graph=False,\n",
    "            create_graph=False,\n",
    "            allow_unused=False,\n",
    "        )[0]\n",
    "\n",
    "        hook.close()\n",
    "\n",
    "        # Pool grad to match the CAV feature space (same pooling as activations)\n",
    "        g = pool_activation_to_vec(grad_act)  # [B,D]\n",
    "        return g.detach().cpu().float()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_over_windows(windows4d_cpu: torch.Tensor) -> Tuple[int, str, float]:\n",
    "    \"\"\"\n",
    "    windows4d_cpu: [B,1,N_MELS,T] CPU\n",
    "    Returns (pred_id, pred_name, pred_prob) using mean softmax over windows.\n",
    "    \"\"\"\n",
    "    x = windows4d_cpu.to(DEVICE)\n",
    "    logits = wrapped_model(x)                 # [B,C]\n",
    "    probs = torch.softmax(logits, dim=1)      # [B,C]\n",
    "    mean_probs = probs.mean(dim=0)            # [C]\n",
    "    pred_id = int(mean_probs.argmax().item())\n",
    "    pred_prob = float(mean_probs[pred_id].item())\n",
    "    pred_name = id_to_name(pred_id, id_to_speaker)\n",
    "    return pred_id, pred_name, pred_prob\n",
    "\n",
    "# ----------------------------\n",
    "# Main runner\n",
    "# ----------------------------\n",
    "def run_tcav_style_to_csv(\n",
    "    concepts_root: Path,\n",
    "    eval_wav_paths: List[Path],\n",
    "    layer_keys: List[str],\n",
    "    *,\n",
    "    target_mode: str = \"pred\",  # \"pred\" or \"true\"\n",
    "    seed: int = 123,\n",
    "):\n",
    "    # Pick wavs for CAV training (generic backgrounds)\n",
    "    cav_wavs = eval_wav_paths.copy()\n",
    "    random.Random(seed).shuffle(cav_wavs)\n",
    "    if MAX_WAVS_FOR_CAV is not None:\n",
    "        cav_wavs = cav_wavs[:MAX_WAVS_FOR_CAV]\n",
    "\n",
    "    bg_ds = BackgroundWindowDataset(\n",
    "        wav_paths=cav_wavs,\n",
    "        target_frames=TARGET_FRAMES,\n",
    "        samples_per_wav=SAMPLES_PER_WAV_FOR_CAV,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    concept_dirs = list_concept_dirs(concepts_root)\n",
    "    if not concept_dirs:\n",
    "        raise ValueError(f\"No concept dirs found under {concepts_root}\")\n",
    "\n",
    "    # Resolve layer modules once\n",
    "    layers: Dict[str, torch.nn.Module] = {}\n",
    "    for lk in layer_keys:\n",
    "        layers[lk] = resolve_layer_module(wrapped_model.backbone, lk)\n",
    "\n",
    "        print(f\"[layer] {lk} -> {layers[lk].__class__.__name__}\")\n",
    "\n",
    "    # Train CAVs: (concept, layer) -> (cav_dir, cav_acc)\n",
    "    cav_dir: Dict[Tuple[str, str], np.ndarray] = {}\n",
    "    cav_acc: Dict[Tuple[str, str], float] = {}\n",
    "\n",
    "    neg_loader = DataLoader(bg_ds, batch_size=BATCH_SIZE_CAV, shuffle=True, num_workers=0)\n",
    "\n",
    "    for cdir in concept_dirs:\n",
    "        cname = cdir.name\n",
    "        patches = load_concept_patches(cdir)\n",
    "\n",
    "        pos_ds = StampedConceptDataset(\n",
    "            bg=bg_ds,\n",
    "            patches=patches,\n",
    "            stamp=StampConfig(strength=STAMP_STRENGTH, max_abs_delta=MAX_ABS_DELTA, seed=seed),\n",
    "        )\n",
    "        pos_loader = DataLoader(pos_ds, batch_size=BATCH_SIZE_CAV, shuffle=True, num_workers=0)\n",
    "\n",
    "        for lk, layer_mod in layers.items():\n",
    "            X_neg = collect_layer_features(neg_loader, layer_mod)\n",
    "            X_pos = collect_layer_features(pos_loader, layer_mod)\n",
    "\n",
    "            w, acc = train_cav_logreg(X_pos=X_pos, X_neg=X_neg, seed=seed)\n",
    "            cav_dir[(cname, lk)] = w\n",
    "            cav_acc[(cname, lk)] = acc\n",
    "            print(f\"[CAV] concept={cname} layer={lk} acc={acc:.3f} dim={w.shape[0]}\")\n",
    "\n",
    "    # Evaluate wavs\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    for wav_path in eval_wav_paths:\n",
    "        true_label = infer_true_label_from_path(wav_path)\n",
    "\n",
    "        mel2d = mel_store.get(wav_path)  # cached CPU mel\n",
    "        windows4d = mel2d_to_windows(mel2d, TARGET_FRAMES, WINDOW_STRIDE)  # [B,1,N_MELS,T] CPU\n",
    "\n",
    "        pred_id, pred_name, pred_prob = predict_over_windows(windows4d)\n",
    "\n",
    "        if target_mode == \"pred\":\n",
    "            target_idx = pred_id\n",
    "        elif target_mode == \"true\":\n",
    "            if true_label not in speaker_to_id:\n",
    "                # skip if label not in head mapping\n",
    "                continue\n",
    "            target_idx = int(speaker_to_id[true_label])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown target_mode={target_mode}\")\n",
    "\n",
    "        # For each layer compute grads once (g [B,D])\n",
    "        grads_by_layer: Dict[str, torch.Tensor] = {}\n",
    "        for lk, layer_mod in layers.items():\n",
    "            grads_by_layer[lk] = directional_derivatives(windows4d, target_idx=target_idx, layer=layer_mod)\n",
    "\n",
    "        # For each concept/layer compute dd and metrics\n",
    "        for cdir in concept_dirs:\n",
    "            cname = cdir.name\n",
    "            for lk in layer_keys:\n",
    "                g = grads_by_layer[lk]  # [B,D]\n",
    "                w = torch.from_numpy(cav_dir[(cname, lk)]).float()  # [D]\n",
    "                dd = (g * w.unsqueeze(0)).sum(dim=1)  # [B]\n",
    "\n",
    "                pos_pct = float((dd > 0).float().mean().item() * 100.0)\n",
    "                magnitude = float(dd.abs().mean().item())\n",
    "\n",
    "                rows.append({\n",
    "                    \"path\": str(wav_path),\n",
    "                    \"concept name\": cname,\n",
    "                    \"layer name\": lk,\n",
    "                    \"positive percentage\": pos_pct,\n",
    "                    \"magnitude\": magnitude,\n",
    "                    \"true label\": str(true_label),\n",
    "                    \"predicted label\": str(pred_name),\n",
    "                    \"predicted probability\": float(pred_prob),\n",
    "                    \"cav acc\": float(cav_acc[(cname, lk)]),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"path\",\n",
    "        \"concept name\",\n",
    "        \"layer name\",\n",
    "        \"positive percentage\",\n",
    "        \"magnitude\",\n",
    "        \"true label\",\n",
    "        \"predicted label\",\n",
    "        \"predicted probability\",\n",
    "        \"cav acc\",\n",
    "    ])\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Wrote:\", OUT_CSV, \"rows:\", len(df))\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "eval_wav_paths = [Path(p) for p in glob.glob(EVAL_WAV_GLOB, recursive=True)]\n",
    "eval_wav_paths.sort()\n",
    "print(\"Eval wavs:\", len(eval_wav_paths))\n",
    "\n",
    "df = run_tcav_style_to_csv(\n",
    "    concepts_root=CONCEPTS_ROOT,\n",
    "    eval_wav_paths=eval_wav_paths,\n",
    "    layer_keys=LAYER_KEYS,\n",
    "    target_mode=\"pred\",  # change to \"true\" to explain ground-truth logit (requires speaker_to_id coverage)\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL shape (72, 183) min/max -8.323339462280273 10.122404098510742 p1/p50/p99 [-7.13723767 -0.20183468  7.66903698]\n",
      "CON  shape (72, 304) min/max -1.6737136840820312 11.715995788574219 p1/p50/p99 [-1.67371368e+00  9.53674316e-07  9.53674316e-07]\n",
      "REAL per-mel mean abs: 3.2983405162667623e-07\n",
      "CON  per-mel mean abs: 9.313661166743259e-07\n"
     ]
    }
   ],
   "source": [
    "# pick 1 real wav\n",
    "wav = next(WAV_FOLDER.glob(\"*.wav\"))\n",
    "mel_real = redim_model.spec(torchaudio.load(str(wav))[0][:1].float())  # (1,N,T)\n",
    "mel_real = mel_real[0].cpu().numpy()\n",
    "\n",
    "# pick 1 concept npy (a numbered file, not raw_energy)\n",
    "cfile = next((CONCEPT_ROOT / concept_names[0]).glob(\"*.npy\"))\n",
    "mel_con = np.load(cfile)\n",
    "\n",
    "def stats(name, x):\n",
    "    print(name, \"shape\", x.shape,\n",
    "          \"min/max\", float(x.min()), float(x.max()),\n",
    "          \"p1/p50/p99\", np.percentile(x, [1,50,99]))\n",
    "\n",
    "stats(\"REAL\", mel_real)\n",
    "stats(\"CON \", mel_con)\n",
    "\n",
    "# also check if spec already has ~zero mean per mel bin\n",
    "print(\"REAL per-mel mean abs:\", float(np.mean(np.abs(mel_real.mean(axis=1)))))\n",
    "print(\"CON  per-mel mean abs:\", float(np.mean(np.abs(mel_con.mean(axis=1)))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
