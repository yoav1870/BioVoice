{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASVspoof5 Train-Only Probe (Tuned) ? Bonafide vs Each Spoof System (A01-A32), train+val -> test\n",
    "\n",
    "For each target system `A01..A08`:\n",
    "- task = `bonafide (0)` vs `target_system spoof (1)`\n",
    "- tune on a speaker-disjoint `train/val` split\n",
    "- retrain on full `B` using chosen hyperparameters\n",
    "- evaluate on `C`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROJECT_ROOT = Path(\"/home/SpeakerRec/BioVoice\")\n",
    "MANIFEST_PATH = PROJECT_ROOT / \"redimnet\" / \"logistic_regression\" / \"asvspoof5\" / \"100_speakers\" / \"asvspoof5_100_speakers_selected_utterances_plan.csv\"\n",
    "SUBSET_AUDIO_ROOT = PROJECT_ROOT / \"data\" / \"datasets\" / \"asvspoof5_100_speakers_32_real_48_spoof\"\n",
    "EMBED_CACHE_DIR = PROJECT_ROOT / \"data\" / \"embeddings\" / \"asvspoof5_100_speakers_32_real_48_spoof\"\n",
    "EMBED_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tuning config (speaker-disjoint split inside group train)\n",
    "VAL_SPEAKER_COUNT = 0      # unused when manifest already provides 'val' group\n",
    "TUNE_SEED = 42\n",
    "C_GRID = [0.01, 0.1, 1.0, 10.0]\n",
    "CLASS_WEIGHT_OPTIONS = [None, 'balanced']\n",
    "THRESH_GRID = np.linspace(0.05, 0.95, 181)\n",
    "FORCE_RECOMPUTE_EMBEDDINGS = False\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "print(\"SUBSET_AUDIO_ROOT:\", SUBSET_AUDIO_ROOT)\n",
    "\n",
    "SYSTEM_IDS = [f'A{i:02d}' for i in range(1, 33)]\n",
    "OUT_DIR = PROJECT_ROOT / 'data' / 'models' / 'asvspoof5_100_speakers_probe_train_val_test_all_systems_vs_bonafide_tuned'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"bonafide\": 0, \"spoof\": 1}\n",
    "\n",
    "\n",
    "def build_audio_path(row, subset_root: Path) -> Path:\n",
    "    return subset_root / str(row[\"group\"]) / str(row[\"label\"]) / f\"{row['utt_id']}.flac\"\n",
    "\n",
    "\n",
    "def load_manifest(manifest_path: Path, subset_root: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(manifest_path).copy()\n",
    "    req = {\"group\",\"speaker_id\",\"utt_id\",\"label\",\"system_id\"}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Manifest missing columns: {sorted(missing)}\")\n",
    "    df[\"label_str\"] = df[\"label\"].astype(str)\n",
    "    df[\"label_id\"] = df[\"label_str\"].map(LABEL_MAP).astype(int)\n",
    "    df[\"audio_path\"] = df.apply(lambda r: str(build_audio_path(r, subset_root)), axis=1)\n",
    "    df[\"audio_exists\"] = df[\"audio_path\"].map(lambda p: Path(p).exists())\n",
    "    return df\n",
    "\n",
    "\n",
    "def embed_with_redim(model, wav_path: str, device: str) -> np.ndarray:\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav[:1, :]\n",
    "    wav = wav.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(wav)\n",
    "    return emb.squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_embeddings_for_df(df_paths: pd.DataFrame, model, device: str, cache_npz: Path, force_recompute: bool=False):\n",
    "    if cache_npz.exists() and not force_recompute:\n",
    "        payload = np.load(cache_npz, allow_pickle=True)\n",
    "        X = payload['X']\n",
    "        utt_ids = payload['utt_ids'].astype(str)\n",
    "        lut = pd.DataFrame({'utt_id': utt_ids, '_idx': np.arange(len(utt_ids))})\n",
    "        m = df_paths[['utt_id']].merge(lut, on='utt_id', how='left', validate='one_to_one')\n",
    "        if m['_idx'].isna().any():\n",
    "            miss = m.loc[m['_idx'].isna(), 'utt_id'].tolist()[:10]\n",
    "            raise RuntimeError(f\"Embedding cache missing utt_ids, examples: {miss}\")\n",
    "        return X[m['_idx'].astype(int).to_numpy()]\n",
    "\n",
    "    vecs, ids = [], []\n",
    "    for rec in tqdm(df_paths.to_dict('records'), desc=f\"Embedding {len(df_paths)}\"):\n",
    "        p = Path(rec['audio_path'])\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing audio: {p}\")\n",
    "        vecs.append(embed_with_redim(model, str(p), device))\n",
    "        ids.append(str(rec['utt_id']))\n",
    "    X = np.stack(vecs).astype(np.float32)\n",
    "    np.savez_compressed(cache_npz, X=X, utt_ids=np.array(ids, dtype=object))\n",
    "    return X\n",
    "\n",
    "\n",
    "def split_B_speakers(df: pd.DataFrame, n_val_speakers=3, seed=42):\n",
    "    # Uses explicit pre-defined groups in manifest: train/val/test\n",
    "    train_speakers = sorted(df.loc[df['group'].eq('train'), 'speaker_id'].astype(str).unique().tolist())\n",
    "    val_speakers = sorted(df.loc[df['group'].eq('val'), 'speaker_id'].astype(str).unique().tolist())\n",
    "    if len(train_speakers) == 0 or len(val_speakers) == 0:\n",
    "        raise ValueError('Expected non-empty train and val speaker groups in manifest')\n",
    "    return train_speakers, val_speakers\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, p1, thr):\n",
    "    y_hat = (p1 >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_hat).tolist()\n",
    "    out = {\n",
    "        'threshold': float(thr),\n",
    "        'accuracy': float(accuracy_score(y_true, y_hat)),\n",
    "        'auc': float(roc_auc_score(y_true, p1)) if len(np.unique(y_true)) == 2 else None,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': classification_report(y_true, y_hat, output_dict=True, zero_division=0),\n",
    "    }\n",
    "    # balanced accuracy (manual to avoid extra import)\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    out['balanced_accuracy'] = float((tnr + tpr) / 2.0)\n",
    "    out['bonafide_recall'] = float(tnr)\n",
    "    out['spoof_recall'] = float(tpr)\n",
    "    return out, y_hat\n",
    "\n",
    "\n",
    "def tune_logreg_with_speaker_val(X, y, df_meta, c_grid, class_weight_options, thresh_grid, val_speaker_count=3, seed=42):\n",
    "    b_train_speakers, b_val_speakers = split_B_speakers(df_meta, n_val_speakers=val_speaker_count, seed=seed)\n",
    "    is_b_train = df_meta['group'].eq('train')\n",
    "    is_b_val = df_meta['group'].eq('val')\n",
    "    is_c_test = df_meta['group'].eq('test')\n",
    "\n",
    "    if not (is_b_train.any() and is_b_val.any() and is_c_test.any()):\n",
    "        raise ValueError('Expected train/val/test groups in task dataframe')\n",
    "\n",
    "    X_btr, y_btr = X[is_b_train.to_numpy()], y[is_b_train.to_numpy()]\n",
    "    X_bval, y_bval = X[is_b_val.to_numpy()], y[is_b_val.to_numpy()]\n",
    "    is_train_or_val = df_meta['group'].isin(['train', 'val'])\n",
    "    X_B, y_B = X[is_train_or_val.to_numpy()], y[is_train_or_val.to_numpy()]\n",
    "    X_C, y_C = X[is_c_test.to_numpy()], y[is_c_test.to_numpy()]\n",
    "\n",
    "    best = None\n",
    "    tuning_rows = []\n",
    "    for cw in class_weight_options:\n",
    "        for cval in c_grid:\n",
    "            scaler = StandardScaler()\n",
    "            X_btr_s = scaler.fit_transform(X_btr)\n",
    "            X_bval_s = scaler.transform(X_bval)\n",
    "            clf = LogisticRegression(max_iter=2000, C=float(cval), class_weight=cw, random_state=42)\n",
    "            clf.fit(X_btr_s, y_btr)\n",
    "            p_bval = clf.predict_proba(X_bval_s)[:, 1]\n",
    "            for thr in thresh_grid:\n",
    "                m_val, _ = metrics_at_threshold(y_bval, p_bval, float(thr))\n",
    "                row = {\n",
    "                    'C': float(cval),\n",
    "                    'class_weight': str(cw),\n",
    "                    'threshold': float(thr),\n",
    "                    'val_balanced_accuracy': m_val['balanced_accuracy'],\n",
    "                    'val_accuracy': m_val['accuracy'],\n",
    "                    'val_auc': m_val['auc'],\n",
    "                    'val_bonafide_recall': m_val['bonafide_recall'],\n",
    "                    'val_spoof_recall': m_val['spoof_recall'],\n",
    "                }\n",
    "                tuning_rows.append(row)\n",
    "                key = (m_val['balanced_accuracy'], m_val['accuracy'], m_val['auc'] if m_val['auc'] is not None else -1.0)\n",
    "                if best is None or key > best['_key']:\n",
    "                    best = {'_key': key, **row}\n",
    "\n",
    "    # retrain final model on train+val using selected hyperparameters\n",
    "    final_scaler = StandardScaler()\n",
    "    X_B_s = final_scaler.fit_transform(X_B)\n",
    "    X_C_s = final_scaler.transform(X_C)\n",
    "    final_clf = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        C=float(best['C']),\n",
    "        class_weight=(None if best['class_weight'] == 'None' else 'balanced'),\n",
    "        random_state=42,\n",
    "    )\n",
    "    final_clf.fit(X_B_s, y_B)\n",
    "    p_B = final_clf.predict_proba(X_B_s)[:, 1]\n",
    "    p_C = final_clf.predict_proba(X_C_s)[:, 1]\n",
    "\n",
    "    chosen_thr = float(best['threshold'])\n",
    "    m_B, yhat_B = metrics_at_threshold(y_B, p_B, chosen_thr)\n",
    "    m_C, yhat_C = metrics_at_threshold(y_C, p_C, chosen_thr)\n",
    "\n",
    "    # default threshold reference (0.5) on test for comparison\n",
    "    m_C_default, yhat_C_default = metrics_at_threshold(y_C, p_C, 0.5)\n",
    "\n",
    "    result = {\n",
    "        'b_train_speakers': b_train_speakers,\n",
    "        'b_val_speakers': b_val_speakers,\n",
    "        'best_params': {k: best[k] for k in ['C','class_weight','threshold','val_balanced_accuracy','val_accuracy','val_auc','val_bonafide_recall','val_spoof_recall']},\n",
    "        'metrics_train_val_tuned_threshold': m_B,\n",
    "        'metrics_test_tuned_threshold': m_C,\n",
    "        'metrics_test_default_threshold_0_5': m_C_default,\n",
    "    }\n",
    "\n",
    "    masks = {\n",
    "        'is_B': df_meta['group'].isin(['train', 'val']).to_numpy(),\n",
    "        'is_C': df_meta['group'].eq('test').to_numpy(),\n",
    "    }\n",
    "    tuning_df = pd.DataFrame(tuning_rows).sort_values(['val_balanced_accuracy','val_accuracy','val_auc'], ascending=False)\n",
    "    return final_scaler, final_clf, chosen_thr, p_B, p_C, yhat_B, yhat_C, yhat_C_default, result, tuning_df, masks\n",
    "\n",
    "\n",
    "\n",
    "def plot_confmat(cm, title):\n",
    "    cm = np.array(cm)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels(['bonafide','spoof'], rotation=30, ha='right')\n",
    "    ax.set_yticklabels(['bonafide','spoof'])\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True'); ax.set_title(title)\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(int(v)), ha='center', va='center')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim_model = (\n",
    "    torch.hub.load(\n",
    "        \"IDRnD/ReDimNet\",\n",
    "        \"ReDimNet\",\n",
    "        model_name=\"b5\",\n",
    "        train_type=\"ptn\",\n",
    "        dataset=\"vox2\",\n",
    "    )\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "print(\"Loaded ReDimNet on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_df = load_manifest(MANIFEST_PATH, SUBSET_AUDIO_ROOT)\n",
    "bc_df = manifest_df[manifest_df['group'].isin(['train','val','test'])].copy().reset_index(drop=True)\n",
    "print('B/C rows:', len(bc_df))\n",
    "print('Missing audio:', int((~bc_df['audio_exists']).sum()))\n",
    "print('Systems in spoof rows:', sorted(bc_df.loc[bc_df['label_str']=='spoof','system_id'].unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "all_pred_parts = []\n",
    "\n",
    "for target_system in SYSTEM_IDS:\n",
    "    print(f'\\n=== {target_system} vs bonafide ===')\n",
    "    task_df = bc_df[(bc_df['label_str'].eq('bonafide')) | ((bc_df['label_str'].eq('spoof')) & (bc_df['system_id'].eq(target_system)))].copy().reset_index(drop=True)\n",
    "    task_df['task_label_id'] = np.where(task_df['label_str'].eq('bonafide'), 0, 1).astype(int)\n",
    "    print(task_df.groupby(['group','label_str','system_id']).size())\n",
    "\n",
    "    cache_npz = EMBED_CACHE_DIR / f'embeddings_train_val_test_{target_system}_vs_bonafide.npz'\n",
    "    X_task = extract_embeddings_for_df(task_df[['utt_id','audio_path']], redim_model, DEVICE, cache_npz, force_recompute=FORCE_RECOMPUTE_EMBEDDINGS)\n",
    "    y_task = task_df['task_label_id'].to_numpy().astype(int)\n",
    "\n",
    "    scaler, clf, thr, p_B, p_C, yhat_B, yhat_C, yhat_C_default, results, tuning_df, masks = tune_logreg_with_speaker_val(\n",
    "        X_task, y_task, task_df.rename(columns={'task_label_id':'label_id'}), C_GRID, CLASS_WEIGHT_OPTIONS, THRESH_GRID,\n",
    "        val_speaker_count=VAL_SPEAKER_COUNT, seed=TUNE_SEED\n",
    "    )\n",
    "\n",
    "    sys_dir = OUT_DIR / target_system\n",
    "    sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(sys_dir / 'scaler.pkl', 'wb') as f: pickle.dump(scaler, f)\n",
    "    with open(sys_dir / 'logistic_regression.pkl', 'wb') as f: pickle.dump(clf, f)\n",
    "    (sys_dir / 'run_summary.json').write_text(json.dumps(results, indent=2), encoding='utf-8')\n",
    "    (sys_dir / 'tuning_results_top200.csv').write_text(tuning_df.head(200).to_csv(index=False), encoding='utf-8')\n",
    "\n",
    "    pred_B = task_df.loc[masks['is_B'], ['group','speaker_id','utt_id','label_str','system_id']].copy().reset_index(drop=True)\n",
    "    pred_B['split'] = 'train_val'; pred_B['task_label_id'] = 0\n",
    "    pred_B.loc[pred_B['label_str'].eq('spoof'),'task_label_id'] = 1\n",
    "    pred_B['prob_target_spoof'] = p_B; pred_B['pred_task_label_id_tuned'] = yhat_B; pred_B['pred_task_label_id_thr_0_5'] = (p_B>=0.5).astype(int)\n",
    "\n",
    "    pred_C = task_df.loc[masks['is_C'], ['group','speaker_id','utt_id','label_str','system_id']].copy().reset_index(drop=True)\n",
    "    pred_C['split'] = 'test'; pred_C['task_label_id'] = 0\n",
    "    pred_C.loc[pred_C['label_str'].eq('spoof'),'task_label_id'] = 1\n",
    "    pred_C['prob_target_spoof'] = p_C; pred_C['pred_task_label_id_tuned'] = yhat_C; pred_C['pred_task_label_id_thr_0_5'] = yhat_C_default\n",
    "\n",
    "    pred_df = pd.concat([pred_B, pred_C], ignore_index=True)\n",
    "    pred_df['target_system'] = target_system\n",
    "    pred_df.to_csv(sys_dir / f'predictions_{target_system}_vs_bonafide.csv', index=False)\n",
    "    all_pred_parts.append(pred_df)\n",
    "\n",
    "    mB = results['metrics_train_val_tuned_threshold']; mC = results['metrics_test_tuned_threshold']; mC05 = results['metrics_test_default_threshold_0_5']\n",
    "    summary_rows.append({\n",
    "        'target_system': target_system,\n",
    "        'chosen_C': results['best_params']['C'],\n",
    "        'chosen_class_weight': results['best_params']['class_weight'],\n",
    "        'chosen_threshold': results['best_params']['threshold'],\n",
    "        'val_balanced_accuracy': results['best_params']['val_balanced_accuracy'],\n",
    "        'train_accuracy_tuned': mB['accuracy'],\n",
    "        'test_accuracy_thr_0_5': mC05['accuracy'],\n",
    "        'test_accuracy_tuned': mC['accuracy'],\n",
    "        'test_auc': mC['auc'],\n",
    "        'test_bonafide_recall_tuned': mC['bonafide_recall'],\n",
    "        'test_spoof_recall_tuned': mC['spoof_recall'],\n",
    "        'test_cm_00': mC['confusion_matrix'][0][0],\n",
    "        'test_cm_01': mC['confusion_matrix'][0][1],\n",
    "        'test_cm_10': mC['confusion_matrix'][1][0],\n",
    "        'test_cm_11': mC['confusion_matrix'][1][1],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values('target_system')\n",
    "summary_df.to_csv(OUT_DIR / 'all_systems_metrics_summary.csv', index=False)\n",
    "if all_pred_parts:\n",
    "    pd.concat(all_pred_parts, ignore_index=True).to_csv(OUT_DIR / 'all_systems_predictions_combined.csv', index=False)\n",
    "print('Saved summary ->', OUT_DIR / 'all_systems_metrics_summary.csv')\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrams: train/test accuracy by system + test AUC by system\n",
    "summary_df = pd.read_csv(OUT_DIR / 'all_systems_metrics_summary.csv')\n",
    "\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.25\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.bar(x - width, summary_df['train_accuracy_tuned'], width, label='Train+Val tuned')\n",
    "ax.bar(x, summary_df['test_accuracy_thr_0_5'], width, label='Test @0.5')\n",
    "ax.bar(x + width, summary_df['test_accuracy_tuned'], width, label='Test tuned')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df['target_system'])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Bonafide vs System Accuracy by Target System')\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(summary_df['target_system'], summary_df['test_auc'], marker='o', label='Test AUC')\n",
    "ax.plot(summary_df['target_system'], summary_df['test_bonafide_recall_tuned'], marker='s', label='Bonafide recall (tuned)')\n",
    "ax.plot(summary_df['target_system'], summary_df['test_spoof_recall_tuned'], marker='^', label='Spoof recall (tuned)')\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Test Metrics by Target System')\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}