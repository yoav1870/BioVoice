{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /home/SpeakerRec/BioVoice\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2162b0cd90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Paths / Device ----------\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SpeakerRec/.cache/torch/hub/IDRnD_ReDimNet_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ReDimNet successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load ReDimNet ----------\n",
    "redim_model = (\n",
    "    torch.hub.load(\n",
    "        \"IDRnD/ReDimNet\",\n",
    "        \"ReDimNet\",\n",
    "        model_name=\"b5\",\n",
    "        train_type=\"ptn\",\n",
    "        dataset=\"vox2\",\n",
    "    )\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n",
    "print(\"Loaded ReDimNet successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReDimNet spec N_MELS = 72\n"
     ]
    }
   ],
   "source": [
    "# ---------- Infer N_MELS from model.spec ----------\n",
    "with torch.no_grad():\n",
    "    dummy_wav = torch.zeros(1, 16000, device=DEVICE)\n",
    "    dummy_mel = redim_model.spec(dummy_wav)  # (1, N_MELS, T)\n",
    "N_MELS = int(dummy_mel.shape[1])\n",
    "print(\"ReDimNet spec N_MELS =\", N_MELS)  # you expect 72\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded speaker head from: /home/SpeakerRec/BioVoice/redimnet/tcav/output/redim_speaker_head_linear.pt\n",
      "Speakers: ['eden', 'idan', 'yoav']\n",
      "L2-normalized embeddings: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3520606/3246021292.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load trained speaker head ----------\n",
    "HEAD_PATH = Path.cwd() / \"output\" / \"redim_speaker_head_linear.pt\"\n",
    "assert HEAD_PATH.exists(), f\"Missing head checkpoint: {HEAD_PATH}\"\n",
    "\n",
    "ckpt = torch.load(HEAD_PATH, map_location=DEVICE)\n",
    "speaker_to_id = ckpt[\"speaker_to_id\"]\n",
    "id_to_speaker = ckpt[\"id_to_speaker\"]\n",
    "SPEAKERS = list(speaker_to_id.keys())\n",
    "\n",
    "print(\"Loaded speaker head from:\", HEAD_PATH)\n",
    "print(\"Speakers:\", SPEAKERS)\n",
    "print(\"L2-normalized embeddings:\", ckpt.get(\"l2_norm_emb\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerHead(\n",
       "  (fc): Linear(in_features=192, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "# IMPORTANT: in_dim must match ReDimNet linear output dim.\n",
    "# In your snippet you used 192; keep it consistent with your trained head.\n",
    "head = SpeakerHead(in_dim=192, num_classes=len(SPEAKERS)).to(DEVICE)\n",
    "head.load_state_dict(ckpt[\"state_dict\"])\n",
    "head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper (logits) ready.\n"
     ]
    }
   ],
   "source": [
    "class ReDimNetMelLogitsWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  mel4d [B, 1, N_MELS, T]\n",
    "    Output: logits [B, num_speakers]\n",
    "    \"\"\"\n",
    "    def __init__(self, redim_model, head, l2_norm_emb: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone\n",
    "        self.pool = redim_model.pool\n",
    "        self.bn = redim_model.bn\n",
    "        self.linear = redim_model.linear  # [B, emb_dim]\n",
    "        self.head = head\n",
    "        self.l2_norm_emb = l2_norm_emb\n",
    "\n",
    "    def forward(self, mel4d: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(mel4d)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn(x)\n",
    "        emb = self.linear(x)\n",
    "\n",
    "        if self.l2_norm_emb:\n",
    "            emb = emb / (emb.norm(p=2, dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        logits = self.head(emb)\n",
    "        return logits\n",
    "\n",
    "wrapped_model = ReDimNetMelLogitsWrapper(\n",
    "    redim_model=redim_model,\n",
    "    head=head,\n",
    "    l2_norm_emb=True,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "print(\"Wrapper (logits) ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target layers: ['stage4']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Choose target layers (name -> module) ----------\n",
    "# NOTE: indices [2] are based on what you had. If any index errors happen,\n",
    "# print the modules and adjust accordingly.\n",
    "TARGET_LAYERS = {\n",
    "    # \"stem\":   wrapped_model.backbone.stem[0],\n",
    "    # \"stage0\": wrapped_model.backbone.stage0[2],\n",
    "    # \"stage1\": wrapped_model.backbone.stage1[2],\n",
    "    # \"stage2\": wrapped_model.backbone.stage2[2],\n",
    "    # \"stage3\": wrapped_model.backbone.stage3[2],\n",
    "    \"stage4\": wrapped_model.backbone.stage4[2],\n",
    "    # \"stage5\": wrapped_model.backbone.stage5[2],\n",
    "}\n",
    "print(\"Target layers:\", list(TARGET_LAYERS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCEPT_ROOT = /home/SpeakerRec/BioVoice/concept/positive_concepts_dataset_72\n",
      "DATA_DIR     = /home/SpeakerRec/BioVoice/data/wavs\n",
      "Found 12 concept dirs\n",
      "Found wav files: 90\n"
     ]
    }
   ],
   "source": [
    "# ---------- Data / Concepts ----------\n",
    "CONCEPT_ROOT = PROJECT_ROOT / \"concept\" / \"positive_concepts_dataset_72\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "\n",
    "print(\"CONCEPT_ROOT =\", CONCEPT_ROOT)\n",
    "print(\"DATA_DIR     =\", DATA_DIR)\n",
    "\n",
    "concept_dirs = sorted([d for d in CONCEPT_ROOT.iterdir() if d.is_dir()])\n",
    "print(f\"Found {len(concept_dirs)} concept dirs\")\n",
    "if len(concept_dirs) < 2:\n",
    "    raise RuntimeError(\"Need at least 2 concept folders (pos vs neg) for TCAV.\")\n",
    "\n",
    "wav_files = sorted(DATA_DIR.glob(\"*.wav\"))\n",
    "if not wav_files:\n",
    "    raise RuntimeError(f\"No wav files found in {DATA_DIR}\")\n",
    "print(\"Found wav files:\", len(wav_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def speaker_from_filename(fname: str) -> str:\n",
    "    n = fname.lower()\n",
    "    if n.startswith(\"eden\"):\n",
    "        return \"eden\"\n",
    "    if n.startswith(\"idan\"):\n",
    "        return \"idan\"\n",
    "    if n.startswith(\"yoav\"):\n",
    "        return \"yoav\"\n",
    "    return \"other\"\n",
    "\n",
    "def wav_to_redim_mel_np(wav_path: Path) -> np.ndarray:\n",
    "    wav, sr = torchaudio.load(str(wav_path))\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)  # (1, N_MELS, T)\n",
    "    return mel.squeeze(0).cpu().numpy().astype(np.float32)  # (N_MELS, T)\n",
    "\n",
    "def mel_to_redim_input(mel: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel: (N_MELS, T) numpy\n",
    "    returns: (1, 1, N_MELS, T) torch on DEVICE\n",
    "    \"\"\"\n",
    "    if mel.ndim != 2:\n",
    "        raise ValueError(f\"Expected mel (F,T), got {mel.shape}\")\n",
    "    F, T = mel.shape\n",
    "    if F != N_MELS:\n",
    "        raise ValueError(f\"Mel bins mismatch: expected {N_MELS}, got {F}\")\n",
    "    x = torch.from_numpy(mel).float().unsqueeze(0).unsqueeze(0)  # (1,1,F,T)\n",
    "    return x.to(DEVICE)\n",
    "\n",
    "def tensor_to_channel_vec(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert activation/grad tensor (with batch) into a 1D channel vector (C,)\n",
    "    by averaging across non-channel dimensions.\n",
    "    Supports:\n",
    "      (B,C,H,W) -> mean over H,W\n",
    "      (B,C,T)   -> mean over T\n",
    "      (B,T,C)   -> mean over T\n",
    "      (B,C)     -> C\n",
    "    \"\"\"\n",
    "    if t.ndim < 2:\n",
    "        raise RuntimeError(f\"Unexpected tensor shape: {tuple(t.shape)}\")\n",
    "\n",
    "    t = t[0]  # remove batch\n",
    "\n",
    "    if t.ndim == 1:        # (C,)\n",
    "        v = t\n",
    "    elif t.ndim == 2:      # (C,T) or (T,C)\n",
    "        v = t.mean(dim=1) if t.shape[0] <= t.shape[1] else t.mean(dim=0)\n",
    "    elif t.ndim == 3:      # (C,H,W)\n",
    "        v = t.mean(dim=(1, 2))\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unsupported tensor shape after batch removed: {tuple(t.shape)}\")\n",
    "\n",
    "    return v.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "def get_activation_vec(mel: np.ndarray, layer: nn.Module) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run wrapped_model and capture layer activation -> channel vec.\n",
    "    \"\"\"\n",
    "    x = mel_to_redim_input(mel)\n",
    "    store = {}\n",
    "\n",
    "    def hook_fn(_m, _inp, out):\n",
    "        store[\"act\"] = out.detach()\n",
    "\n",
    "    h = layer.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        _ = wrapped_model(x)\n",
    "    h.remove()\n",
    "\n",
    "    if \"act\" not in store:\n",
    "        raise RuntimeError(\"Hook did not capture activation.\")\n",
    "    return tensor_to_channel_vec(store[\"act\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing eval mels: 100%|██████████| 90/90 [00:00<00:00, 98.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval items: 90\n",
      "eden: 30\n",
      "idan: 30\n",
      "yoav: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Precompute eval mels (using model.spec for consistency) ----------\n",
    "eval_items = []  # list of dicts: {wav_path, speaker, mel_np}\n",
    "speaker_items = defaultdict(list)\n",
    "\n",
    "for w in tqdm(wav_files, desc=\"Precomputing eval mels\"):\n",
    "    spk = speaker_from_filename(w.name)\n",
    "    if spk == \"other\":\n",
    "        continue\n",
    "    if spk not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    mel_np = wav_to_redim_mel_np(w)  # (N_MELS, T)\n",
    "    if mel_np.shape[0] != N_MELS:\n",
    "        raise RuntimeError(f\"{w.name}: expected {N_MELS} mel bins, got {mel_np.shape}\")\n",
    "\n",
    "    item = {\"wav\": w, \"speaker\": spk, \"mel\": mel_np}\n",
    "    eval_items.append(item)\n",
    "    speaker_items[spk].append(item)\n",
    "\n",
    "print(\"Eval items:\", len(eval_items))\n",
    "for spk, items in speaker_items.items():\n",
    "    print(f\"{spk}: {len(items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Build CAV ----------\n",
    "def build_cav_for_concept_and_layer(\n",
    "    layer_name: str,\n",
    "    concept_dir: Path,\n",
    "    all_concept_dirs: list[Path],\n",
    ") -> np.ndarray:\n",
    "    layer = TARGET_LAYERS[layer_name]\n",
    "\n",
    "    pos_paths = sorted(concept_dir.glob(\"*.npy\"))\n",
    "    if not pos_paths:\n",
    "        raise RuntimeError(f\"No .npy files in {concept_dir}\")\n",
    "\n",
    "    neg_paths_all = []\n",
    "    for d in all_concept_dirs:\n",
    "        if d == concept_dir:\n",
    "            continue\n",
    "        neg_paths_all.extend(sorted(d.glob(\"*.npy\")))\n",
    "    if not neg_paths_all:\n",
    "        raise RuntimeError(\"No negative samples found in other concept dirs.\")\n",
    "\n",
    "    n_pos = len(pos_paths)\n",
    "    n_neg = min(len(neg_paths_all), n_pos * 2)\n",
    "    neg_paths = [neg_paths_all[i] for i in np.random.choice(len(neg_paths_all), n_neg, replace=False)]\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Positive\n",
    "    for p in pos_paths:\n",
    "        mel = np.load(p).astype(np.float32)  # expected (N_MELS, T)\n",
    "        if mel.shape[0] != N_MELS:\n",
    "            raise RuntimeError(f\"{p.name}: expected {N_MELS} mel bins, got {mel.shape}\")\n",
    "        X.append(get_activation_vec(mel, layer))\n",
    "        Y.append(1)\n",
    "\n",
    "    # Negative\n",
    "    for p in neg_paths:\n",
    "        mel = np.load(p).astype(np.float32)\n",
    "        if mel.shape[0] != N_MELS:\n",
    "            raise RuntimeError(f\"{p.name}: expected {N_MELS} mel bins, got {mel.shape}\")\n",
    "        X.append(get_activation_vec(mel, layer))\n",
    "        Y.append(0)\n",
    "\n",
    "    X = np.vstack(X).astype(np.float32)\n",
    "    Y = np.array(Y, dtype=np.int64)\n",
    "\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=1e-4, max_iter=2000, tol=1e-3)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    cav = clf.coef_.reshape(-1).astype(np.float32)\n",
    "    cav /= (np.linalg.norm(cav) + 1e-8)\n",
    "    return cav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- TCAV Score (directional derivative sign on class logit) ----------\n",
    "def tcav_score_for_items(layer_name: str, cav: np.ndarray, items: list[dict]) -> float:\n",
    "    layer = TARGET_LAYERS[layer_name]\n",
    "    positives = 0\n",
    "    total = 0\n",
    "\n",
    "    for item in items:\n",
    "        spk = item[\"speaker\"]\n",
    "        mel_np = item[\"mel\"]\n",
    "\n",
    "        x = mel_to_redim_input(mel_np)\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        store = {}\n",
    "\n",
    "        def hook_fn(_m, _inp, out):\n",
    "            out.retain_grad()\n",
    "            store[\"act\"] = out\n",
    "\n",
    "        h = layer.register_forward_hook(hook_fn)\n",
    "        logits = wrapped_model(x)  # (1, num_speakers)\n",
    "        h.remove()\n",
    "\n",
    "        cls = speaker_to_id[spk]\n",
    "        scalar = logits[0, cls]\n",
    "\n",
    "        wrapped_model.zero_grad()\n",
    "        scalar.backward()\n",
    "\n",
    "        if \"act\" not in store or store[\"act\"].grad is None:\n",
    "            raise RuntimeError(\"No gradient captured. Layer might not be connected to output.\")\n",
    "\n",
    "        grad_vec = tensor_to_channel_vec(store[\"act\"].grad)\n",
    "        dd = float(np.dot(grad_vec, cav))\n",
    "\n",
    "        positives += (dd > 0.0)\n",
    "        total += 1\n",
    "\n",
    "    return positives / total if total else 0.0\n",
    "\n",
    "def tcav_score_all(layer_name: str, cav: np.ndarray) -> float:\n",
    "    return tcav_score_for_items(layer_name, cav, eval_items)\n",
    "\n",
    "def tcav_score_for_speaker(layer_name: str, cav: np.ndarray, speaker: str) -> float:\n",
    "    return tcav_score_for_items(layer_name, cav, speaker_items.get(speaker, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept: long_constant_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.356\n",
      "    TCAV(eden) = 0.067\n",
      "    TCAV(idan) = 0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:   8%|▊         | 1/12 [00:32<06:02, 32.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.867\n",
      "\n",
      "=== Concept: long_dropping_flat_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.622\n",
      "    TCAV(eden) = 0.867\n",
      "    TCAV(idan) = 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  17%|█▋        | 2/12 [01:05<05:28, 32.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.367\n",
      "\n",
      "=== Concept: long_dropping_steep_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.622\n",
      "    TCAV(eden) = 0.833\n",
      "    TCAV(idan) = 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  25%|██▌       | 3/12 [01:38<04:53, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.300\n",
      "\n",
      "=== Concept: long_dropping_steep_thin ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.744\n",
      "    TCAV(eden) = 0.967\n",
      "    TCAV(idan) = 0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  33%|███▎      | 4/12 [02:10<04:20, 32.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.500\n",
      "\n",
      "=== Concept: long_rising_flat_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.300\n",
      "    TCAV(eden) = 0.267\n",
      "    TCAV(idan) = 0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  42%|████▏     | 5/12 [02:43<03:48, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.400\n",
      "\n",
      "=== Concept: long_rising_steep_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.356\n",
      "    TCAV(eden) = 0.833\n",
      "    TCAV(idan) = 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  50%|█████     | 6/12 [03:16<03:15, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.033\n",
      "\n",
      "=== Concept: long_rising_steep_thin ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.711\n",
      "    TCAV(eden) = 0.967\n",
      "    TCAV(idan) = 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  58%|█████▊    | 7/12 [03:48<02:43, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.433\n",
      "\n",
      "=== Concept: short_constant_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.378\n",
      "    TCAV(eden) = 0.033\n",
      "    TCAV(idan) = 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  67%|██████▋   | 8/12 [04:21<02:10, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.900\n",
      "\n",
      "=== Concept: short_dropping_steep_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.478\n",
      "    TCAV(eden) = 0.333\n",
      "    TCAV(idan) = 0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  75%|███████▌  | 9/12 [04:54<01:38, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.933\n",
      "\n",
      "=== Concept: short_dropping_steep_thin ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.400\n",
      "    TCAV(eden) = 0.033\n",
      "    TCAV(idan) = 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  83%|████████▎ | 10/12 [05:27<01:05, 32.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.767\n",
      "\n",
      "=== Concept: short_rising_steep_thick ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.289\n",
      "    TCAV(eden) = 0.333\n",
      "    TCAV(idan) = 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts:  92%|█████████▏| 11/12 [06:00<00:32, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.233\n",
      "\n",
      "=== Concept: short_rising_steep_thin ===\n",
      "  Building CAV for layer stage4 ...\n",
      "    TCAV(all) = 0.400\n",
      "    TCAV(eden) = 0.167\n",
      "    TCAV(idan) = 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concepts: 100%|██████████| 12/12 [06:33<00:00, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TCAV(yoav) = 0.333\n",
      "Saved: /home/SpeakerRec/BioVoice/redimnet/tcav/tcav_redimnet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Run TCAV ----------\n",
    "results = []\n",
    "cavs = {}\n",
    "\n",
    "for cdir in tqdm(concept_dirs, desc=\"Concepts\"):\n",
    "    cname = cdir.name\n",
    "    print(f\"\\n=== Concept: {cname} ===\")\n",
    "\n",
    "    for layer_name in TARGET_LAYERS.keys():\n",
    "        print(f\"  Building CAV for layer {layer_name} ...\")\n",
    "        cav = build_cav_for_concept_and_layer(layer_name, cdir, concept_dirs)\n",
    "        cavs[(cname, layer_name)] = cav\n",
    "\n",
    "        s_all = tcav_score_all(layer_name, cav)\n",
    "        results.append({\"Concept\": cname, \"Layer\": layer_name, \"Speaker\": \"all\", \"TCAV\": s_all})\n",
    "        print(f\"    TCAV(all) = {s_all:.3f}\")\n",
    "\n",
    "        for spk in [\"eden\", \"idan\", \"yoav\"]:\n",
    "            if spk not in speaker_items:\n",
    "                continue\n",
    "            s_spk = tcav_score_for_speaker(layer_name, cav, spk)\n",
    "            results.append({\"Concept\": cname, \"Layer\": layer_name, \"Speaker\": spk, \"TCAV\": s_spk})\n",
    "            print(f\"    TCAV({spk}) = {s_spk:.3f}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "out_csv = Path.cwd() / \"tcav_redimnet.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>TCAV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long_constant_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>all</td>\n",
       "      <td>0.355556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long_constant_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>eden</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>long_constant_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>idan</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long_constant_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>yoav</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long_dropping_flat_thick</td>\n",
       "      <td>stage4</td>\n",
       "      <td>all</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Concept   Layer Speaker      TCAV\n",
       "0       long_constant_thick  stage4     all  0.355556\n",
       "1       long_constant_thick  stage4    eden  0.066667\n",
       "2       long_constant_thick  stage4    idan  0.133333\n",
       "3       long_constant_thick  stage4    yoav  0.866667\n",
       "4  long_dropping_flat_thick  stage4     all  0.622222"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: show a quick preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → tcav_per_wav_stage4_wide.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>long_constant_thick</th>\n",
       "      <th>long_dropping_flat_thick</th>\n",
       "      <th>long_dropping_steep_thick</th>\n",
       "      <th>long_dropping_steep_thin</th>\n",
       "      <th>long_rising_flat_thick</th>\n",
       "      <th>long_rising_steep_thick</th>\n",
       "      <th>long_rising_steep_thin</th>\n",
       "      <th>short_constant_thick</th>\n",
       "      <th>short_dropping_steep_thick</th>\n",
       "      <th>short_dropping_steep_thin</th>\n",
       "      <th>short_rising_steep_thick</th>\n",
       "      <th>short_rising_steep_thin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eden_001.wav</td>\n",
       "      <td>eden</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>1.180272e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>4.145732e-07</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>9.265759e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>5.853912e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eden_002.wav</td>\n",
       "      <td>eden</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3.175466e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>4.479923e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-2.939742e-05</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-2.572330e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eden_003.wav</td>\n",
       "      <td>eden</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2.771273e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.685402e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-7.898740e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-4.419076e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eden_004.wav</td>\n",
       "      <td>eden</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-3.250373e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-1.168117e-05</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-1.068907e-05</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.820491e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eden_005.wav</td>\n",
       "      <td>eden</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.885032e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-1.340914e-05</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-2.873736e-05</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.163866e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wav_name speaker  long_constant_thick  long_dropping_flat_thick  \\\n",
       "0  eden_001.wav    eden            -0.000004                 -0.000019   \n",
       "1  eden_002.wav    eden            -0.000025                  0.000029   \n",
       "2  eden_003.wav    eden            -0.000001                  0.000013   \n",
       "3  eden_004.wav    eden            -0.000006                  0.000013   \n",
       "4  eden_005.wav    eden            -0.000031                  0.000005   \n",
       "\n",
       "   long_dropping_steep_thick  long_dropping_steep_thin  \\\n",
       "0               1.180272e-05                 -0.000017   \n",
       "1               3.175466e-06                  0.000028   \n",
       "2               2.771273e-06                  0.000007   \n",
       "3              -3.250373e-07                  0.000008   \n",
       "4               1.885032e-05                  0.000017   \n",
       "\n",
       "   long_rising_flat_thick  long_rising_steep_thick  long_rising_steep_thin  \\\n",
       "0            4.145732e-07                 0.000026               -0.000017   \n",
       "1            4.479923e-06                 0.000022                0.000030   \n",
       "2            1.685402e-06                 0.000005                0.000004   \n",
       "3           -1.168117e-05                -0.000001                0.000005   \n",
       "4           -1.340914e-05                 0.000022                0.000018   \n",
       "\n",
       "   short_constant_thick  short_dropping_steep_thick  \\\n",
       "0          9.265759e-07                    0.000008   \n",
       "1         -2.939742e-05                   -0.000018   \n",
       "2         -7.898740e-06                    0.000012   \n",
       "3         -1.068907e-05                   -0.000002   \n",
       "4         -2.873736e-05                   -0.000009   \n",
       "\n",
       "   short_dropping_steep_thin  short_rising_steep_thick  \\\n",
       "0                   0.000011                  0.000023   \n",
       "1                  -0.000044                 -0.000022   \n",
       "2                  -0.000007                  0.000005   \n",
       "3                  -0.000006                  0.000003   \n",
       "4                  -0.000019                 -0.000007   \n",
       "\n",
       "   short_rising_steep_thin  \n",
       "0             5.853912e-06  \n",
       "1            -2.572330e-05  \n",
       "2            -4.419076e-08  \n",
       "3             5.820491e-06  \n",
       "4            -1.163866e-05  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# -----------------------------\n",
    "# WIDE CSV: one row per wav\n",
    "# columns: wav_name, speaker, concept1, concept2, ...\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "selected_layer = \"stage4\"  # change to the layer you want\n",
    "layer = TARGET_LAYERS[selected_layer]\n",
    "\n",
    "concept_names = sorted({c for (c, l) in cavs.keys() if l == selected_layer})\n",
    "if not concept_names:\n",
    "    raise RuntimeError(f\"No CAVs for layer {selected_layer}\")\n",
    "\n",
    "CAV_MAT = np.stack([cavs[(c, selected_layer)] for c in concept_names], axis=0).astype(np.float32)\n",
    "cav_dim = CAV_MAT.shape[1]\n",
    "\n",
    "def tensor_to_channel_vec(t: torch.Tensor) -> np.ndarray:\n",
    "    t = t[0]\n",
    "    if t.ndim == 1:\n",
    "        v = t\n",
    "    elif t.ndim == 2:\n",
    "        v = t.mean(dim=1) if t.shape[0] <= t.shape[1] else t.mean(dim=0)\n",
    "    elif t.ndim == 3:\n",
    "        v = t.mean(dim=(1, 2))\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unsupported tensor shape: {tuple(t.shape)}\")\n",
    "    return v.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "rows = []\n",
    "for item in eval_items:  # {\"wav\": Path, \"speaker\": str, \"mel\": np.ndarray}\n",
    "    wav_path = item[\"wav\"]\n",
    "    speaker = item[\"speaker\"]\n",
    "    mel_np = item[\"mel\"]\n",
    "\n",
    "    if speaker not in speaker_to_id:\n",
    "        continue\n",
    "\n",
    "    x = mel_to_redim_input(mel_np)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    store = {}\n",
    "    def hook_fn(_m, _inp, out):\n",
    "        out.retain_grad()\n",
    "        store[\"act\"] = out\n",
    "\n",
    "    h = layer.register_forward_hook(hook_fn)\n",
    "    logits = wrapped_model(x)\n",
    "    h.remove()\n",
    "\n",
    "    scalar = logits[0, speaker_to_id[speaker]]\n",
    "    wrapped_model.zero_grad(set_to_none=True)\n",
    "    scalar.backward()\n",
    "\n",
    "    g_vec = tensor_to_channel_vec(store[\"act\"].grad)\n",
    "    if g_vec.shape[0] != cav_dim:\n",
    "        raise RuntimeError(f\"Grad dim {g_vec.shape[0]} != CAV dim {cav_dim} at {selected_layer}\")\n",
    "\n",
    "    dd_vals = (CAV_MAT @ g_vec).astype(np.float32)\n",
    "\n",
    "    row = {\"wav_name\": wav_path.name, \"speaker\": speaker}\n",
    "    for c, dd in zip(concept_names, dd_vals):\n",
    "        row[c] = float(dd)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"wav_name\", \"speaker\"] + concept_names)\n",
    "csv_path = f\"tcav_per_wav_{selected_layer}_wide.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved →\", csv_path)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
