{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# PROJECT_ROOT = BioVoice/\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "redim_model = torch.hub.load(\n",
    "    \"IDRnD/ReDimNet\",\n",
    "    \"ReDimNet\",\n",
    "    model_name=\"b5\",\n",
    "    train_type=\"ptn\",\n",
    "    dataset=\"vox2\",\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "print(\"Loaded ReDimNet successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_redim(wav_path: str):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = redim_model(wav)      # [1,192]\n",
    "\n",
    "    emb = emb.squeeze().cpu().numpy()\n",
    "    emb = emb / (np.linalg.norm(emb) + 1e-12)\n",
    "    return emb\n",
    "\n",
    "class CosineSimilarityTarget:\n",
    "    def __init__(self, ref_embedding):\n",
    "        self.ref = torch.tensor(ref_embedding).float().to(DEVICE)\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        emb = model_output.squeeze(0)     # [192]\n",
    "        return F.cosine_similarity(emb, self.ref, dim=0)\n",
    "\n",
    "def wav_to_model_mel(wav_path):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    wav = wav[:1, :].float().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mel = redim_model.spec(wav)   # [1, 80, T]\n",
    "\n",
    "    mel4d = mel.unsqueeze(0)          # [1, 1, 80, T]\n",
    "\n",
    "    mel_np = mel.squeeze(0).cpu().numpy()\n",
    "    mel_norm = (mel_np - mel_np.min()) / (mel_np.max() - mel_np.min() + 1e-8)\n",
    "    rgb_base = np.stack([mel_norm] * 3, axis=-1)\n",
    "    # rgb_base = np.flipud(rgb_base)\n",
    "\n",
    "    return mel4d, rgb_base\n",
    "\n",
    "\n",
    "def plot_mel(mel, save_path=None, title=None):\n",
    "    \"\"\"\n",
    "    mel: torch.Tensor or np.ndarray with shape [80, T] or [1, 80, T]\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(mel, torch.Tensor):\n",
    "        mel = mel.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # normalize for visualization\n",
    "    mel_norm = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
    "\n",
    "    # flip so low freq is at bottom\n",
    "    mel_norm = np.flipud(mel_norm)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_norm, aspect=\"auto\", cmap=\"magma\")\n",
    "    plt.colorbar(label=\"Normalized Energy\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Frequency Bins\")\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = PROJECT_ROOT / \"data\" / \"wavs\"\n",
    "\n",
    "best_templates = {\n",
    "    \"eden\": wav_dir / \"eden_013.wav\",\n",
    "    \"idan\": wav_dir / \"idan_012.wav\",\n",
    "    \"yoav\": wav_dir / \"yoav_022.wav\",\n",
    "}\n",
    "\n",
    "ref_emb = embed_with_redim(str(best_templates[\"yoav\"]))\n",
    "\n",
    "print(\"Reference embedding shape =\", ref_emb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReDimNetMelWrapper(torch.nn.Module): # grad cam expects a model and hooks into forward (4d input)\n",
    "    def __init__(self, redim_model):\n",
    "        super().__init__()\n",
    "        self.backbone = redim_model.backbone # copy the feature extractor\n",
    "        self.pool     = redim_model.pool \n",
    "        self.bn       = redim_model.bn\n",
    "        self.linear   = redim_model.linear\n",
    "\n",
    "    def forward(self, mel4d):\n",
    "        \"\"\"\n",
    "        mel4d shape: [B, 1, 80, T]\n",
    "        \"\"\"\n",
    "        x = self.backbone(mel4d)   # goes through stem, stage0..stage5 automatically\n",
    "        x = self.pool(x) \n",
    "        x = self.bn(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "wrapped_model = ReDimNetMelWrapper(redim_model).to(DEVICE).eval()\n",
    "print(\"Wrapper ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick ONE Conv2d per stage + stem\n",
    "target_layers = [\n",
    "    wrapped_model.backbone.stem[0],      # Conv2d(1, 32, 3x3, ...)\n",
    "    wrapped_model.backbone.stage0[2],    # Conv2d in stage0\n",
    "    wrapped_model.backbone.stage1[2],    # Conv2d in stage1\n",
    "    wrapped_model.backbone.stage2[2],    # Conv2d in stage2\n",
    "    wrapped_model.backbone.stage3[2],    # Conv2d in stage3\n",
    "    wrapped_model.backbone.stage4[2],    # Conv2d in stage4\n",
    "    wrapped_model.backbone.stage5[2],    # Conv2d in stage5\n",
    "]\n",
    "\n",
    "layer_names = [\"stem\", \"stage0\", \"stage1\", \"stage2\", \"stage3\", \"stage4\", \"stage5\"]\n",
    "\n",
    "print(\"Target Conv2d layers:\")\n",
    "for name, layer in zip(layer_names, target_layers):\n",
    "    print(f\"{name}: {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float01(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "    return np.clip(img, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradcam_redim(wav_path, ref_emb, save_dir=\"gradcam_results_redim_v2\", thr=0.8):\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fname = Path(wav_path).stem\n",
    "\n",
    "    originals_dir = save_dir / \"original\"\n",
    "    originals_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- compute mel + base image ----\n",
    "    mel4d, rgb_base = wav_to_model_mel(wav_path)\n",
    "    rgb_base01 = to_float01(rgb_base)\n",
    "\n",
    "    out_file = originals_dir / f\"{fname}_mel_rgbbase.png\"\n",
    "    plt.imsave(out_file, rgb_base01)\n",
    "    print(\"Saved:\", out_file)\n",
    "\n",
    "    mel = mel4d.squeeze(0)  # expected (80, T) or (1,80,T) depending on your pipeline\n",
    "    plot_mel(\n",
    "        mel,\n",
    "        save_path=originals_dir / f\"{fname}_original_mel.png\",\n",
    "        title=fname\n",
    "    )\n",
    "    print(\"Saved:\", originals_dir / f\"{fname}_original_mel.png\")\n",
    "\n",
    "    target = CosineSimilarityTarget(ref_emb)\n",
    "\n",
    "    for layer, lname in zip(target_layers, layer_names):\n",
    "        layer_dir = save_dir / lname\n",
    "        layer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cam = GradCAM(model=wrapped_model, target_layers=[layer])\n",
    "\n",
    "        grayscale_cam = cam(\n",
    "            input_tensor=mel4d,\n",
    "            targets=[target],\n",
    "            aug_smooth=True,\n",
    "            eigen_smooth=True\n",
    "        )[0]\n",
    "\n",
    "        h, w = rgb_base01.shape[:2]\n",
    "        if grayscale_cam.shape == (w, h):\n",
    "            grayscale_cam = grayscale_cam.T\n",
    "        if grayscale_cam.shape != (h, w):\n",
    "            raise ValueError(\n",
    "                f\"shape mismatch: rgb_base={rgb_base01.shape}, grayscale_cam={grayscale_cam.shape} ({lname})\"\n",
    "            )\n",
    "\n",
    "        mask = (grayscale_cam >= thr).astype(np.float32)\n",
    "        cam_masked = grayscale_cam * mask\n",
    "\n",
    "        overlay_masked = show_cam_on_image(rgb_base01, cam_masked, use_rgb=True)\n",
    "        overlay_masked01 = to_float01(overlay_masked)\n",
    "\n",
    "        overlay_only = overlay_masked01 * mask[..., None]\n",
    "        overlay_only = np.clip(overlay_only, 0.0, 1.0)\n",
    "\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay_thr{thr:.2f}.png\", overlay_masked01)\n",
    "        plt.imsave(layer_dir / f\"{fname}_overlay_only_thr{thr:.2f}.png\", overlay_only)\n",
    "        plt.imsave(layer_dir / f\"{fname}_mask_thr{thr:.2f}.png\", mask, cmap=\"gray\")\n",
    "\n",
    "        print(f\"Saved layer outputs in: {layer_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wav = wav_dir / \"idan_022.wav\"\n",
    "run_gradcam_redim(str(test_wav), ref_emb , save_dir=f\"gradcam_results_redim_v3/idan/{Path(test_wav).stem}\", thr=0.3)\n",
    "\n",
    "# wav_by_user = {\n",
    "#     \"eden\": [],\n",
    "#     \"idan\": [],\n",
    "#     \"yoav\": [],\n",
    "# }\n",
    "\n",
    "# for wav_file in sorted(wav_dir.glob(\"eden_*.wav\"))[:5]:\n",
    "#     wav_by_user[\"eden\"].append(wav_file)\n",
    "# for wav_file in sorted(wav_dir.glob(\"idan_*.wav\"))[:5]:\n",
    "#     wav_by_user[\"idan\"].append(wav_file)\n",
    "\n",
    "# for wav_file in sorted(wav_dir.glob(\"yoav_*.wav\"))[:5]:\n",
    "#     wav_by_user[\"yoav\"].append(wav_file)\n",
    "\n",
    "\n",
    "\n",
    "# for speaker, wav_list in wav_by_user.items():\n",
    "#     for wav_path in wav_list:\n",
    "#         print(f\"Processing {speaker} - {wav_path.name} ...\")\n",
    "#         run_gradcam_redim(str(wav_path), ref_emb, save_dir=f\"gradcam_results_redim_v3/{speaker}/{Path(wav_path).stem}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker, wav_list in wav_by_user.items():\n",
    "    for wav_path in wav_list:\n",
    "        test_emb = embed_with_redim(str(wav_path))\n",
    "        cos = float(np.dot(test_emb, ref_emb))  \n",
    "        print(wav_path.name, cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
